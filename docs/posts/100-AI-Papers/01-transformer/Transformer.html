<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Transformer 是一种基于自注意力机制的深度学习架构， 能够并行处理序列，在语言、视觉和多模态任务中表现出色， 并且作为 GPT、BERT 等大型语言模型（LLM）的核心基础， 推动了当今生成式人工智能的快速发展。 在本篇文章中，我们将深入探讨 Transformer 的基本原理， 以及关键组件，包括 Word Embedding、 Position Embedding、 Attention、 Normalization Layer 和 Feed Forward Layer。 并且通过在  Ted Talks  数据集上的实验，展示 Transformer 在实际任务中的应用效果。">

<title>01: Attention is All You Need (Transformer) – Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" rel="next">
<link href="../../../posts/CS336/Lecture16&amp;17/lec16.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-c0eab5a31fbea23c8affb95fb4fbb9c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // 可访问性（键盘）
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../../logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/sta210-s22" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">100 AI Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function"><span class="header-section-number">1.1</span> Softmax Function</a></li>
  <li><a href="#vector-similarity" id="toc-vector-similarity" class="nav-link" data-scroll-target="#vector-similarity"><span class="header-section-number">1.2</span> Vector Similarity</a></li>
  </ul></li>
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer"><span class="header-section-number">2</span> Transformer</a>
  <ul>
  <li><a href="#sec-word-embedding" id="toc-sec-word-embedding" class="nav-link" data-scroll-target="#sec-word-embedding"><span class="header-section-number">2.1</span> Word Embedding Layer</a></li>
  <li><a href="#sec-postion-embedding" id="toc-sec-postion-embedding" class="nav-link" data-scroll-target="#sec-postion-embedding"><span class="header-section-number">2.2</span> Position Embedding Layer</a>
  <ul>
  <li><a href="#why-sinusoidal-position-embedding" id="toc-why-sinusoidal-position-embedding" class="nav-link" data-scroll-target="#why-sinusoidal-position-embedding"><span class="header-section-number">2.2.1</span> Why Sinusoidal Position Embedding?</a></li>
  </ul></li>
  <li><a href="#sec-attention" id="toc-sec-attention" class="nav-link" data-scroll-target="#sec-attention"><span class="header-section-number">2.3</span> Attention Layer</a>
  <ul>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">2.3.1</span> Multi-Head Attention</a></li>
  <li><a href="#self-attention-layer" id="toc-self-attention-layer" class="nav-link" data-scroll-target="#self-attention-layer"><span class="header-section-number">2.3.2</span> Self-Attention Layer</a></li>
  <li><a href="#causal-self-attention-layer" id="toc-causal-self-attention-layer" class="nav-link" data-scroll-target="#causal-self-attention-layer"><span class="header-section-number">2.3.3</span> Causal Self-Attention Layer</a></li>
  <li><a href="#cross-attention-layer" id="toc-cross-attention-layer" class="nav-link" data-scroll-target="#cross-attention-layer"><span class="header-section-number">2.3.4</span> Cross Attention Layer</a></li>
  <li><a href="#sec-attention-complexity" id="toc-sec-attention-complexity" class="nav-link" data-scroll-target="#sec-attention-complexity"><span class="header-section-number">2.3.5</span> Time Complexity of Attention</a></li>
  </ul></li>
  <li><a href="#sec-normalization" id="toc-sec-normalization" class="nav-link" data-scroll-target="#sec-normalization"><span class="header-section-number">2.4</span> Normalization Layer</a></li>
  <li><a href="#sec-feed-forward" id="toc-sec-feed-forward" class="nav-link" data-scroll-target="#sec-feed-forward"><span class="header-section-number">2.5</span> Feed Forward Layer</a></li>
  <li><a href="#residual-connection" id="toc-residual-connection" class="nav-link" data-scroll-target="#residual-connection"><span class="header-section-number">2.6</span> Residual Connection</a>
  <ul>
  <li><a href="#backpropagation-through-residual-connection" id="toc-backpropagation-through-residual-connection" class="nav-link" data-scroll-target="#backpropagation-through-residual-connection"><span class="header-section-number">2.6.1</span> Backpropagation through Residual Connection</a></li>
  </ul></li>
  <li><a href="#sec-output-layer" id="toc-sec-output-layer" class="nav-link" data-scroll-target="#sec-output-layer"><span class="header-section-number">2.7</span> Output Layer</a></li>
  <li><a href="#encoder-decoder-layer" id="toc-encoder-decoder-layer" class="nav-link" data-scroll-target="#encoder-decoder-layer"><span class="header-section-number">2.8</span> Encoder &amp; Decoder Layer</a></li>
  <li><a href="#others" id="toc-others" class="nav-link" data-scroll-target="#others"><span class="header-section-number">2.9</span> Others</a>
  <ul>
  <li><a href="#dropout-layer" id="toc-dropout-layer" class="nav-link" data-scroll-target="#dropout-layer"><span class="header-section-number">2.9.1</span> Dropout Layer</a></li>
  <li><a href="#label-smoothing" id="toc-label-smoothing" class="nav-link" data-scroll-target="#label-smoothing"><span class="header-section-number">2.9.2</span> Label Smoothing</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#experiment" id="toc-experiment" class="nav-link" data-scroll-target="#experiment"><span class="header-section-number">3</span> Experiment</a>
  <ul>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset"><span class="header-section-number">3.1</span> Dataset</a>
  <ul>
  <li><a href="#tokenizer-vocabulary" id="toc-tokenizer-vocabulary" class="nav-link" data-scroll-target="#tokenizer-vocabulary"><span class="header-section-number">3.1.1</span> Tokenizer &amp; Vocabulary</a></li>
  <li><a href="#padding-samples" id="toc-padding-samples" class="nav-link" data-scroll-target="#padding-samples"><span class="header-section-number">3.1.2</span> Padding Samples</a></li>
  </ul></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization"><span class="header-section-number">3.2</span> Weight Initialization</a></li>
  <li><a href="#optimizer" id="toc-optimizer" class="nav-link" data-scroll-target="#optimizer"><span class="header-section-number">3.3</span> Optimizer</a>
  <ul>
  <li><a href="#learning-rate-scheduler" id="toc-learning-rate-scheduler" class="nav-link" data-scroll-target="#learning-rate-scheduler"><span class="header-section-number">3.3.1</span> Learning Rate Scheduler</a></li>
  </ul></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">3.4</span> Loss Function</a></li>
  <li><a href="#evaluation-metric" id="toc-evaluation-metric" class="nav-link" data-scroll-target="#evaluation-metric"><span class="header-section-number">3.5</span> Evaluation Metric</a>
  <ul>
  <li><a href="#blue" id="toc-blue" class="nav-link" data-scroll-target="#blue"><span class="header-section-number">3.5.1</span> BLUE</a></li>
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity"><span class="header-section-number">3.5.2</span> Perplexity</a></li>
  </ul></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">3.6</span> Training</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3.7</span> Results</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4</span> Summary</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">5</span> Key Concepts</a></li>
  <li><a href="#q-a" id="toc-q-a" class="nav-link" data-scroll-target="#q-a"><span class="header-section-number">6</span> Q &amp; A</a></li>
  <li><a href="#related-resource-further-reading" id="toc-related-resource-further-reading" class="nav-link" data-scroll-target="#related-resource-further-reading"><span class="header-section-number">7</span> Related resource &amp; Further Reading</a></li>
  <li><a href="#in-the-end" id="toc-in-the-end" class="nav-link" data-scroll-target="#in-the-end"><span class="header-section-number">8</span> In the end</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">01: Attention is All You Need (<tag style="color:orange">Transformer</tag>)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Architecture</div>
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">⭐️⭐️⭐️⭐️⭐️</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>Transformer 是一种基于<strong>自注意力机制</strong>的深度学习架构， 能够并行处理序列，在语言、视觉和多模态任务中表现出色， 并且作为 <strong>GPT</strong>、<strong>BERT</strong> 等大型语言模型（LLM）的核心基础， 推动了当今生成式人工智能的快速发展。 在本篇文章中，我们将深入探讨 Transformer 的基本原理， 以及关键组件，包括 <strong>Word Embedding</strong>、 <strong>Position Embedding</strong>、 <strong>Attention</strong>、 <strong>Normalization Layer</strong> 和 <strong>Feed Forward Layer</strong>。 并且通过在 <a href="https://huggingface.co/datasets/IWSLT/ted_talks_iwslt" target="_blank" rel="noopener"> Ted Talks </a> 数据集上的实验，展示 Transformer 在实际任务中的应用效果。</p>
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>我们开始第一篇论文的学习： 《Attention is All You Need》 <span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>，也就是传说中的Transformer模型。Transformer模型的提出，彻底改变了自然语言处理（NLP）以及更广泛的领域。该架构完全基于<strong>注意力机制(Attention)</strong>，不再依赖循环（RNN）或卷积（CNN），因此在训练时<em>更易并行化、效率更高</em>。Transformer 已成为众多前沿模型的基础，不仅在 NLP 中表现突出，也扩展到计算机视觉等领域。比如 ChatGPT、DeepSeek 等大语言模型（LLM）都以 Transformer 为核心架构。所以我们自然就把它当作我们第一篇文章的首选。</p>
<section id="preliminary" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preliminary</h1>
<p>在开始学习Transformer之前，我们预习一下一些需要的知识，以便我们可以更好的理解这个模型。</p>
<section id="softmax-function" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="softmax-function"><span class="header-section-number">1.1</span> Softmax Function</h2>
<p>Softmax Function 是一个 <span class="hilite-teal">将实数向量转换为概率分布</span> 的函数，定义如下：</p>
<p><span id="eq-softmax"><span class="math display">\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\tag{1}\]</span></span></p>
<p>其中，<span class="math inline">\(z_i\)</span> 是输入向量的第 <span class="math inline">\(i\)</span> 个元素，<span class="math inline">\(e\)</span> 是自然对数的底数。Softmax 函数的输出是一个概率分布，所有输出值的和为 1。</p>
<div class="sourceCode" id="cb1" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> softmax(z):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    exp_z <span class="op">=</span> torch.exp(z)</span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="cf">return</span> exp_z <span class="op">/</span> torch.<span class="bu">sum</span>(exp_z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="vector-similarity" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="vector-similarity"><span class="header-section-number">1.2</span> Vector Similarity</h2>
<p>在Transformer中，计算向量之间的相似性是一个重要的步骤，常用的方法有点积（Dot Product）和余弦相似度（Cosine Similarity）。在Transformer中，主要使用Dot Product来衡量向量之间的相似性，接下来我们来简单回顾一下Dot Product的计算方法：</p>
<p><span id="eq-dot-product"><span class="math display">\[
\text{Dot Product}(A, B) = \sum_{i=1}^{n} A_i \cdot B_i
\tag{2}\]</span></span></p>
<p>其中，<span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 是两个向量，<span class="math inline">\(n\)</span> 是向量的维度，<span class="math inline">\(A_i\)</span> 和 <span class="math inline">\(B_i\)</span> 分别是向量 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 在第 <span class="math inline">\(i\)</span> 个维度的分量：</p>
<ul>
<li>Dot Product 的值越<strong>大</strong>，表示两个向量越<strong>相似</strong>。</li>
<li>Dot Product 的值越<strong>小</strong>，表示两个向量越<strong>不相似</strong>。</li>
</ul>
<p>Dot Product也可以看作是 <strong>Unnormalized Cosine Similarity</strong>，因为它没有对向量进行归一化处理。</p>
<p>我们也可以使用矩阵乘法来计算多个向量之间的相似性：</p>
<p><span id="eq-dot-product-matrix"><span class="math display">\[
\text{Dot Product Matrix}(A, B) = A B^\top
\tag{3}\]</span></span></p>
<p>其中，<span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(m \times n\)</span> 的矩阵，<span class="math inline">\(B\)</span> 是一个 <span class="math inline">\(p \times n\)</span> 的矩阵，<span class="math inline">\(B^\top\)</span> 是 <span class="math inline">\(B\)</span> 的转置矩阵，结果是一个 <span class="math inline">\(m \times p\)</span> 的矩阵，表示 <span class="math inline">\(A\)</span> 中的每个向量与 <span class="math inline">\(B\)</span> 中的每个向量之间的点积。</p>
<div class="sourceCode" id="cb2" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> dot_product_matrix(A, B):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="cf">return</span> torch.matmul(A, B.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="transformer" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Transformer</h1>
<p>简单回顾了一下这些数学知识，接下来，让我们来看看Transformer到底是个什么东西。</p>
<div id="fig-transformer-model-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-model-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/transformer-model-overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-model-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Transformer模型主要由两个部分组成：编码器（Encoder）和解码器（Decoder）。编码器负责处理输入序列，将其转换为一组连续的表示（通常称为“Context”）。解码器则根据这些表示生成输出序列。编码器和解码器都由多个相同的层（Layer）堆叠而成. 并且在最后通过一个线性层（Linear Layer）和 Softmax 层将解码器的输出转换为预测的词汇分布。
</figcaption>
</figure>
</div>
<p>Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span> 是 Google 在2017年提出的新的神经网络架构，它的提出主要是为了解决，</p>
<ul>
<li><span class="hilite-pink">Sequence Modeling的效率问题</span>:
<ul>
<li>在 Transformer 出现之前，主流方法是 <strong>RNN</strong>和 <strong>CNN</strong>。
<ul>
<li>RNN 需要<u>按顺序逐步处理序列，无法并行化</u>，训练和推理效率低下。</li>
<li>CNN 虽然有一定的并行性，但<u>捕捉长距离依赖需要堆叠很多层，计算开销大</u>。</li>
</ul></li>
</ul></li>
<li><span class="hilite-pink">Long Distance Dependency Modeling</span>:
<ul>
<li>RNN 在捕捉长距离依赖时容易出现<strong>Gradient Vanish</strong> 或<strong>Gradient Explosion</strong>，导致模型难以学习远距离的信息。</li>
</ul></li>
</ul>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Gradient Vanish &amp; Gradient Explosion</p>
</div>
<div class="note-container foldable-content">
<ul>
<li><strong>Gradient Vanish</strong> 问题是指在深度神经网络中，随着梯度在反向传播过程中逐层传递，<u>梯度值逐渐变小，最终趋近于零</u>的现象。这会导致前面的层几乎没有梯度更新，从而无法有效学习。</li>
<li><strong>Gradient Explosion</strong> 则是指在深度神经网络中，随着梯度在反向传播过程中逐层传递，<u>梯度值逐渐变大，最终变得非常大</u>，导致模型参数更新过大，训练过程不稳定，甚至发散。</li>
</ul>
</div>
</div>
<blockquote class="blockquote">
<p>This <u>inherently sequential nature precludes parallelization</u> within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. … In these models, <u>the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions</u>, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions <cite> Attention is all you need, p.2 </cite></p>
</blockquote>
<p>Transformer的基本框架如 <a href="#fig-transformer-model-overview" class="quarto-xref">Figure&nbsp;1</a> 所示。整体结构比较简单清晰，主要包括Encoder和Decoder两大部分:</p>
<div class="sourceCode" id="cb3" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(config)</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(config)</span>
<span id="cb3-7"><a href="#cb3-7"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.d_model, config.tgt_vocab_size)</span>
<span id="cb3-8"><a href="#cb3-8"></a>        ...</span>
<span id="cb3-9"><a href="#cb3-9"></a>    </span>
<span id="cb3-10"><a href="#cb3-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, original, target):</span>
<span id="cb3-11"><a href="#cb3-11"></a>        enc_output <span class="op">=</span> <span class="va">self</span>.encoder(original)</span>
<span id="cb3-12"><a href="#cb3-12"></a>        dec_output <span class="op">=</span> <span class="va">self</span>.decoder(target, enc_output)</span>
<span id="cb3-13"><a href="#cb3-13"></a>        output <span class="op">=</span> <span class="va">self</span>.output_layer(dec_output)</span>
<span id="cb3-14"><a href="#cb3-14"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>接下来让我们从下至上，来深度解刨Transformer的模型结构，主要包括以下几个关键组件:</p>
<ol type="1">
<li>Word Embedding Layer (<a href="#sec-word-embedding" class="quarto-xref">Section&nbsp;2.1</a>): 将词汇转换为向量表示</li>
<li>Position Embedding Layer (<a href="#sec-postion-embedding" class="quarto-xref">Section&nbsp;2.2</a>): 注入位置信息</li>
<li>Attention Layer (<a href="#sec-attention" class="quarto-xref">Section&nbsp;2.3</a>): Transformer的核心组件
<ol type="1">
<li>Self-Attention Layer: 处理输入序列内部的依赖关系</li>
<li>Causal Self-Attention Layer: 处理解码器中的Auto-Regressive依赖关系</li>
<li>Cross-Attention Layer: 处理输入序列和输出序列之间的依赖关系</li>
</ol></li>
<li>Normalization Layer (<a href="#sec-normalization" class="quarto-xref">Section&nbsp;2.4</a>): 标准化输入，稳定训练</li>
<li>Feed Forward Layer (<a href="#sec-feed-forward" class="quarto-xref">Section&nbsp;2.5</a>): 非线性变换，增强模型表达能力</li>
<li>Output Layer(<a href="#sec-output-layer" class="quarto-xref">Section&nbsp;2.7</a>): 生成最终的预测结果</li>
</ol>
<section id="sec-word-embedding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-word-embedding"><span class="header-section-number">2.1</span> Word Embedding Layer</h2>
<p>Word Embedding 基本是所有语言模型的第一步，它的作用是 <span class="hilite-teal">将离散的词汇转换为连续的向量表示</span>。这样，模型就可以在一个<strong>高维空间</strong>中处理词汇之间的关系和相似性。我们通常使用一个嵌入矩阵（Embedding Matrix）来实现这一点:</p>
<p><span id="eq-word-embedding"><span class="math display">\[
\text{Embedding}(w) = W_{e}[w]
\tag{4}\]</span></span></p>
<p>其中，<span class="math inline">\(W_{e} \in \mathbb{R}^{V \times d}\)</span> 是嵌入矩阵, <span class="math inline">\(w \in {0, 1, \dots, V-1}\)</span> 是词汇在词表中的索引，<span class="math inline">\(d\)</span> 是嵌入维度，<span class="math inline">\(V\)</span> 是词汇表大小。 该操作等价于将词汇 <span class="math inline">\(w\)</span> 的 One-Hot Encoding 与嵌入矩阵相乘，即：</p>
<p><span id="eq-word-embedding-one-hot"><span class="math display">\[
\text{Embedding}(w) = W_{e}^{\top} \cdot \text{one hot}(w), \quad \text{one hot}(w) \in \mathbb{R}^{V}
\tag{5}\]</span></span></p>
<p>从实现角度看，这一过程可以直接理解为：通过词汇索引 <span class="math inline">\(w\)</span>，从嵌入矩阵 <span class="math inline">\(W_e\)</span> 中取出第 <span class="math inline">\(w\)</span> 行作为该词的向量表示。</p>
<p>更直观的方式就是，我们可以将它看作一个查找表（Lookup Table），<u>通过词汇的索引直接获取对应的嵌入向量</u>。接下来我们来看一下代码实现：</p>
<div class="sourceCode" id="cb4" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> navie_embedding(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, v, d):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Parameter(torch.randn(v, d)) <span class="co"># 初始化Embedding Table</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>    </span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="co"># x: (batch_size, seq_len)</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>        </span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="co"># 第一种方法: </span></span>
<span id="cb4-10"><a href="#cb4-10"></a>        <span class="co"># return self.embedding[x]  # 直接索引获取嵌入向量</span></span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>        <span class="co"># 第二种方法: One Hot Encoding</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>        <span class="co"># x_one_hot = F.one_hot(x, num_classes=self.embedding.size(0)).float() # (batch_size, seq_len, v)</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>        <span class="co"># return torch.matmul(x_one_hot, self.embedding) # (batch_size, seq_len, d)</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>        <span class="co"># 第三种方法，利用Gather函数</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>        <span class="co"># batch_size, seq_len = x.size()</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>        <span class="co"># x = x.unsqueeze(-1).expand(-1, -1, self.embedding.size(1)) # (batch_size, seq_len, d)</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>        <span class="co"># return torch.gather(self.embedding.unsqueeze(0).expand(batch_size, -1, -1), 1, x) # (batch_size, seq_len, d)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>在代码中，我们定义了一个简单的嵌入层 <code>navie_embedding</code>，它接受词汇表大小 <code>v</code> 和嵌入维度 <code>d</code> 作为参数。我们初始化了一个嵌入矩阵 <code>self.embedding</code>，并在前向传播中通过<strong>索引</strong>、<strong>One-Hot 编码</strong>或 <strong><code>gather</code> 函数</strong>来获取对应的嵌入向量。</p>
<p>在实际应用中，我们通常会使用 PyTorch 提供的 <code>nn.Embedding</code> 类来简化这一过程：</p>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> WordEmbedding(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, d_model):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb5-5"><a href="#cb5-5"></a>    </span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>在 Transformer 中，词嵌入层不仅用于将输入词汇转换为向量表示，还用于将解码器的输出词汇转换为向量表示。为了保持输入和输出的一致性，Transformer 采用了<strong>Weight Tying</strong>的策略: 即Output Layer的权重矩阵与Embedding Layer的权重矩阵共享:</p>
<p><span id="eq-weight-tying"><span class="math display">\[
\text{Output Layer Weight} = \text{Embedding Layer Weight}^\top
\tag{6}\]</span></span></p>
<p>并且在初始化时，对嵌入向量进行了缩放处理，即乘以 <span class="math inline">\(\sqrt{d_{model}}\)</span>，以确保嵌入向量的尺度适合后续的注意力计算</p>
<blockquote class="blockquote">
<p>In our model, <u>we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation</u>. In the embedding layers, we multiply those weights by <span class="math inline">\(\sqrt{d_{model}}\)</span>. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
</section>
<section id="sec-postion-embedding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-postion-embedding"><span class="header-section-number">2.2</span> Position Embedding Layer</h2>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>Transformer模型中没有使用RNN或CNN，因此缺乏对序列中词汇顺序的建模能力。也就是说，Transformer是 <u>Permutation Invariant</u> 的模型，它无法区分输入序列中词汇的顺序。为了解决这个问题，Transformer引入了位置编码（Position Embedding）来注入位置信息，使模型能够感知词汇在序列中的位置。 其中，位置编码有两种主要的方法: <strong>绝对位置编码</strong>和<strong>相对位置编码</strong>。在原始的Transformer论文中，使用的是<strong>绝对位置编码</strong>:</p>
<p><span id="eq-position-embedding"><span class="math display">\[
\begin{split}
PE_{(pos, 2i)} &amp; = \sin (pos / 10,000^{2i / d_{model}}) \\
PE_{(pos, 2i+1)} &amp; = \cos (pos / 10,000^{2i+1 / d_{model}})
\end{split}
\tag{7}\]</span></span></p>
<p>其中，<span class="math inline">\(pos\)</span> 是词汇在序列中的位置，<span class="math inline">\(i \in [0, d_{model} / 2 )\)</span> 是嵌入维度的索引，<span class="math inline">\(d_{model}\)</span> 是嵌入维度的大小。通过这种方式，我们可以为每个位置生成一个唯一的向量表示。</p>
<p>仔细观察上面的公式，我们可以发现:</p>
<ul>
<li>位置编码的维度与词汇嵌入的 <span class="hilite-purple">维度相同</span>，这样可以 <span class="hilite-purple">方便地将两者相加</span>。</li>
<li>使用正弦和余弦函数可以确保不同位置的编码具有不同的频率，从而捕捉到不同的位置信息。</li>
<li>这种方法还具有一个优点，即它可以推广到比训练时更长的序列，因为位置编码是基于位置计算的，而不是依赖于具体的词汇。</li>
</ul>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question: 为什么是与Word Vector相加，而不是相乘或者 concat 呢？</p>
</div>
<div class="question-container foldable-content">
<p>如果用相乘 <span class="math inline">\(\odot\)</span>, 那么位置编码中为0的维度会直接将词向量的对应维度置为0，导致信息丢失。</p>
<p>如果用concat, 那么词向量和位置编码的维度会增加一倍，导致后续的Attention计算复杂度增加，同时也会改变模型的参数规模，影响训练效果。</p>
<p>用相加的方式，可以保持词向量的维度不变，同时将位置信息注入到词向量中，使得模型能够同时利用词汇信息和位置信息进行学习。</p>
</div>
</div>
<p>我们来仔细看一下<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>，假设我们固定位置 <span class="math inline">\(pos=1\)</span>，并且嵌入维度 <span class="math inline">\(d_{model}=6\)</span>，我们可以计算出对应的位置信息：</p>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>pos <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>d_model <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>pe <span class="op">=</span> torch.zeros(d_model)</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d_model <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb6-5"><a href="#cb6-5"></a>    pe[<span class="dv">2</span> <span class="op">*</span> i] <span class="op">=</span> torch.sin(pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (<span class="dv">2</span> <span class="op">*</span> i <span class="op">/</span> d_model)))</span>
<span id="cb6-6"><a href="#cb6-6"></a>    pe[<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> torch.cos(pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> d_model)))</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="bu">print</span>(pe)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>计算的方式很简单。接下来，我们来看一下在Transformer中，我们如何实现它。在实际的实现当中，会利用一些数学的技巧来防止Overflow:</p>
<p><span id="eq-prevent-overflow"><span class="math display">\[
\frac{1}{(10000^{2i/d_{model}})} = e^{\ln(10000^{- 2i/d_{model}})} = e^{-(2i/d_{model}) \cdot \ln(10000)}
\tag{8}\]</span></span></p>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>) <span class="co"># (max_len, 1)</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>        i <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)  <span class="co"># (d_model/2,)</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>        div_term <span class="op">=</span> torch.exp(i <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model)) <span class="co"># (d_model/2,)</span></span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb7-10"><a href="#cb7-10"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb7-11"><a href="#cb7-11"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb7-12"><a href="#cb7-12"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># (1, max_len, d_model)</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-16"><a href="#cb7-16"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-pos-embedding" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pos-embedding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pos-embedding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pos-embedding-low-dimension" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pos-embedding-low-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/pos-embedding-low-dimension-viz.png" class="img-fluid figure-img" data-ref-parent="fig-pos-embedding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pos-embedding-low-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Display Position Embedding in Low Dimension, with sequence length 100
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pos-embedding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pos-embedding-high-dimension" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pos-embedding-high-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/low-dim-viz-18-19.png" class="img-fluid figure-img" data-ref-parent="fig-pos-embedding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pos-embedding-high-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pos-embedding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Display Position Embedding in High Dimension, with sequence length 100
</figcaption>
</figure>
</div>
<p>从 <a href="#fig-pos-embedding" class="quarto-xref">Figure&nbsp;2</a> 中我们可以看到：Sinusoidal PE 是一个「多尺度表示」，<span class="hilite-blue">不同的维度对应不同的频率，从而捕捉到不同的位置信息</span>：</p>
<ul>
<li>低维 → 高频 → 局部、精细位置信息</li>
<li>高维 → 低频 → 全局、长距离位置信息</li>
</ul>
<section id="why-sinusoidal-position-embedding" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="why-sinusoidal-position-embedding"><span class="header-section-number">2.2.1</span> Why Sinusoidal Position Embedding?</h3>
<blockquote class="blockquote">
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. We also experimented with using learned positional embeddings. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>论文中提到的第一个好处就是：<span class="hilite-blue">相对位置编码</span>。假设我们有两个位置 <span class="math inline">\(pos\)</span> 和 <span class="math inline">\(pos + k\)</span>，其中 <span class="math inline">\(k\)</span> 是一个固定的偏移量。那么根据<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>，我们可以表示为: <span id="eq-pos-embedding-relative-1"><span class="math display">\[
\begin{split}
PE_{(pos \textcolor{orange}{+ k}, 2i:2i+1)}
&amp;=
\begin{bmatrix}
\sin\big((pos+k)\,\omega_i\big)\\
\cos\big((pos+k)\,\omega_i\big)
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
\sin(pos\,\omega_i)\cos(k\,\omega_i)+\cos(pos\,\omega_i)\sin(k\,\omega_i)\\
\cos(pos\,\omega_i)\cos(k\,\omega_i)-\sin(pos\,\omega_i)\sin(k\,\omega_i)
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix}
\textcolor{orange}{
    \begin{bmatrix}
    \sin(pos\,\omega_i)\\
    \cos(pos\,\omega_i)
    \end{bmatrix}
} \\
&amp;= \begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix} \textcolor{orange}{PE_{(pos, 2i:2i+1)}}
\end{split}
\tag{9}\]</span></span></p>
<p>其中， <span class="math inline">\(\omega_i = 1 / 10,000^{2i/d_{model}}\)</span>。我们可以看到，位置 <span class="math inline">\(pos + k\)</span> 的编码可以表示为位置 <span class="math inline">\(pos\)</span> 的编码通过一个线性变换得到的结果。这意味着模型可以通过学习这个<strong>线性变换</strong>来捕捉相对位置关系。 <span class="math inline">\(\begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix}\)</span> 是一个旋转矩阵，表示在二维空间中的旋转变换.</p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Rotate Matrix</p>
</div>
<div class="note-container foldable-content">
<p>Rotate Matrix 是一种二维空间中的线性变换，用于表示点绕原点旋转一定角度的操作。对于一个角度 <span class="math inline">\(\theta\)</span>，其旋转矩阵定义如下: <span class="math display">\[
R(\theta) = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta)\\
\sin(\theta) &amp; \cos(\theta)
\end{bmatrix}
\]</span></p>
<p>当我们将一个二维向量 <span class="math inline">\(v = \begin{bmatrix} x \\ y \end{bmatrix}\)</span> 乘以旋转矩阵 <span class="math inline">\(R(\theta)\)</span> 时，得到的新向量 <span class="math inline">\(v'\)</span> 表示原始向量绕原点旋转了 <span class="math inline">\(\theta\)</span> 角度: <span class="math display">\[
v' = R(\theta) \cdot v = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta)\\
\sin(\theta) &amp; \cos(\theta)
\end{bmatrix} \cdot \begin{bmatrix}
x \\ y
\end{bmatrix} = \begin{bmatrix}
x \cos(\theta) - y \sin(\theta) \\
x \sin(\theta) + y \cos(\theta)
\end{bmatrix}
\]</span></p>
<p>之后我们要学习的RoPE <span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span>，也是基于这个性质来设计的。</p>
</div>
</div>
<hr>
<p>第二个好处是：<span class="hilite-blue">可推广性</span>。由于位置编码是基于位置计算的，而不是依赖于具体的词汇，因此模型可以推广到比训练时更长的序列。这对于处理长文本或长序列任务非常有用。</p>
<blockquote class="blockquote">
<p>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>假设我们在训练时，模型见过的最大序列长度是 <span class="math inline">\(L_{train}\)</span>，那么在测试时，如果遇到一个更长的序列，长度为 <span class="math inline">\(L_{test} &gt; L_{train}\)</span>，我们仍然可以使用相同的公式来计算位置编码: <span id="eq-pos-embedding-generalization"><span class="math display">\[
PE_{(pos, 2i)} = \sin (pos / 10,000^{2i / d_{model}}), \quad pos \in [0, L_{test}-1]
\tag{10}\]</span></span></p>
<p>不过个人认为，可拓展的还有一个原因是：<u>由于 (<span class="math inline">\(pos+k\)</span>) 的编码可以表示为仅依赖 <span class="math inline">\(k\)</span> 的线性变换作用在 <span class="math inline">\(pos\)</span> 的编码上，模型更容易学习“相对位移”的规律，从而在更长序列上具备一定外推能力</u>（论文用词为 may allow，表示倾向性而非严格保证）。</p>
</section>
</section>
<section id="sec-attention" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-attention"><span class="header-section-number">2.3</span> Attention Layer</h2>
<p>Attention机制是Transformer的核心组件，它允许模型在处理序列时动态地关注输入序列中的不同部分。Attention机制的基本思想是通过计算查询（Query）、键（Key）和值（Value）之间的相似性<a href="#eq-dot-product-matrix" class="quarto-xref">Equation&nbsp;3</a> 来决定如何加权输入信息。具体来说，Attention的计算过程如下:</p>
<p><span id="eq-attention"><span class="math display">\[
\boxed{\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V}
\tag{11}\]</span></span></p>
<p>用代码来表示就是:</p>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> scaled_dot_product_attention(q, k, v):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    d_k <span class="op">=</span> q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3"></a>    scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb8-4"><a href="#cb8-4"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a>    output <span class="op">=</span> torch.matmul(attn, v)</span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="cf">return</span> output, attn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>我们来拆看看一下这个公式，由四部分组成:</p>
<ol type="1">
<li><span class="math inline">\(Q K^\top\)</span>: 计算Query和Key之间的点积，得到相似性矩阵，表示每个查询与所有键的相关性。</li>
<li><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>: 这是一个缩放因子，用于防止点积值过大，导致Softmax函数的梯度变得非常小，从而影响模型的训练效果。这里，<span class="math inline">\(d_k\)</span> 是键向量的维度。</li>
<li><span class="math inline">\(\text{softmax}(\cdot)\)</span>: 对相似性矩阵进行归一化，得到每个查询对所有键的注意力权重。</li>
<li><span class="math inline">\(\cdot V\)</span>: 使用注意力权重对值进行加权求和，得到最终的输出表示。</li>
</ol>
<hr>
<p><tag style="color:blue"><span class="math inline">\(Q K^\top\)</span></tag></p>
<p>Attention的第一步，就是计算Query和Key之间的点积 (<a href="#eq-dot-product-matrix" class="quarto-xref">Equation&nbsp;3</a>) ，得到相似性矩阵, 表示每个查询与所有键的相关性。假设我们有一个查询矩阵 <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span> 和一个键矩阵 <span class="math inline">\(K \in \mathbb{R}^{m \times d_k}\)</span>，那么点积矩阵 <span class="math inline">\(Q K^\top\)</span> 的计算过程如下:</p>
<p><span id="eq-attention-dot-product-matrix"><span class="math display">\[
Q K^\top = \begin{bmatrix}
q_1 \\
q_2 \\
\vdots \\
q_n
\end{bmatrix}
\begin{bmatrix}
k_1^\top &amp; k_2^\top &amp; \cdots &amp; k_m^\top
\end{bmatrix} =
\begin{bmatrix}
q_1 k_1^\top &amp; q_1 k_2^\top &amp; \cdots &amp; q_1 k_m^\top \\
q_2 k_1^\top &amp; q_2 k_2^\top &amp; \cdots &amp; q_2 k_m^\top \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_n k_1^\top &amp; q_n k_2^\top &amp; \cdots &amp; q_n k_m^\top
\end{bmatrix}
\tag{12}\]</span></span></p>
<p>其中，<span class="math inline">\(q_i \in \mathbb{R}^{1 \times d_k}\)</span> 是查询矩阵 <span class="math inline">\(Q\)</span> 的第 <span class="math inline">\(i\)</span> 行，<span class="math inline">\(k_j \in \mathbb{R}^{1 \times d_k}\)</span> 是键矩阵 <span class="math inline">\(K\)</span> 的第 <span class="math inline">\(j\)</span> 行。结果矩阵 <span class="math inline">\(Q K^\top \in \mathbb{R}^{n \times m}\)</span> 的每个元素 <span class="math inline">\((i, j)\)</span> 表示查询 <span class="math inline">\(q_i\)</span> 与键 <span class="math inline">\(k_j\)</span> 之间的点积。 <span class="math inline">\(QK^\top\)</span> 的作用就是告诉我们，<u>每个查询向量与所有键向量之间的相似性</u>，用于之后从值向量中提取相关信息。</p>
<blockquote class="blockquote">
<p>Dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. <cite> Attention is all you need, p.4 </cite></p>
</blockquote>
<hr>
<p><tag style="color:blue"><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span></tag></p>
<p>Attention的第二步，是对点积矩阵进行缩放，使用 <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> 作为缩放因子。假设 <span class="math inline">\(q, k \sim \mathcal{N}(0, I)\)</span>，那么点积 <span class="math inline">\(q k^\top\)</span> 的期望和方差分别为:</p>
<p><span id="eq-attention-dot-product-variance"><span class="math display">\[
\mathbb{E}[q k^\top] = 0, \quad \text{Var}(q k^\top) = d_k
\tag{13}\]</span></span></p>
<p>我们可以看到，点积的方差与键向量的维度 <span class="math inline">\(d_k\)</span> 成正比。随着 <span class="math inline">\(d_k\)</span> 的增加，点积 logits 的尺度会不断放大，使得 softmax 的输入更容易进入饱和区（saturation regime），此时某些位置的概率接近 1，其余接近 0。在该区域内，softmax 的梯度会显著变小，从而导致反向传播不稳定、训练效率下降。通过除以 <span class="math inline">\(\sqrt{d_k}\)</span>，可以将点积 logits 的方差重新归一化到 <span class="math inline">\(O(1)\)</span> 的尺度，使 softmax 始终工作在梯度较为敏感的区域，从而稳定训练过程。</p>
<div id="fig-scaling-d-k" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-d-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/scaling-d_k.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-d-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Effect of different scaling factor <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> on the distribution of dot-product values。可以看到，未缩放的点积值分布在几个点上，随着维度增加，分布变得更分散，导致softmax更容易饱和。引入缩放因子后，点积值分布保持稳定，有助于softmax的稳定性。
</figcaption>
</figure>
</div>
<p>因此这个有时候我们称之为 “Scale Dot-Product Attention”。通过引入缩放因子 <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>，我们可以将点积的方差控制在一个合理的范围内，从而稳定Softmax函数的输出，改善模型的训练效果。</p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE Gradient of Softmax</p>
</div>
<div class="note-container foldable-content">
<p><span class="math display">\[
\frac{\partial \,\text{softmax}_i}{\partial z_j} = \text{softmax}_i (\delta_{ij} - \text{softmax}_j)
\]</span></p>
<p>其中， <span class="math inline">\(\delta_{ij}\)</span> 是 <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker Delta</a>，当 <span class="math inline">\(i=j\)</span> 时为1，否则为0。</p>
<p>当某一项 <span class="math inline">\(\text{softmax}_i \approx 1\)</span> 时：</p>
<ul>
<li><span class="math inline">\(\text{softmax}_i (1 - \text{softmax}_i) \approx 0\)</span></li>
<li>其他项 <span class="math inline">\(\text{softmax}_j \approx 0\)</span></li>
</ul>
<p>所以所有的梯度都接近于0</p>
</div>
</div>
<div id="fig-scaling-d-k-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-d-k-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/scaling-d-k-gradient.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-d-k-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Gradient of Softmax with different scaling factor <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>
</figcaption>
</figure>
</div>
<hr>
<p><tag style="color:blue">第三项 <span class="math inline">\(\text{softmax}(\cdot)\)</span></tag></p>
<p>Attention的第三步，是对缩放后的点积矩阵进行Softmax归一化，得到每个查询对所有键的注意力权重。假设我们有一个缩放后的点积矩阵 <span class="math inline">\(S = \frac{Q K^\top}{\sqrt{d_k}}\)</span>。 这部分很直观，我们对矩阵 <span class="math inline">\(S\)</span> 的每一行应用Softmax函数，得到注意力权重矩阵 <span class="math inline">\(A\)</span>:</p>
<p><span id="eq-attention-softmax"><span class="math display">\[
A_{ij} = \frac{e^{S_{ij}}}{\sum_{k} e^{S_{ik}}}
\tag{14}\]</span></span></p>
<p>其中，<span class="math inline">\(A_{ij}\)</span> 表示查询 <span class="math inline">\(q_i\)</span> 对键 <span class="math inline">\(k_j\)</span> 的注意力权重。通过Softmax归一化，我们确保每个查询的注意力权重之和为1，从而可以将其解释为概率分布。这些注意力权重反映了每个查询与所有键之间的相关性，<u>帮助模型动态地关注输入序列中的不同部分</u>。</p>
<hr>
<p><tag style="color:blue">第四项 <span class="math inline">\(\cdot V\)</span></tag></p>
<p>Attention的最后一步，是使用注意力权重对值进行加权求和，得到最终的输出表示。假设我们有一个值矩阵 <span class="math inline">\(V \in \mathbb{R}^{m \times d_v}\)</span> 和注意力权重矩阵 <span class="math inline">\(A \in \mathbb{R}^{n \times m}\)</span>，那么输出矩阵 <span class="math inline">\(O \in \mathbb{R}^{n \times d_v}\)</span> 的计算过程如下:</p>
<p><span id="eq-attention-weighted-sum"><span class="math display">\[
O = A V = \begin{bmatrix}
A_{11} &amp; A_{12} &amp; \cdots &amp; A_{1m} \\
A_{21} &amp; A_{22} &amp; \cdots &amp; A_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n1} &amp; A_{n2} &amp; \cdots &amp; A_{nm}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_m
\end{bmatrix} =
\begin{bmatrix}
\sum_{j=1}^{m} A_{1j} v_j \\
\sum_{j=1}^{m} A_{2j} v_j \\
\vdots \\       
\sum_{j=1}^{m} A_{nj} v_j
\end{bmatrix}
\tag{15}\]</span></span> 其中，<span class="math inline">\(v_j \in \mathbb{R}^{1 \times d_v}\)</span> 是值矩阵 <span class="math inline">\(V\)</span> 的第 <span class="math inline">\(j\)</span> 行。结果矩阵 <span class="math inline">\(O \in \mathbb{R}^{n \times d_v}\)</span> 的每一行表示对应查询的加权值向量。通过这种方式，模型能够根据注意力权重动态地聚合输入信息，从而生成更具表达力的输出表示。</p>
<div class="highlight">
<p>拆看来看，Attention也没有想象的这么复杂。它主要是通过计算查询和键之间的相似性，来决定如何加权输入的值，从而生成输出表示。</p>
<p>Attention 就像做菜：</p>
<ul>
<li>Query 决定你想做什么，</li>
<li>Key 决定每个食材的特点，</li>
<li><span class="math inline">\(QK^\top\)</span> 是把所有食材摆在桌上，看看哪些比较Match你的需求，</li>
<li><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> 是调整食材的分量，</li>
<li>Softmax 决定用多少，</li>
<li>Value 决定最终味道。</li>
</ul>
<p>它不像 RNN 或者 CNN</p>
<ul>
<li>你不是按“食材顺序”处理（不是 RNN）</li>
<li>也不是只看相邻几样（不是 CNN）</li>
<li>而是 一次性看完整桌食材，再决定重点</li>
</ul>
</div>
<section id="multi-head-attention" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">2.3.1</span> Multi-Head Attention</h3>
<p>Multi Head Attention 就是在 Attention 的基础上，<u>并行地计算多个注意力头（Attention Head）</u>，从而捕捉输入序列中的不同子空间信息。其中每一个Head，都是独立的 Self-Attention 机制。Multi-Head Attention 的计算过程如下:</p>
<p><span id="eq-multi-head-attention"><span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W^O
\tag{16}\]</span></span></p>
<p>其中，每个注意力头 <span class="math inline">\(\text{head}_i\)</span> 的计算过程如下: <span id="eq-multi-head-attention-head"><span class="math display">\[
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
\tag{17}\]</span></span></p>
<p>每个Head独立的运行，然后将所有Head的输出进行拼接（Concat），最后通过一个线性变换 <span class="math inline">\(W^O\)</span> 得到最终的输出表示。通过这种方式，Multi-Head Attention 能够同时关注输入序列中的不同部分，从而增强模型的表达能力。</p>
<blockquote class="blockquote">
<p>Multi-head attention allows the model to <u>jointly attend to information from different representation subspaces at different positions</u>. With a single attention head, averaging inhibits this. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
<div class="sourceCode" id="cb9" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig, is_causal: <span class="bu">bool</span>):</span>
<span id="cb9-3"><a href="#cb9-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>.is_causal <span class="op">=</span> is_causal</span>
<span id="cb9-6"><a href="#cb9-6"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_heads</span>
<span id="cb9-7"><a href="#cb9-7"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads</span>
<span id="cb9-8"><a href="#cb9-8"></a>        <span class="cf">assert</span> <span class="va">self</span>.head_dim <span class="op">*</span> <span class="va">self</span>.num_heads <span class="op">==</span> config.d_model, <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb9-9"><a href="#cb9-9"></a></span>
<span id="cb9-10"><a href="#cb9-10"></a>        <span class="va">self</span>.q_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-11"><a href="#cb9-11"></a>        <span class="va">self</span>.k_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-12"><a href="#cb9-12"></a>        <span class="va">self</span>.v_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-13"><a href="#cb9-13"></a>    </span>
<span id="cb9-14"><a href="#cb9-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, q, k, v):</span>
<span id="cb9-15"><a href="#cb9-15"></a>        b, q_len, _ <span class="op">=</span> q.size()</span>
<span id="cb9-16"><a href="#cb9-16"></a>        kv_len <span class="op">=</span> k.size(<span class="dv">1</span>)</span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>        <span class="co"># 通过创建view和transpose将q, k, v拆分成多个head</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>        q <span class="op">=</span> <span class="va">self</span>.q_proj(q).view(b, q_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-20"><a href="#cb9-20"></a>        k <span class="op">=</span> <span class="va">self</span>.k_proj(k).view(b, kv_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-21"><a href="#cb9-21"></a>        v <span class="op">=</span> <span class="va">self</span>.v_proj(v).view(b, kv_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="self-attention-layer" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="self-attention-layer"><span class="header-section-number">2.3.2</span> Self-Attention Layer</h3>
<p>Self-Attention, 顾名思义，是指在计算Attention时，<u>查询（Query）、键（Key）和值（Value）都来自同一个序列</u>。这种机制允许模型在处理序列时，动态地关注序列中的不同位置，从而捕捉到序列内部的依赖关系。Self-Attention的计算过程如下:</p>
<p><span id="eq-self-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
\tag{18}\]</span></span></p>
<p>在没有任何掩码的情况下，Self-Attention允许每个位置的查询向量关注序列中的所有位置，包括当前位置和未来位置的信息。这种机制使得模型能够捕捉到长距离的依赖关系，从而增强了模型的表达能力。</p>
<div class="sourceCode" id="cb10" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>out <span class="op">=</span> <span class="va">self</span>.attention(x, x, x) <span class="co"># Self-Attention</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="causal-self-attention-layer" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="causal-self-attention-layer"><span class="header-section-number">2.3.3</span> Causal Self-Attention Layer</h3>
<p><span id="eq-causal-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}} \textcolor{red}{+ M}\right) V
\tag{19}\]</span></span></p>
<p>其中，<span class="math inline">\(M \in \mathbb{R}^{n \times n}\)</span> 是一个掩码矩阵（Mask Matrix），用于阻止模型在生成序列时访问未来的信息。具体来说，掩码矩阵 <span class="math inline">\(M\)</span> 的定义如下:</p>
<p><span id="eq-mask-matrix"><span class="math display">\[
M_{ij} = \begin{cases}
-\infty, &amp; \text{if } j &gt; i \\
0, &amp; \text{otherwise}
\end{cases}
\tag{20}\]</span></span> 这个掩码矩阵确保了在计算注意力权重时，查询位置 <span class="math inline">\(i\)</span> 只能关注到键位置 <span class="math inline">\(j \leq i\)</span> 的信息，从而实现了自回归（Auto-Regressive）的特性，防止信息泄露。</p>
<div class="sourceCode" id="cb11" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">def</span> create_causal_mask(q_len: <span class="bu">int</span>, k_len: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="cf">return</span> torch.tril(torch.ones((q_len, k_len), dtype<span class="op">=</span>torch.<span class="bu">bool</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <span class="math inline">\(-\infty\)</span>) all values in the input of the softmax which correspond to illegal connections. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
<div id="fig-mask-in-attention" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-in-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mask-in-attention" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-attention-causal-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-attention-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/causal-mask.png" class="img-fluid figure-img" data-ref-parent="fig-mask-in-attention">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-attention-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Causal Mask in Attention
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mask-in-attention" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-attention-causal-padding-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-attention-causal-padding-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/causal-padding-mask.png" class="img-fluid figure-img" data-ref-parent="fig-mask-in-attention">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-attention-causal-padding-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Casual Mask with Padding Positions
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-in-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 在 <a href="#fig-attention-causal-mask" class="quarto-xref">Figure&nbsp;5 (a)</a> 中，我们可以看到掩码矩阵 <span class="math inline">\(M\)</span> 的结构。上三角部分被设置为 <span class="math inline">\(-\infty\)</span>，表示这些位置的注意力权重在经过 Softmax 归一化后将变为 0，从而阻止模型关注未来的信息。在 <a href="#fig-attention-causal-padding-mask" class="quarto-xref">Figure&nbsp;5 (b)</a> 中，<span class="math inline">\((t_5, t_6, t_7)\)</span> 是填充位置（Padding Positions），这些位置同样被掩码掉，确保模型不会关注到这些无效的信息。
</figcaption>
</figure>
</div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Padding Mask</p>
</div>
<div class="note-container foldable-content">
<p>除了Causal Mask之外, 在实际应用中, 我们还需要处理变长序列中的填充位置(Padding Positions)。这些位置通常用特殊的填充值(如0)表示, 不包含有效信息。在计算Attention时, 我们需要确保模型不会关注到这些填充位置, 因此我们引入了Padding Mask。 在计算Attention中，我们可以将Padding Mask与Causal Mask结合使用, 形成一个综合的掩码矩阵 <a href="#fig-attention-causal-padding-mask" class="quarto-xref">Figure&nbsp;5 (b)</a> 。具体来说, 对于填充位置, 我们同样将对应的注意力权重设置为 <span class="math inline">\(-\infty\)</span>，确保这些位置在经过Softmax归一化后不会被关注到。</p>
<div class="sourceCode" id="cb12" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    ...</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="kw">def</span> construct_mask(<span class="va">self</span>, pad_mask, q_len: <span class="bu">int</span>, k_len: <span class="bu">int</span>, device):</span>
<span id="cb12-4"><a href="#cb12-4"></a>        <span class="co"># True=allowed, False=masked</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>        mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a>        <span class="co"># causal mask (decoder self-attention only)</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>        <span class="cf">if</span> <span class="va">self</span>.is_causal:</span>
<span id="cb12-9"><a href="#cb12-9"></a>            causal_mask <span class="op">=</span> create_causal_mask(q_len, k_len)</span>
<span id="cb12-10"><a href="#cb12-10"></a>            causal_mask <span class="op">=</span> causal_mask.to(device)</span>
<span id="cb12-11"><a href="#cb12-11"></a>            mask <span class="op">=</span> causal_mask[<span class="va">None</span>, <span class="va">None</span>, :, :]</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a>        <span class="cf">if</span> pad_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-14"><a href="#cb12-14"></a>            <span class="co"># True means allowed</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>            pad_mask <span class="op">=</span> pad_mask[:, <span class="va">None</span>, <span class="va">None</span>, :].to(device)  <span class="co"># Shape: (batch, 1, 1, kv_len)</span></span>
<span id="cb12-16"><a href="#cb12-16"></a>            mask <span class="op">=</span> pad_mask <span class="cf">if</span> mask <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> (mask <span class="op">&amp;</span> pad_mask)</span>
<span id="cb12-17"><a href="#cb12-17"></a></span>
<span id="cb12-18"><a href="#cb12-18"></a>        <span class="cf">return</span> mask</span>
<span id="cb12-19"><a href="#cb12-19"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, q, k, v, pad_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-20"><a href="#cb12-20"></a>        ...</span>
<span id="cb12-21"><a href="#cb12-21"></a>        mask <span class="op">=</span> <span class="va">self</span>.construct_mask(pad_mask, q_len, kv_len, q.device)</span>
<span id="cb12-22"><a href="#cb12-22"></a>        out, attn <span class="op">=</span> scaled_dot_product_attention(q, k, v, mask)</span>
<span id="cb12-23"><a href="#cb12-23"></a>        ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>用代码来表示Causal Self-Attention， 我们只需要做原来的基础上，<span class="hilite-purple">在计算Softmax之前</span>，添加掩码矩阵 <span class="math inline">\(M\)</span> 即可:</p>
<div class="sourceCode" id="cb13" data-code-line-numbers="5,10,11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> scaled_dot_product_attention(</span>
<span id="cb13-2"><a href="#cb13-2"></a>    q, </span>
<span id="cb13-3"><a href="#cb13-3"></a>    k, </span>
<span id="cb13-4"><a href="#cb13-4"></a>    v, </span>
<span id="cb13-5"><a href="#cb13-5"></a>    mask<span class="op">=</span><span class="va">None</span> </span>
<span id="cb13-6"><a href="#cb13-6"></a>    ):</span>
<span id="cb13-7"><a href="#cb13-7"></a>    d_k <span class="op">=</span> q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-8"><a href="#cb13-8"></a>    scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb13-9"><a href="#cb13-9"></a>    </span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: </span>
<span id="cb13-11"><a href="#cb13-11"></a>        scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>)) </span>
<span id="cb13-12"><a href="#cb13-12"></a>        </span>
<span id="cb13-13"><a href="#cb13-13"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-14"><a href="#cb13-14"></a>    output <span class="op">=</span> torch.matmul(attn, v)</span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="cf">return</span> output, attn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>其中<code>mask</code>参数是 Padding Mask 与 Causal Mask 的结合。</p>
</section>
<section id="cross-attention-layer" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="cross-attention-layer"><span class="header-section-number">2.3.4</span> Cross Attention Layer</h3>
<p>Cross-Attention 是指在计算 Attention 时，<u>查询（Query）来自解码器的输入序列，而键（Key）和值（Value）来自编码器的输出序列</u>。这种机制允许解码器在生成输出序列时，动态地关注输入序列中的不同部分，从而捕捉到输入和输出之间的依赖关系。Cross-Attention 的计算过程如下:</p>
<p><span id="eq-cross-attention"><span class="math display">\[
\text{Attention}(Q_{dec}, \textcolor{red}{K_{enc}}, \textcolor{red}{V_{enc}}) = \text{softmax}\left(\frac{Q_{dec} \textcolor{red}{K_{enc}^\top}}{\sqrt{d_k}}\right) \textcolor{red}{V_{enc}}
\tag{21}\]</span></span></p>
<div class="sourceCode" id="cb14" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>out <span class="op">=</span> <span class="va">self</span>.attention(decoder_x, encoder_x, encoder_x) <span class="co"># Cross-Att</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-attention-complexity" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="sec-attention-complexity"><span class="header-section-number">2.3.5</span> Time Complexity of Attention</h3>
<p>接下来，我们来分析一下 Self-Attention 的时间复杂度。假设输入序列的长度为 <span class="math inline">\(n\)</span>，嵌入维度为 <span class="math inline">\(d\)</span>，那么 Self-Attention 的时间复杂度主要包括以下几个部分:</p>
<ol type="1">
<li>计算点积矩阵 <span class="math inline">\(Q K^\top\)</span> 的时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>，因为我们需要对每个查询向量与所有键向量进行点积计算，共有 <span class="math inline">\(n\)</span> 个查询和 <span class="math inline">\(n\)</span> 个键，每个点积计算的时间复杂度为 <span class="math inline">\(\mathcal{O}(d)\)</span>。</li>
<li>计算 Softmax 的时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2)\)</span>，因为我们需要对每个查询向量的点积结果进行归一化，共有 <span class="math inline">\(n\)</span> 个查询，每个查询需要对 <span class="math inline">\(n\)</span> 个键进行归一化。</li>
<li>计算加权和 $ V$ 的时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>，因为我们需要对每个查询向量与所有值向量进行加权求和， 共有 <span class="math inline">\(n\)</span> 个查询和 <span class="math inline">\(n\)</span> 个值，每个加权求和的时间复杂度为 <span class="math inline">\(\mathcal{O}(d)\)</span>。</li>
</ol>
<p><span id="eq-self-attention-time-complexity"><span class="math display">\[
\begin{array}{|l|l|}
\hline
\textbf{Step} &amp; \textbf{Time Complexity} \\
\hline
QK^\top &amp; \mathcal{O}(n^2 d) \\
\text{softmax}(QK^\top) &amp; \mathcal{O}(n^2) \\
\text{attention} \times V &amp; \mathcal{O}(n^2 d) \\
\hline
\textbf{Total} &amp; \mathcal{O}(n^2 d) \\
\hline
\end{array}
\tag{22}\]</span></span></p>
<p>综上所述，Self-Attention 的总时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>。随着输入序列长度 <span class="math inline">\(n\)</span> 的增加，时间复杂度呈二次增长，这可能会导致在处理长序列时计算开销较大。因此，在实际应用中，研究人员提出了各种优化方法，如稀疏注意力（Sparse Attention）、局部注意力（Local Attention）等，以降低 Self-Attention 的时间复杂度，提高模型的效率。</p>
<div class="warning foldable is-open">
<div class="warning-header foldable-header">
<p>Warning: 理解Attention Complexity的重要性</p>
</div>
<div class="warning-container foldable-content">
<p>理解Attention的Complexity很重要, 因为它直接影响到Transformer模型的效率和可扩展性。也就是说, 当处理长序列时, Attention的计算复杂度会显著增加, 这可能导致训练和推理的时间成本变得非常高。因此, 研究人员提出了各种优化方法, 如稀疏注意力（Sparse Attention）、局部注意力（Local Attention），Linear Attention，Flash Attention，包括Deep Sparse Attention等，都是为了降低Attention的计算复杂度，从而提升Transformer在处理长序列时的效率和性能。可以说，理解了Attention的Complexity, 就理解了Transformer的效率瓶颈所在。</p>
</div>
</div>
<div id="fig-time-complexity-of-self-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-time-complexity-of-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/time-complexity-of-self-attention.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-time-complexity-of-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Discussion on Time Complexity and Maximum Sequence Length of Self-Attention
</figcaption>
</figure>
</div>
<div class="columns">
<div class="column">
<p><img src="./assets/wait-meme.png" class="img-fluid" style="width:90.0%" data-align="center"></p>
</div><div class="column">
<p><span class="hilite-pink">等一等，稳一稳，忍一忍</span></p>
<p>RNN的时间复杂度是<span class="math inline">\(\mathcal{O}(n d^2)\)</span>，Transformer的时间复杂度是 <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>，那RNN不是更快吗？</p>
<p>不一定! Transformer 往往更快的关<u>键不在于把总复杂度变成 <span class="math inline">\(\mathcal{O}(1)\)</span>，而在于把“序列维度上的计算”从必须串行，变成可并行的矩阵运算也就是<a href="#fig-time-complexity-of-self-attention" class="quarto-xref">Figure&nbsp;6</a>里的 <em>Sequential Operations</em> 对比）</u>。因此在 GPU/TPU 上，Transformer 的吞吐通常更高。</p>
<ol type="1">
<li><strong>并行计算 / 并行深度（critical path）</strong>：RNN 存在严格的时间步依赖，必须按步计算，导致并行深度随序列长度线性增长（<span class="math inline">\(\mathcal{O}(n)\)</span>）；而 Self-Attention 在一个层内可以用几次矩阵乘法同时处理所有位置，因此并行深度是常数级<span class="math inline">\(\mathcal{O}(1)\)</span>。</li>
<li><strong>瓶颈不同（<span class="math inline">\(d\)</span> vs <span class="math inline">\(n\)</span>）</strong>：RNN 对隐藏维 <span class="math inline">\(d\)</span> 的主要成本是二次（<span class="math inline">\(\mathcal{O}(n d^2)\)</span>），而 attention 对 <span class="math inline">\(d\)</span> 近似一次、但对序列长度 <span class="math inline">\(n\)</span> 是二次（<span class="math inline">\(\mathcal{O}(n^2 d)\)</span>）。所以当序列非常长时，attention 的 <span class="math inline">\(n^2\)</span> 会成为瓶颈，实践中常用
<ul>
<li>FlashAttention<span class="citation" data-cites="FlashAttentionFastMemoryEfficient2022dao">(<a href="#ref-FlashAttentionFastMemoryEfficient2022dao" role="doc-biblioref">Dao et al. 2022</a>)</span> (优化常数与显存/IO)</li>
<li><strong>Window / restricted attention</strong>（将全局注意力改为局部窗口，类似图<a href="#fig-time-complexity-of-self-attention" class="quarto-xref">Figure&nbsp;6</a> 中的“restricted self-attention”那一行）来进一步提升长序列效率。</li>
</ul></li>
</ol>
</div>
</div>
<p>一句话总结Attention就是:</p>
<div class="highlight">
<p><tag style="color:green">Attention is Weighted Sum of Values, where Weights are from Softmax of Scaled Dot-Product of Queries and Keys.</tag></p>
</div>
</section>
</section>
<section id="sec-normalization" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-normalization"><span class="header-section-number">2.4</span> Normalization Layer</h2>
<p>Layer Normalization <span class="citation" data-cites="LayerNormalization2016ba">(<a href="#ref-LayerNormalization2016ba" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span> 是一种用于深度神经网络的归一化技术，旨在提高训练的稳定性和速度。与批量归一化（Batch Normalization）不同，Layer Normalization 是在<u>每个样本的特征维度上</u> (<span class="math inline">\(d_{model}\)</span>) 进行归一化，而不是在批量维度上进行归一化。这使得 Layer Normalization 特别适用于循环神经网络（RNN）和 Transformer 等模型。</p>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question：为什么Layer Normalization更适合Sequence Modeling？</p>
</div>
<div class="question-container foldable-content">
<ol type="1">
<li><strong>序列长度变化</strong>: 在处理变长序列时，批量归一化可能会受到不同长度序列的影响，而 Layer Normalization 可以独立于序列长度进行归一化。</li>
<li><strong>时间步依赖</strong>: 在 RNN 中，时间步之间存在依赖关系，批量归一化可能会破坏这种依赖关系，而 Layer Normalization 保持了时间步之间的独立性。</li>
<li><strong>小批量大小</strong>: 在某些任务中，批量大小可能非常小，甚至为1，这使得批量归一化效果不佳，而 Layer Normalization 不依赖于批量大小。</li>
</ol>
</div>
</div>
<p>Layer Normalization 的计算过程如下:</p>
<p><span id="eq-layer-normalization"><span class="math display">\[
\begin{split}
\mu &amp; = \frac{1}{d} \sum_{i=1}^{d} x_i \\
\sigma^2 &amp; = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2 \\
\hat{x_i} &amp; = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
y_i &amp; = \gamma \odot \hat{x_i} + \beta
\end{split}
\tag{23}\]</span></span></p>
<p>其中，<span class="math inline">\(x_i\)</span> 是输入向量的第 <span class="math inline">\(i\)</span> 个元素，<span class="math inline">\(d\)</span> 是向量的维度，<span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma^2\)</span> 分别是均值和方差，<span class="math inline">\(\epsilon\)</span> 是一个小常数，用于防止除零错误，<span class="math inline">\(\gamma \in \mathbb{R}^d\)</span> 和 <span class="math inline">\(\beta \in \mathbb{R}^d\)</span> 是可学习的参数，用于缩放和平移归一化后的输出。</p>
<div class="sourceCode" id="cb15" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span>, eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb15-3"><a href="#cb15-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-4"><a href="#cb15-4"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb15-5"><a href="#cb15-5"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(dim))</span>
<span id="cb15-6"><a href="#cb15-6"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(dim))</span>
<span id="cb15-7"><a href="#cb15-7"></a></span>
<span id="cb15-8"><a href="#cb15-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb15-9"><a href="#cb15-9"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-10"><a href="#cb15-10"></a>        var <span class="op">=</span> x.var(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-11"><a href="#cb15-11"></a></span>
<span id="cb15-12"><a href="#cb15-12"></a>        x_hat <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> torch.sqrt(var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb15-13"><a href="#cb15-13"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> x_hat <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question: 为什么 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 是必要的? 并且要初始化为1和0?</p>
</div>
<div class="question-container foldable-content">
<p>为什么是必要的？</p>
<p>在Normalization之后 <span class="math inline">\(\hat{x_i}\)</span> 是归一化的输出, 其均值为0，方差为1。这很稳定，但也有一个副作用：模型失去了“想要多大尺度/什么均值”的自由度。如果没有 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span>，模型将失去对原始数据分布的表达能力。通过引入 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span>，模型可以学习到适合当前任务的缩放和平移，从而恢复或调整数据的分布。</p>
<p>为什么初始化为1和0？</p>
<p>初始化成 <span class="math inline">\(\gamma=1, \beta=0\)</span> 时，<span class="math inline">\(y_i = \hat{x_i}\)</span>，即初始时 Layer Normalization 的输出与归一化后的输入相同。这种初始化方式确保了在训练开始时，Layer Normalization 不会对数据进行任何缩放或平移，从而避免了对模型训练的干扰。随着训练的进行，模型可以根据需要调整 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 的值，以适应具体任务的需求。</p>
</div>
</div>
</section>
<section id="sec-feed-forward" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-feed-forward"><span class="header-section-number">2.5</span> Feed Forward Layer</h2>
<p>在Transformer中，前馈神经网络（Feed Forward Network, FFN）是每个编码器和解码器层中的一个重要组成部分。它的主要作用是对每个位置的表示进行非线性变换，从而增强模型的表达能力。前馈神经网络通常由两个线性变换和一个非线性激活函数组成，具体计算过程如下: <span id="eq-feed-forward-network"><span class="math display">\[
\text{FFN}(\mathrm{x}) = \underset{}{\max} (0, \mathrm{x} W_{1} + b_{1}) W_{2} + b_{2}
\tag{24}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathrm{x} \in \mathbb{R}^{d_{model}}\)</span> 是输入向量，<span class="math inline">\(W_{1} \in \mathbb{R}^{d_{model} \times d_{ff}}\)</span> 和 <span class="math inline">\(W_{2} \in \mathbb{R}^{d_{ff} \times d_{model}}\)</span> 是权重矩阵，<span class="math inline">\(b_{1} \in \mathbb{R}^{d_{ff}}\)</span> 和 <span class="math inline">\(b_{2} \in \mathbb{R}^{d_{model}}\)</span> 是偏置向量，<span class="math inline">\(d_{ff}\)</span> 是前馈网络的隐藏层维度，通常大于 <span class="math inline">\(d_{model}\)</span>，在原始的Transformer论文中，<span class="math inline">\(d_{ff}\)</span> 通常设置为 <span class="math inline">\(4 \times d_{model}\)</span>。</p>
<p>为什么要使用前馈神经网络？主要有以下几个原因:</p>
<ol type="1">
<li><strong>非线性变换</strong>: 前馈神经网络引入了非线性激活函数（如ReLU），使模型能够学习复杂的非线性关系，从而增强了模型的表达能力。</li>
<li><strong>位置独立性</strong>: 前馈神经网络对每个位置的表示进行独立的变换，这有助于模型捕捉每个位置的特征，而不受其他位置的影响。</li>
<li><strong>增加模型容量</strong>: 通过增加前馈神经网络的隐藏层维度 <span class="math inline">\(d_{ff}\)</span>，可以显著增加模型的容量，从而提升模型的性能。</li>
</ol>
<div class="sourceCode" id="cb16" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="kw">class</span> FFN(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb16-3"><a href="#cb16-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-4"><a href="#cb16-4"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(config.d_model, config.d_ff)</span>
<span id="cb16-5"><a href="#cb16-5"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(config.d_ff, config.d_model)</span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-8"><a href="#cb16-8"></a>        <span class="cf">return</span> <span class="va">self</span>.fc2(F.relu(<span class="va">self</span>.fc1(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="residual-connection" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="residual-connection"><span class="header-section-number">2.6</span> Residual Connection</h2>
<p>当然，如果要训练一个DEEP Transformer模型，避不开的就是Residual Connection <span class="citation" data-cites="DeepResidualLearning2015he">(<a href="#ref-DeepResidualLearning2015he" role="doc-biblioref">He et al. 2015</a>)</span>, 它的作用是缓解深层网络中的梯度消失问题，从而使得更深层的网络能够被有效训练。其基本思想是通过引入跳跃连接（Skip Connection），将输入直接添加到输出上，从而形成一个“捷径”，使得梯度可以直接传递到更早的层。具体来说，假设我们有一个子层（Sublayer），其输入为 <span class="math inline">\(\mathbf   {x}\)</span>，输出为 <span class="math inline">\(\mathrm{Sublayer}(\mathbf{x})\)</span>，那么引入残差连接后的输出 <span class="math inline">\(\mathbf{y}\)</span> 可以表示为:</p>
<p><span class="math display">\[
\mathbf{y} = \text{LayerNorm}(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x}))
\]</span></p>
<section id="backpropagation-through-residual-connection" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="backpropagation-through-residual-connection"><span class="header-section-number">2.6.1</span> Backpropagation through Residual Connection</h3>
<p>接下来，我们来简单分析一下，残差连接是如何帮助缓解梯度消失问题的。假设我们有一个损失函数 <span class="math inline">\(\mathcal{L}\)</span>，</p>
<p><span id="eq-residual-connection"><span class="math display">\[
\mathbf{y} = \mathbf{x} + \mathrm{Sublayer}(\mathbf{x})
\tag{25}\]</span></span></p>
<p>我们想要计算损失函数对输入 <span class="math inline">\(\mathbf{x}\)</span> 的梯度 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span>。根据链式法则，我们可以得到:</p>
<p><span id="eq-backprop-residual"><span class="math display">\[
\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial}{\partial \mathbf{x}}\left(\mathbf{x}+\mathrm{Sublayer}(\mathbf{x})\right) \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left(\frac{\partial \mathbf{x}}{\partial \mathbf{x}} +
\frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left( \mathbf{I} + \frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}_{\text{straight path}} +
\underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot
\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}}
\end{split}
\tag{26}\]</span></span></p>
<p>其中，<span class="math inline">\(y\)</span> 是残差连接的输出，<span class="math inline">\(\mathbf{I}\)</span> 是单位矩阵。可以看到，梯度 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span> 包含了两部分:</p>
<ul>
<li><strong>Straight Path</strong>: 这部分梯度直接来自于损失函数对输出 <span class="math inline">\(\mathbf{y}\)</span> 的梯度 <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\)</span>，它不经过任何子层的变换，因此不会受到梯度消失的影响。</li>
<li><strong>Through the Sub-layer</strong>: 这部分梯度通过子层的变换传播，可能会受到梯度消失的影响。</li>
</ul>
<p>通过引入残差连接，模型可以确保梯度在反向传播过程中至少有一部分（Straight Path）能够直接传递到更早的层，从而缓解了梯度消失的问题。这使得深层网络能够被有效训练，从而提升了模型的性能。</p>
</section>
</section>
<section id="sec-output-layer" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="sec-output-layer"><span class="header-section-number">2.7</span> Output Layer</h2>
<p>在Transformer的输出层，通常会使用一个线性层（Linear Layer）将解码器的输出转换为词汇表大小的向量，然后通过Softmax函数<a href="#eq-softmax" class="quarto-xref">Equation&nbsp;1</a> 将其转换为概率分布，从而生成最终的预测结果。具体来说，假设解码器的输出为 <span class="math inline">\(\mathbf{h} \in \mathbb{R}^{d_{model}}\)</span>，词汇表大小为 <span class="math inline">\(V\)</span>，那么输出层的计算过程如下:</p>
<p><span id="eq-output-layer"><span class="math display">\[
\mathbf{y} = \text{Softmax}(\mathbf{h} W_{o} + b_o)
\tag{27}\]</span></span></p>
<p>其中，<span class="math inline">\(W_{o} \in \mathbb{R}^{d_{model} \times V}\)</span> 是线性层的权重矩阵，<span class="math inline">\(b_o \in \mathbb{R}^{V}\)</span> 是偏置向量，<span class="math inline">\(\mathbf{y} \in \mathbb{R}^{V}\)</span> 是最终的预测结果，表示每个词汇的概率分布。</p>
<div class="sourceCode" id="cb17" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb17-2"><a href="#cb17-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb17-3"><a href="#cb17-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-4"><a href="#cb17-4"></a>        ...</span>
<span id="cb17-5"><a href="#cb17-5"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.d_model, config.tgt_vocab_size)</span>
<span id="cb17-6"><a href="#cb17-6"></a>    </span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, original, target):</span>
<span id="cb17-8"><a href="#cb17-8"></a>        ...</span>
<span id="cb17-9"><a href="#cb17-9"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_layer(y_dec)</span>
<span id="cb17-10"><a href="#cb17-10"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="encoder-decoder-layer" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="encoder-decoder-layer"><span class="header-section-number">2.8</span> Encoder &amp; Decoder Layer</h2>
<p>有了这些基础组件，我们就可以和叠积木一样，来搭建Transformer的Encoder和Decoder层了。</p>
<p>Encoder 可以表示为:</p>
<p><span id="eq-encoder-layer"><span class="math display">\[
\begin{split}
\text{EncoderLayer}_{i}(\mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{x} + \text{MultiHeadSelfAttention}_{i}(\mathrm{x}, \mathrm{x},\mathrm{x})\right) \\
\text{EncoderLayer}_{i}(\mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{x} + \text{FFN}_{i}(\mathrm{x})\right) \\
\end{split}
\tag{28}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathrm{x}\)</span> 是输入向量，<span class="math inline">\(\text{MultiHeadSelfAttention}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个编码器层的多头自注意力机制，<span class="math inline">\(\text{FFN}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个编码器层的前馈神经网络，<span class="math inline">\(\text{LayerNorm}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个编码器层的层归一化。</p>
<div class="sourceCode" id="cb18" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb18-3"><a href="#cb18-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-6"><a href="#cb18-6"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb18-7"><a href="#cb18-7"></a></span>
<span id="cb18-8"><a href="#cb18-8"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb18-9"><a href="#cb18-9"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb18-10"><a href="#cb18-10"></a></span>
<span id="cb18-11"><a href="#cb18-11"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor, pad_mask<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-14"><a href="#cb18-14"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attn(x, x, x, pad_mask<span class="op">=</span>pad_mask)</span>
<span id="cb18-15"><a href="#cb18-15"></a>        x <span class="op">=</span> <span class="va">self</span>.ln1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))</span>
<span id="cb18-16"><a href="#cb18-16"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb18-17"><a href="#cb18-17"></a>        x <span class="op">=</span> <span class="va">self</span>.ln2(x <span class="op">+</span> ffn_output)</span>
<span id="cb18-18"><a href="#cb18-18"></a></span>
<span id="cb18-19"><a href="#cb18-19"></a>        <span class="cf">return</span> x</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb18-22"><a href="#cb18-22"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb18-23"><a href="#cb18-23"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-24"><a href="#cb18-24"></a></span>
<span id="cb18-25"><a href="#cb18-25"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderBlock(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_layers)])</span>
<span id="cb18-26"><a href="#cb18-26"></a></span>
<span id="cb18-27"><a href="#cb18-27"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor, pad_mask<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-28"><a href="#cb18-28"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-29"><a href="#cb18-29"></a>            x <span class="op">=</span> layer(x, pad_mask<span class="op">=</span>pad_mask)</span>
<span id="cb18-30"><a href="#cb18-30"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>下图展示了Encoder Layer的过程：</p>
<div id="fig-encoder-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/transformer-encoding.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Encoding Layer Process
</figcaption>
</figure>
</div>
<p>Decoder 可以表示为: <span id="eq-decoder-layer"><span class="math display">\[
\begin{split}
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{CausalMultiHeadSelfAttention}_{i}(\mathrm{y}, \mathrm{y}, \mathrm{y})\right) \\
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{CrossAttention}_{i}(\mathrm{y}, \mathrm{x}_{enc}, \mathrm{x}_{enc})\right) \\
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{FFN}_{i}(\mathrm{y})\right) \\
\end{split}
\tag{29}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathrm{y}\)</span> 是解码器的输入向量，<span class="math inline">\(\mathrm{x}_{enc}\)</span> 是编码器的输出向量，<span class="math inline">\(\text{CausalMultiHeadSelfAttention}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个解码器层的因果多头自注意力机制，<span class="math inline">\(\text{CrossAttention}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个解码器层的交叉注意力机制，<span class="math inline">\(\text{FFN}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个解码器层的前馈神经网络，<span class="math inline">\(\text{LayerNorm}_{i}\)</span> 是第 <span class="math inline">\(i\)</span> 个解码器层的层归一化。</p>
<div class="sourceCode" id="cb19" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">class</span> DecoderBlock(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb19-3"><a href="#cb19-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-9"><a href="#cb19-9"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb19-12"><a href="#cb19-12"></a>        <span class="va">self</span>.ln3 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-13"><a href="#cb19-13"></a></span>
<span id="cb19-14"><a href="#cb19-14"></a>    <span class="kw">def</span> forward(</span>
<span id="cb19-15"><a href="#cb19-15"></a>        <span class="va">self</span>,</span>
<span id="cb19-16"><a href="#cb19-16"></a>        x: torch.Tensor,</span>
<span id="cb19-17"><a href="#cb19-17"></a>        enc_output: torch.Tensor,</span>
<span id="cb19-18"><a href="#cb19-18"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-19"><a href="#cb19-19"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-20"><a href="#cb19-20"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb19-21"><a href="#cb19-21"></a>        self_attn_output, _ <span class="op">=</span> <span class="va">self</span>.self_attn(x, x, x, pad_mask<span class="op">=</span>tgt_pad_mask)</span>
<span id="cb19-22"><a href="#cb19-22"></a>        x <span class="op">=</span> <span class="va">self</span>.ln1(x <span class="op">+</span> self_attn_output)</span>
<span id="cb19-23"><a href="#cb19-23"></a></span>
<span id="cb19-24"><a href="#cb19-24"></a>        cross_attn_output, _ <span class="op">=</span> <span class="va">self</span>.cross_attn(x, enc_output, enc_output, pad_mask<span class="op">=</span>src_pad_mask)</span>
<span id="cb19-25"><a href="#cb19-25"></a>        x <span class="op">=</span> <span class="va">self</span>.ln2(x <span class="op">+</span> cross_attn_output)</span>
<span id="cb19-26"><a href="#cb19-26"></a></span>
<span id="cb19-27"><a href="#cb19-27"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb19-28"><a href="#cb19-28"></a>        x <span class="op">=</span> <span class="va">self</span>.ln3(x <span class="op">+</span> ffn_output)</span>
<span id="cb19-29"><a href="#cb19-29"></a></span>
<span id="cb19-30"><a href="#cb19-30"></a>        <span class="cf">return</span> x</span>
<span id="cb19-31"><a href="#cb19-31"></a></span>
<span id="cb19-32"><a href="#cb19-32"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb19-33"><a href="#cb19-33"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb19-34"><a href="#cb19-34"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-35"><a href="#cb19-35"></a></span>
<span id="cb19-36"><a href="#cb19-36"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([DecoderBlock(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_layers)])</span>
<span id="cb19-37"><a href="#cb19-37"></a></span>
<span id="cb19-38"><a href="#cb19-38"></a>    <span class="kw">def</span> forward(</span>
<span id="cb19-39"><a href="#cb19-39"></a>        <span class="va">self</span>,</span>
<span id="cb19-40"><a href="#cb19-40"></a>        x: torch.Tensor,</span>
<span id="cb19-41"><a href="#cb19-41"></a>        enc_output: torch.Tensor,</span>
<span id="cb19-42"><a href="#cb19-42"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-43"><a href="#cb19-43"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-44"><a href="#cb19-44"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb19-45"><a href="#cb19-45"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb19-46"><a href="#cb19-46"></a>            x <span class="op">=</span> layer(</span>
<span id="cb19-47"><a href="#cb19-47"></a>                x,</span>
<span id="cb19-48"><a href="#cb19-48"></a>                enc_output,</span>
<span id="cb19-49"><a href="#cb19-49"></a>                src_pad_mask<span class="op">=</span>src_pad_mask,</span>
<span id="cb19-50"><a href="#cb19-50"></a>                tgt_pad_mask<span class="op">=</span>tgt_pad_mask,</span>
<span id="cb19-51"><a href="#cb19-51"></a>            )</span>
<span id="cb19-52"><a href="#cb19-52"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>下图展示了Decoder Layer的过程：</p>
<div id="fig-decoder-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/transformer-decoding.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Decoding Layer Process
</figcaption>
</figure>
</div>
<p>通过堆叠多个编码器层和解码器层，我们就可以构建出完整的Transformer模型。</p>
<div class="sourceCode" id="cb20" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb20-3"><a href="#cb20-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4"></a></span>
<span id="cb20-5"><a href="#cb20-5"></a>        <span class="va">self</span>.vocab_embedding <span class="op">=</span> WordEmbedding(config.vocab_size, config.d_model)</span>
<span id="cb20-6"><a href="#cb20-6"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> PositionalEmbedding(config)</span>
<span id="cb20-7"><a href="#cb20-7"></a></span>
<span id="cb20-8"><a href="#cb20-8"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(config)</span>
<span id="cb20-9"><a href="#cb20-9"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(config)</span>
<span id="cb20-10"><a href="#cb20-10"></a></span>
<span id="cb20-11"><a href="#cb20-11"></a>        <span class="va">self</span>.output_proj <span class="op">=</span> nn.Linear(config.d_model, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-12"><a href="#cb20-12"></a></span>
<span id="cb20-13"><a href="#cb20-13"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb20-14"><a href="#cb20-14"></a>        <span class="va">self</span>._tie_weights()</span>
<span id="cb20-15"><a href="#cb20-15"></a></span>
<span id="cb20-16"><a href="#cb20-16"></a>    <span class="kw">def</span> _tie_weights(<span class="va">self</span>):</span>
<span id="cb20-17"><a href="#cb20-17"></a>        <span class="va">self</span>.output_proj.weight <span class="op">=</span> <span class="va">self</span>.vocab_embedding.embedding.weight</span>
<span id="cb20-18"><a href="#cb20-18"></a></span>
<span id="cb20-19"><a href="#cb20-19"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb20-20"><a href="#cb20-20"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb20-21"><a href="#cb20-21"></a>            nn.init.xavier_uniform_(module.weight)</span>
<span id="cb20-22"><a href="#cb20-22"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb20-23"><a href="#cb20-23"></a>                nn.init.zeros_(module.bias)</span>
<span id="cb20-24"><a href="#cb20-24"></a></span>
<span id="cb20-25"><a href="#cb20-25"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb20-26"><a href="#cb20-26"></a>            nn.init.xavier_uniform_(module.weight)</span>
<span id="cb20-27"><a href="#cb20-27"></a></span>
<span id="cb20-28"><a href="#cb20-28"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, LayerNorm):</span>
<span id="cb20-29"><a href="#cb20-29"></a>            nn.init.ones_(module.gamma)</span>
<span id="cb20-30"><a href="#cb20-30"></a>            nn.init.zeros_(module.beta)</span>
<span id="cb20-31"><a href="#cb20-31"></a></span>
<span id="cb20-32"><a href="#cb20-32"></a>    <span class="kw">def</span> forward(</span>
<span id="cb20-33"><a href="#cb20-33"></a>        <span class="va">self</span>,</span>
<span id="cb20-34"><a href="#cb20-34"></a>        src_input: torch.Tensor,</span>
<span id="cb20-35"><a href="#cb20-35"></a>        tgt_input: torch.Tensor,</span>
<span id="cb20-36"><a href="#cb20-36"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb20-37"><a href="#cb20-37"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb20-38"><a href="#cb20-38"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb20-39"><a href="#cb20-39"></a>        <span class="co"># Get Src and Tgt embeddings</span></span>
<span id="cb20-40"><a href="#cb20-40"></a>        src_embeddings <span class="op">=</span> <span class="va">self</span>.vocab_embedding(src_input) <span class="op">*</span> math.sqrt(</span>
<span id="cb20-41"><a href="#cb20-41"></a>            <span class="va">self</span>.vocab_embedding.embedding.embedding_dim</span>
<span id="cb20-42"><a href="#cb20-42"></a>        ) <span class="op">+</span> <span class="va">self</span>.positional_embedding(src_input)</span>
<span id="cb20-43"><a href="#cb20-43"></a>        tgt_embeddings <span class="op">=</span> <span class="va">self</span>.vocab_embedding(tgt_input) <span class="op">*</span> math.sqrt(</span>
<span id="cb20-44"><a href="#cb20-44"></a>            <span class="va">self</span>.vocab_embedding.embedding.embedding_dim</span>
<span id="cb20-45"><a href="#cb20-45"></a>        ) <span class="op">+</span> <span class="va">self</span>.positional_embedding(tgt_input)</span>
<span id="cb20-46"><a href="#cb20-46"></a></span>
<span id="cb20-47"><a href="#cb20-47"></a>        <span class="co"># Feed through Encoder</span></span>
<span id="cb20-48"><a href="#cb20-48"></a>        enc_output <span class="op">=</span> <span class="va">self</span>.encoder(src_embeddings, pad_mask<span class="op">=</span>src_pad_mask)</span>
<span id="cb20-49"><a href="#cb20-49"></a></span>
<span id="cb20-50"><a href="#cb20-50"></a>        <span class="co"># Feed through Decoder with Encoder output</span></span>
<span id="cb20-51"><a href="#cb20-51"></a>        dec_output <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb20-52"><a href="#cb20-52"></a>            tgt_embeddings,</span>
<span id="cb20-53"><a href="#cb20-53"></a>            enc_output,</span>
<span id="cb20-54"><a href="#cb20-54"></a>            src_pad_mask<span class="op">=</span>src_pad_mask,</span>
<span id="cb20-55"><a href="#cb20-55"></a>            tgt_pad_mask<span class="op">=</span>tgt_pad_mask,</span>
<span id="cb20-56"><a href="#cb20-56"></a>        )</span>
<span id="cb20-57"><a href="#cb20-57"></a></span>
<span id="cb20-58"><a href="#cb20-58"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_proj(dec_output)</span>
<span id="cb20-59"><a href="#cb20-59"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="others" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="others"><span class="header-section-number">2.9</span> Others</h2>
<p>当然，除了以上的一个部分，Transformer中还有几个值得一提的部分,比如:</p>
<ul>
<li>Dropout Layer: 用于防止过拟合</li>
<li>Label Smoothing: 用于提高模型的泛化能力</li>
</ul>
<section id="dropout-layer" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="dropout-layer"><span class="header-section-number">2.9.1</span> Dropout Layer</h3>
<p>Dropout <span class="citation" data-cites="Dropout2014srivastava">(<a href="#ref-Dropout2014srivastava" role="doc-biblioref"><strong>Dropout2014srivastava?</strong></a>)</span> 是一种常用的正则化技术，旨在防止神经网络在训练过程中过拟合。其基本思想是在训练过程中，<u>随机地“丢弃”一部分神经元</u>，即将它们的输出设置为零，从而减少神经元之间的相互依赖，提高模型的泛化能力。具体来说，假设我们有一个神经网络层的输入向量 <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{d}\)</span>，Dropout 的计算过程如下:</p>
<p><span id="eq-dropout"><span class="math display">\[
\begin{split}
\mathbf{r} &amp; \sim \text{Bernoulli}(p) \\
\hat{\mathbf{x}} &amp; = \mathbf{x} \odot \mathbf{r} \\
\mathbf{y} &amp; = \frac{1}{p} \hat{\mathbf{x}}
\end{split}
\tag{30}\]</span></span></p>
<p>其中，<span class="math inline">\(\mathbf{r} \in \mathbb{R}^{d}\)</span> 是一个与输入向量 <span class="math inline">\(\mathbf{x}\)</span> 形状相同的二进制掩码向量，其每个元素独立地服从伯努利分布，取值为1的概率为 <span class="math inline">\(p\)</span>（保留概率），取值为0的概率为 <span class="math inline">\(1-p\)</span>（丢弃概率）。<span class="math inline">\(\odot\)</span> 表示逐元素乘法操作，<span class="math inline">\(\hat{\mathbf{x}}\)</span> 是经过 Dropout 处理后的输入向量，<span class="math inline">\(\mathbf{y}\)</span> 是最终的输出向量，通过除以保留概率 <span class="math inline">\(p\)</span> 来进行缩放，以保持输出的期望值不变。</p>
<p>用Python实现Dropout如下:</p>
<div class="sourceCode" id="cb21" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb21-3"><a href="#cb21-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-6"><a href="#cb21-6"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb21-7"><a href="#cb21-7"></a>            mask <span class="op">=</span> (torch.rand_like(x) <span class="op">&lt;</span> <span class="va">self</span>.p).<span class="bu">float</span>()</span>
<span id="cb21-8"><a href="#cb21-8"></a>            <span class="cf">return</span> (x <span class="op">*</span> mask) <span class="op">/</span> <span class="va">self</span>.p</span>
<span id="cb21-9"><a href="#cb21-9"></a>        <span class="cf">else</span>:</span>
<span id="cb21-10"><a href="#cb21-10"></a>            <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="label-smoothing" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="label-smoothing"><span class="header-section-number">2.9.2</span> Label Smoothing</h3>
<p>Label Smoothing <span class="citation" data-cites="RethinkingInception2016szegedy">(<a href="#ref-RethinkingInception2016szegedy" role="doc-biblioref"><strong>RethinkingInception2016szegedy?</strong></a>)</span> 是一种用于分类任务的正则化技术，旨在提高模型的泛化能力。其基本思想是将目标标签从“硬标签”（one-hot encoding）转换为“软标签”，即在目标标签中引入一定的平滑度，从而防止模型过于自信地预测某个类别。具体来说，假设我们有一个分类任务，类别总数为 <span class="math inline">\(C\)</span>，原始的目标标签为 <span class="math inline">\(\mathbf{y} \in \mathbb{R}^{C}\)</span>，其中只有一个元素为1，其余元素为0（one-hot encoding）。Label Smoothing 的计算过程如下:</p>
<p><span id="eq-label-smoothing"><span class="math display">\[
\mathbf{y}_{smooth} = (1 - \epsilon) \mathbf{y} + \frac{\epsilon}{C}
\tag{31}\]</span></span></p>
<p>其中，<span class="math inline">\(\epsilon\)</span> 是平滑参数，控制标签的平滑程度，<span class="math inline">\(\mathbf{y}_{smooth} \in \mathbb{R}^{C}\)</span> 是经过 Label Smoothing 处理后的目标标签。</p>
</section>
</section>
</section>
<section id="experiment" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Experiment</h1>
<p>接下来，我们来看一下Transformer模型在训练时的一些细节设置。</p>
<section id="dataset" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="dataset"><span class="header-section-number">3.1</span> Dataset</h2>
<p>首先，我们来看一下我们的数据集，在这里，我们使用的<a href="https://huggingface.co/datasets/IWSLT/iwslt2017">Ted Talks</a>的数据集中的英文-中文Pairs，它包含了大量的TED演讲视频的字幕文本，涵盖了多个领域和主题。我们看一下其中几个例子：</p>
<div class="sourceCode" id="cb22" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>[{<span class="st">'en'</span>: <span class="st">"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful."</span>,</span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="st">'zh'</span>: <span class="st">'非常谢谢，克里斯。的确非常荣幸 能有第二次站在这个台上的机会，我真是非常感激。'</span>},</span>
<span id="cb22-3"><a href="#cb22-3"></a> {<span class="st">'en'</span>: <span class="st">'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.'</span>,</span>
<span id="cb22-4"><a href="#cb22-4"></a>  <span class="st">'zh'</span>: <span class="st">'这个会议真是让我感到惊叹不已，我还要谢谢你们留下的 关于我上次演讲的精彩评论'</span>},</span>
<span id="cb22-5"><a href="#cb22-5"></a> {<span class="st">'en'</span>: <span class="st">'And I say that sincerely, partly because  I need that.  Put yourselves in my position.'</span>,</span>
<span id="cb22-6"><a href="#cb22-6"></a>  <span class="st">'zh'</span>: <span class="st">'我是非常真诚的，部分原因是因为----我的确非常需要！ 你设身处地为我想想！'</span>},</span>
<span id="cb22-7"><a href="#cb22-7"></a> {<span class="st">'en'</span>: <span class="st">'I flew on Air Force Two for eight years.'</span>, <span class="st">'zh'</span>: <span class="st">'我坐了8年的空军二号。'</span>},</span>
<span id="cb22-8"><a href="#cb22-8"></a> {<span class="st">'en'</span>: <span class="st">'Now I have to take off my shoes or boots to get on an airplane!'</span>,</span>
<span id="cb22-9"><a href="#cb22-9"></a>  <span class="st">'zh'</span>: <span class="st">'不过现在上飞机前我则要脱掉我的鞋子'</span>}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>其中 <code>train_dataset</code> 有231,266条数据，<code>test_dataset</code> 有 8,549 条数据。</p>
<section id="tokenizer-vocabulary" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="tokenizer-vocabulary"><span class="header-section-number">3.1.1</span> Tokenizer &amp; Vocabulary</h3>
<p>在论文中，Target 和 Source 使用同一个Tokenizer，并且共享同一个词表（Vocabulary）。并且使用 BPE (Byte Pair Encoding) <span class="citation" data-cites="NeuralMachineTranslation2016sennrich">(<a href="#ref-NeuralMachineTranslation2016sennrich" role="doc-biblioref">Sennrich, Haddow, and Birch 2016</a>)</span> 来进行分词和构建词表。</p>
<p>在这里，我们用<code>Hugging Face</code>的<code>transformers</code>库中的<code>Tokenizer</code>来进行分词和构建词表，词表大小设置为10,000。</p>
<div class="sourceCode" id="cb23" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">def</span> load_or_train_joint_bpe_tokenizer(</span>
<span id="cb23-2"><a href="#cb23-2"></a>    vocab_size: <span class="bu">int</span>,</span>
<span id="cb23-3"><a href="#cb23-3"></a>    save_prefix: <span class="bu">str</span>,</span>
<span id="cb23-4"><a href="#cb23-4"></a>    save_name: <span class="bu">str</span> <span class="op">=</span> <span class="st">"bpe_joint.json"</span>,</span>
<span id="cb23-5"><a href="#cb23-5"></a>    src_corpus_file: <span class="bu">str</span> <span class="op">=</span> <span class="st">"train_src.txt"</span>,</span>
<span id="cb23-6"><a href="#cb23-6"></a>    tgt_corpus_file: <span class="bu">str</span> <span class="op">=</span> <span class="st">"train_tgt.txt"</span>,</span>
<span id="cb23-7"><a href="#cb23-7"></a>):</span>
<span id="cb23-8"><a href="#cb23-8"></a>    save_path <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>save_prefix<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>save_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb23-9"><a href="#cb23-9"></a></span>
<span id="cb23-10"><a href="#cb23-10"></a>    <span class="cf">if</span> os.path.exists(save_path):</span>
<span id="cb23-11"><a href="#cb23-11"></a>        <span class="bu">print</span>(<span class="ss">f"Loading tokenizer from </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-12"><a href="#cb23-12"></a>        <span class="cf">return</span> Tokenizer.from_file(save_path)</span>
<span id="cb23-13"><a href="#cb23-13"></a></span>
<span id="cb23-14"><a href="#cb23-14"></a>    <span class="co"># Train ONE tokenizer on BOTH corpora (concatenated dataset)</span></span>
<span id="cb23-15"><a href="#cb23-15"></a>    tokenizer <span class="op">=</span> Tokenizer(models.BPE(unk_token<span class="op">=</span><span class="st">"&lt;unk&gt;"</span>))</span>
<span id="cb23-16"><a href="#cb23-16"></a>    tokenizer.normalizer <span class="op">=</span> NFKC()</span>
<span id="cb23-17"><a href="#cb23-17"></a>    tokenizer.pre_tokenizer <span class="op">=</span> ByteLevel(add_prefix_space<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-18"><a href="#cb23-18"></a>    tokenizer.decoder <span class="op">=</span> ByteLevelDecoder()</span>
<span id="cb23-19"><a href="#cb23-19"></a></span>
<span id="cb23-20"><a href="#cb23-20"></a>    trainer <span class="op">=</span> trainers.BpeTrainer(</span>
<span id="cb23-21"><a href="#cb23-21"></a>        vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb23-22"><a href="#cb23-22"></a>        special_tokens<span class="op">=</span>[<span class="st">"&lt;pad&gt;"</span>, <span class="st">"&lt;unk&gt;"</span>, <span class="st">"&lt;s&gt;"</span>, <span class="st">"&lt;/s&gt;"</span>],</span>
<span id="cb23-23"><a href="#cb23-23"></a>    )</span>
<span id="cb23-24"><a href="#cb23-24"></a></span>
<span id="cb23-25"><a href="#cb23-25"></a>    tokenizer.train([src_corpus_file, tgt_corpus_file], trainer)</span>
<span id="cb23-26"><a href="#cb23-26"></a>    tokenizer.save(save_path)</span>
<span id="cb23-27"><a href="#cb23-27"></a>    <span class="bu">print</span>(<span class="ss">f"Saved tokenizer to </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-28"><a href="#cb23-28"></a></span>
<span id="cb23-29"><a href="#cb23-29"></a>    <span class="cf">return</span> tokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>在训练完成后，我们提前处理好数据集，保存为<code>PT</code>格式，方便后续的训练使用。</p>
<div class="sourceCode" id="cb24" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">def</span> encode_file_to_pt(</span>
<span id="cb24-2"><a href="#cb24-2"></a>    tokenizer,</span>
<span id="cb24-3"><a href="#cb24-3"></a>    in_path: <span class="bu">str</span>,</span>
<span id="cb24-4"><a href="#cb24-4"></a>    out_path: <span class="bu">str</span>,</span>
<span id="cb24-5"><a href="#cb24-5"></a>    add_special_tokens: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb24-6"><a href="#cb24-6"></a>    bos_token: <span class="bu">str</span> <span class="op">=</span> <span class="st">"&lt;s&gt;"</span>,</span>
<span id="cb24-7"><a href="#cb24-7"></a>    eos_token: <span class="bu">str</span> <span class="op">=</span> <span class="st">"&lt;/s&gt;"</span>,</span>
<span id="cb24-8"><a href="#cb24-8"></a>    max_lines: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb24-9"><a href="#cb24-9"></a>):</span>
<span id="cb24-10"><a href="#cb24-10"></a>    <span class="cf">if</span> os.path.exists(out_path):</span>
<span id="cb24-11"><a href="#cb24-11"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>out_path<span class="sc">}</span><span class="ss"> already exists"</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a>        <span class="cf">return</span></span>
<span id="cb24-13"><a href="#cb24-13"></a></span>
<span id="cb24-14"><a href="#cb24-14"></a>    bos_id <span class="op">=</span> tokenizer.token_to_id(bos_token)</span>
<span id="cb24-15"><a href="#cb24-15"></a>    eos_id <span class="op">=</span> tokenizer.token_to_id(eos_token)</span>
<span id="cb24-16"><a href="#cb24-16"></a></span>
<span id="cb24-17"><a href="#cb24-17"></a>    all_ids <span class="op">=</span> []</span>
<span id="cb24-18"><a href="#cb24-18"></a>    <span class="cf">with</span> <span class="bu">open</span>(in_path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb24-19"><a href="#cb24-19"></a>        <span class="cf">for</span> i, line <span class="kw">in</span> <span class="bu">enumerate</span>(f):</span>
<span id="cb24-20"><a href="#cb24-20"></a>            <span class="cf">if</span> max_lines <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> i <span class="op">&gt;=</span> max_lines:</span>
<span id="cb24-21"><a href="#cb24-21"></a>                <span class="cf">break</span></span>
<span id="cb24-22"><a href="#cb24-22"></a>            text <span class="op">=</span> line.rstrip(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb24-23"><a href="#cb24-23"></a>            enc <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb24-24"><a href="#cb24-24"></a>            ids <span class="op">=</span> enc.ids</span>
<span id="cb24-25"><a href="#cb24-25"></a></span>
<span id="cb24-26"><a href="#cb24-26"></a>            <span class="cf">if</span> add_special_tokens:</span>
<span id="cb24-27"><a href="#cb24-27"></a>                ids <span class="op">=</span> [bos_id] <span class="op">+</span> ids <span class="op">+</span> [eos_id]</span>
<span id="cb24-28"><a href="#cb24-28"></a></span>
<span id="cb24-29"><a href="#cb24-29"></a>            all_ids.append(torch.tensor(ids, dtype<span class="op">=</span>torch.int32))</span>
<span id="cb24-30"><a href="#cb24-30"></a></span>
<span id="cb24-31"><a href="#cb24-31"></a>    os.makedirs(os.path.dirname(out_path) <span class="kw">or</span> <span class="st">"."</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-32"><a href="#cb24-32"></a>    torch.save(all_ids, out_path)</span>
<span id="cb24-33"><a href="#cb24-33"></a>    <span class="bu">print</span>(<span class="ss">f"Saved </span><span class="sc">{</span><span class="bu">len</span>(all_ids)<span class="sc">}</span><span class="ss"> sequences to </span><span class="sc">{</span>out_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-34"><a href="#cb24-34"></a></span>
<span id="cb24-35"><a href="#cb24-35"></a></span>
<span id="cb24-36"><a href="#cb24-36"></a>encode_file_to_pt(tokenizer, <span class="st">"train_src.txt"</span>, <span class="st">"train_src_ids.pt"</span>)</span>
<span id="cb24-37"><a href="#cb24-37"></a>encode_file_to_pt(tokenizer, <span class="st">"train_tgt.txt"</span>, <span class="st">"train_tgt_ids.pt"</span>)</span>
<span id="cb24-38"><a href="#cb24-38"></a>encode_file_to_pt(tokenizer, <span class="st">"test_src.txt"</span>, <span class="st">"test_src_ids.pt"</span>)</span>
<span id="cb24-39"><a href="#cb24-39"></a>encode_file_to_pt(tokenizer, <span class="st">"test_tgt.txt"</span>, <span class="st">"test_tgt_ids.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="padding-samples" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="padding-samples"><span class="header-section-number">3.1.2</span> Padding Samples</h3>
<p>至此，我们的数据集就准备好了。在后续的训练中，我们可以直接加载这些预处理好的数据集进行训练，需要注意的一点是，我们在加载数据集时，需要对输入序列进行Padding，以确保每个Batch中的序列长度一致。在论文中有一个方式，就是将长度差不多的序列放在同一个Batch中，这样可以减少Padding的数量，从而提高训练效率。在这里，我们使用一个 <code>Sampler</code> 来实现这个功能。</p>
<div class="sourceCode" id="cb25" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="kw">class</span> BucketBatchSampler(Sampler[<span class="bu">list</span>[<span class="bu">int</span>]]):</span>
<span id="cb25-2"><a href="#cb25-2"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="co">    Yields batches of indices where sequences have similar lengths.</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co">    length_fn: function(idx) -&gt; int</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="co">    """</span></span>
<span id="cb25-6"><a href="#cb25-6"></a></span>
<span id="cb25-7"><a href="#cb25-7"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb25-8"><a href="#cb25-8"></a>        <span class="va">self</span>,</span>
<span id="cb25-9"><a href="#cb25-9"></a>        lengths,</span>
<span id="cb25-10"><a href="#cb25-10"></a>        batch_size: <span class="bu">int</span>,</span>
<span id="cb25-11"><a href="#cb25-11"></a>        bucket_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span>,</span>
<span id="cb25-12"><a href="#cb25-12"></a>        shuffle: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb25-13"><a href="#cb25-13"></a>        drop_last: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb25-14"><a href="#cb25-14"></a>        seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb25-15"><a href="#cb25-15"></a>    ):</span>
<span id="cb25-16"><a href="#cb25-16"></a>        <span class="va">self</span>.lengths <span class="op">=</span> <span class="bu">list</span>(lengths)</span>
<span id="cb25-17"><a href="#cb25-17"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb25-18"><a href="#cb25-18"></a>        <span class="va">self</span>.bucket_size <span class="op">=</span> bucket_size</span>
<span id="cb25-19"><a href="#cb25-19"></a>        <span class="va">self</span>.shuffle <span class="op">=</span> shuffle</span>
<span id="cb25-20"><a href="#cb25-20"></a>        <span class="va">self</span>.drop_last <span class="op">=</span> drop_last</span>
<span id="cb25-21"><a href="#cb25-21"></a>        <span class="va">self</span>.seed <span class="op">=</span> seed</span>
<span id="cb25-22"><a href="#cb25-22"></a></span>
<span id="cb25-23"><a href="#cb25-23"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb25-24"><a href="#cb25-24"></a>        rng <span class="op">=</span> random.Random(<span class="va">self</span>.seed)</span>
<span id="cb25-25"><a href="#cb25-25"></a></span>
<span id="cb25-26"><a href="#cb25-26"></a>        indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.lengths)))</span>
<span id="cb25-27"><a href="#cb25-27"></a>        <span class="cf">if</span> <span class="va">self</span>.shuffle:</span>
<span id="cb25-28"><a href="#cb25-28"></a>            rng.shuffle(indices)</span>
<span id="cb25-29"><a href="#cb25-29"></a></span>
<span id="cb25-30"><a href="#cb25-30"></a>        <span class="co"># chunk into buckets</span></span>
<span id="cb25-31"><a href="#cb25-31"></a>        <span class="cf">for</span> b_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(indices), <span class="va">self</span>.bucket_size):</span>
<span id="cb25-32"><a href="#cb25-32"></a>            bucket <span class="op">=</span> indices[b_start : b_start <span class="op">+</span> <span class="va">self</span>.bucket_size]</span>
<span id="cb25-33"><a href="#cb25-33"></a>            <span class="co"># sort inside bucket by length</span></span>
<span id="cb25-34"><a href="#cb25-34"></a>            bucket.sort(key<span class="op">=</span><span class="kw">lambda</span> i: <span class="va">self</span>.lengths[i])</span>
<span id="cb25-35"><a href="#cb25-35"></a></span>
<span id="cb25-36"><a href="#cb25-36"></a>            <span class="co"># make batches</span></span>
<span id="cb25-37"><a href="#cb25-37"></a>            batches <span class="op">=</span> [bucket[i : i <span class="op">+</span> <span class="va">self</span>.batch_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(bucket), <span class="va">self</span>.batch_size)]</span>
<span id="cb25-38"><a href="#cb25-38"></a>            <span class="cf">if</span> <span class="va">self</span>.drop_last <span class="kw">and</span> <span class="bu">len</span>(batches) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(batches[<span class="op">-</span><span class="dv">1</span>]) <span class="op">&lt;</span> <span class="va">self</span>.batch_size:</span>
<span id="cb25-39"><a href="#cb25-39"></a>                batches <span class="op">=</span> batches[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-40"><a href="#cb25-40"></a></span>
<span id="cb25-41"><a href="#cb25-41"></a>            <span class="cf">if</span> <span class="va">self</span>.shuffle:</span>
<span id="cb25-42"><a href="#cb25-42"></a>                rng.shuffle(batches)</span>
<span id="cb25-43"><a href="#cb25-43"></a></span>
<span id="cb25-44"><a href="#cb25-44"></a>            <span class="cf">for</span> batch <span class="kw">in</span> batches:</span>
<span id="cb25-45"><a href="#cb25-45"></a>                <span class="cf">yield</span> batch</span>
<span id="cb25-46"><a href="#cb25-46"></a></span>
<span id="cb25-47"><a href="#cb25-47"></a>        <span class="co"># update seed so next epoch reshuffles differently</span></span>
<span id="cb25-48"><a href="#cb25-48"></a>        <span class="va">self</span>.seed <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-49"><a href="#cb25-49"></a></span>
<span id="cb25-50"><a href="#cb25-50"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb25-51"><a href="#cb25-51"></a>        n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.lengths)</span>
<span id="cb25-52"><a href="#cb25-52"></a>        <span class="cf">if</span> <span class="va">self</span>.drop_last:</span>
<span id="cb25-53"><a href="#cb25-53"></a>            <span class="cf">return</span> n <span class="op">//</span> <span class="va">self</span>.batch_size</span>
<span id="cb25-54"><a href="#cb25-54"></a>        <span class="cf">return</span> math.ceil(n <span class="op">/</span> <span class="va">self</span>.batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>这个 <code>BucketBatchSampler</code> 会根据序列的长度将它们分配到不同的Bucket中，然后在每个Bucket内按长度排序，最后生成Batch。这样可以确保每个Batch中的序列长度相似，从而减少Padding的数量。</p>
<p>有了一个Batch之后，我们还需要一个 <code>collate_fn</code> 来对Batch中的序列进行Padding:</p>
<div class="sourceCode" id="cb26" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="kw">def</span> translation_collate(batch, pad_id: <span class="bu">int</span>, sos_id: <span class="bu">int</span>, eos_id: <span class="bu">int</span>, max_len: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb26-3"><a href="#cb26-3"></a>    src_list <span class="op">=</span> [item[<span class="st">"src_ids"</span>] <span class="cf">for</span> item <span class="kw">in</span> batch]</span>
<span id="cb26-4"><a href="#cb26-4"></a>    tgt_list <span class="op">=</span> [item[<span class="st">"tgt_ids"</span>] <span class="cf">for</span> item <span class="kw">in</span> batch]</span>
<span id="cb26-5"><a href="#cb26-5"></a></span>
<span id="cb26-6"><a href="#cb26-6"></a>    <span class="cf">if</span> max_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb26-7"><a href="#cb26-7"></a>        src_list <span class="op">=</span> [x[:max_len] <span class="cf">for</span> x <span class="kw">in</span> src_list]</span>
<span id="cb26-8"><a href="#cb26-8"></a>        tgt_list <span class="op">=</span> [x[:max_len] <span class="cf">for</span> x <span class="kw">in</span> tgt_list]  <span class="co"># leave room for EOS</span></span>
<span id="cb26-9"><a href="#cb26-9"></a></span>
<span id="cb26-10"><a href="#cb26-10"></a>    decoder_list <span class="op">=</span> [t[:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> tgt_list]</span>
<span id="cb26-11"><a href="#cb26-11"></a>    labels_list <span class="op">=</span> [t[<span class="dv">1</span>:] <span class="cf">for</span> t <span class="kw">in</span> tgt_list]</span>
<span id="cb26-12"><a href="#cb26-12"></a></span>
<span id="cb26-13"><a href="#cb26-13"></a>    src_max <span class="op">=</span> <span class="bu">max</span>(x.numel() <span class="cf">for</span> x <span class="kw">in</span> src_list)</span>
<span id="cb26-14"><a href="#cb26-14"></a>    dec_max <span class="op">=</span> <span class="bu">max</span>(x.numel() <span class="cf">for</span> x <span class="kw">in</span> decoder_list)</span>
<span id="cb26-15"><a href="#cb26-15"></a></span>
<span id="cb26-16"><a href="#cb26-16"></a>    <span class="kw">def</span> pad_1d(x: torch.Tensor, L: <span class="bu">int</span>):</span>
<span id="cb26-17"><a href="#cb26-17"></a>        x <span class="op">=</span> x.to(torch.<span class="bu">long</span>)</span>
<span id="cb26-18"><a href="#cb26-18"></a>        <span class="cf">if</span> x.numel() <span class="op">==</span> L:</span>
<span id="cb26-19"><a href="#cb26-19"></a>            <span class="cf">return</span> x</span>
<span id="cb26-20"><a href="#cb26-20"></a>        <span class="cf">return</span> torch.cat([x, x.new_full((L <span class="op">-</span> x.numel(),), pad_id, dtype<span class="op">=</span>torch.<span class="bu">long</span>)])</span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a>    encoder_input_ids <span class="op">=</span> torch.stack([pad_1d(x, src_max) <span class="cf">for</span> x <span class="kw">in</span> src_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-23"><a href="#cb26-23"></a>    decoder_input_ids <span class="op">=</span> torch.stack([pad_1d(x, dec_max) <span class="cf">for</span> x <span class="kw">in</span> decoder_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-24"><a href="#cb26-24"></a>    labels <span class="op">=</span> torch.stack([pad_1d(x, dec_max) <span class="cf">for</span> x <span class="kw">in</span> labels_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a>    <span class="cf">return</span> {</span>
<span id="cb26-27"><a href="#cb26-27"></a>        <span class="st">"encoder_input_ids"</span>: encoder_input_ids,</span>
<span id="cb26-28"><a href="#cb26-28"></a>        <span class="st">"decoder_input_ids"</span>: decoder_input_ids,</span>
<span id="cb26-29"><a href="#cb26-29"></a>        <span class="st">"labels"</span>: labels,</span>
<span id="cb26-30"><a href="#cb26-30"></a>        <span class="st">"encoder_mask"</span>: create_padding_mask(encoder_input_ids, pad_id),</span>
<span id="cb26-31"><a href="#cb26-31"></a>        <span class="st">"decoder_mask"</span>: create_padding_mask(decoder_input_ids, pad_id),</span>
<span id="cb26-32"><a href="#cb26-32"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>在这个 <code>collate_fn</code> 中，我们还同时构造了Labels， Labels是Decoder输入序列右移一位得到的<code>[t[1:] for t in tgt_list]</code>，这样可以确保模型在训练时，能够正确地预测下一个词。</p>
<p>至此，我们的数据预处理和Batch准备工作就完成了，接下来我们来看一下模型的训练细节。</p>
</section>
</section>
<section id="weight-initialization" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="weight-initialization"><span class="header-section-number">3.2</span> Weight Initialization</h2>
<p>在论文中，没有提到如何Initialize的，在这里，我用Xavier initialization, 来初始化Transformer模型的权重参数。Xavier初始化旨在保持每层神经网络的输入和输出的方差相等，从而避免梯度消失或爆炸的问题。具体来说，假设我们有一个神经网络层，其输入维度为 <span class="math inline">\(n_{in}\)</span>，输出维度为 <span class="math inline">\(n_{out}\)</span>，那么Xavier初始化的权重矩阵 <span class="math inline">\(W\)</span> 的每个元素可以从以下均匀分布中采样:</p>
<p><span id="eq-xavier-initialization"><span class="math display">\[
W_{i,j} \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
\tag{32}\]</span></span></p>
<div class="sourceCode" id="cb27" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb27-2"><a href="#cb27-2"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb27-3"><a href="#cb27-3"></a>        nn.init.xavier_uniform_(module.weight)</span>
<span id="cb27-4"><a href="#cb27-4"></a>        <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb27-5"><a href="#cb27-5"></a>            nn.init.zeros_(module.bias)</span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb27-8"><a href="#cb27-8"></a>        nn.init.xavier_uniform_(module.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>具体的原因为什么Xavier initialization有效，在这里就不多赘述了，之后可能会有专门的文章来介绍这个内容。</p>
</section>
<section id="optimizer" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="optimizer"><span class="header-section-number">3.3</span> Optimizer</h2>
<p>Transformer的论文中，用的是Adam Optimizer <span class="citation" data-cites="AdamMethodStochastic2017kingma">(<a href="#ref-AdamMethodStochastic2017kingma" role="doc-biblioref">Kingma and Ba 2017</a>)</span>, 它是一种自适应学习率优化算法，结合了动量法和RMSProp的优点。Adam通过计算梯度的一阶矩估计（动量）和二阶矩估计（梯度的平方的指数加权平均）来调整每个参数的学习率，从而提高训练的稳定性和收敛速度。Adam的更新规则如下:</p>
<div id="algo-adam-wd" class="pseudocode-container quarto-float" data-line-number="true" data-comment-delimiter="#" data-indent-size="1.2em" data-caption-prefix="Algorithm" data-pseudocode-number="1" data-no-end="false" data-line-number-punc=":">
<div class="pseudocode">
\begin{algorithm} \caption{Adam with L2 Regularization(Weight Decay)} \begin{algorithmic} \Require Parameters $\theta_0$ \Require Learning rate $\alpha$, betas $(\beta_1,\beta_2)$ \Require Weight decay coefficient $\lambda$ \State Initialize $m_0 \gets 0$, $v_0 \gets 0$, $t \gets 0$ \While{not converged} \State $t \gets t + 1$ \State Compute gradient $g_t \gets \nabla_{\theta}\mathcal{L}(\theta_{t-1})$ \State Apply L2 regularization: $g_t^{\text{wd}} \gets g_t + \lambda \theta_{t-1}$ \State First moment: $m_t \gets \beta_1 m_{t-1} + (1-\beta_1) g_t^{\text{wd}}$ \State Second moment: $v_t \gets \beta_2 v_{t-1} + (1-\beta_2)\left(g_t^{\text{wd}}\right)^2$ \State Bias-corrected moments: $\hat m_t \gets \dfrac{m_t}{1-\beta_1^t}$, $\hat v_t \gets \dfrac{v_t}{1-\beta_2^t}$ \State Parameter update: $\theta_t \gets \theta_{t-1} - \alpha \dfrac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}$ \EndWhile \end{algorithmic} \end{algorithm}
</div>
</div>
<p>用Python实现Adam Optimizer如下:</p>
<div class="sourceCode" id="cb28" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="kw">class</span> Adam:</span>
<span id="cb28-2"><a href="#cb28-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.98</span>), weight_decay<span class="op">=</span><span class="fl">0.01</span>, eps<span class="op">=</span><span class="fl">1e-9</span>):</span>
<span id="cb28-3"><a href="#cb28-3"></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb28-4"><a href="#cb28-4"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb28-5"><a href="#cb28-5"></a>        <span class="va">self</span>.betas <span class="op">=</span> betas</span>
<span id="cb28-6"><a href="#cb28-6"></a>        <span class="va">self</span>.weight_decay <span class="op">=</span> weight_decay</span>
<span id="cb28-7"><a href="#cb28-7"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb28-8"><a href="#cb28-8"></a></span>
<span id="cb28-9"><a href="#cb28-9"></a>        <span class="va">self</span>.state <span class="op">=</span> {}</span>
<span id="cb28-10"><a href="#cb28-10"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-11"><a href="#cb28-11"></a>            <span class="va">self</span>.state[p] <span class="op">=</span> {</span>
<span id="cb28-12"><a href="#cb28-12"></a>                <span class="st">"step"</span>: <span class="dv">0</span>,</span>
<span id="cb28-13"><a href="#cb28-13"></a>                <span class="st">"m"</span>: torch.zeros_like(p.data),</span>
<span id="cb28-14"><a href="#cb28-14"></a>                <span class="st">"v"</span>: torch.zeros_like(p.data),</span>
<span id="cb28-15"><a href="#cb28-15"></a>            }</span>
<span id="cb28-16"><a href="#cb28-16"></a></span>
<span id="cb28-17"><a href="#cb28-17"></a>    <span class="kw">def</span> update_lr(<span class="va">self</span>, new_lr):</span>
<span id="cb28-18"><a href="#cb28-18"></a>        <span class="va">self</span>.lr <span class="op">=</span> new_lr</span>
<span id="cb28-19"><a href="#cb28-19"></a></span>
<span id="cb28-20"><a href="#cb28-20"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb28-21"><a href="#cb28-21"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-22"><a href="#cb28-22"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb28-23"><a href="#cb28-23"></a>                <span class="cf">continue</span></span>
<span id="cb28-24"><a href="#cb28-24"></a></span>
<span id="cb28-25"><a href="#cb28-25"></a>            grad <span class="op">=</span> p.grad.data</span>
<span id="cb28-26"><a href="#cb28-26"></a>            state <span class="op">=</span> <span class="va">self</span>.state[p]</span>
<span id="cb28-27"><a href="#cb28-27"></a></span>
<span id="cb28-28"><a href="#cb28-28"></a>            state[<span class="st">"step"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb28-29"><a href="#cb28-29"></a>            beta1, beta2 <span class="op">=</span> <span class="va">self</span>.betas</span>
<span id="cb28-30"><a href="#cb28-30"></a></span>
<span id="cb28-31"><a href="#cb28-31"></a>            <span class="co"># Update biased first moment estimate</span></span>
<span id="cb28-32"><a href="#cb28-32"></a>            state[<span class="st">"m"</span>] <span class="op">=</span> beta1 <span class="op">*</span> state[<span class="st">"m"</span>] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> grad</span>
<span id="cb28-33"><a href="#cb28-33"></a>            <span class="co"># Update biased second raw moment estimate</span></span>
<span id="cb28-34"><a href="#cb28-34"></a>            state[<span class="st">"v"</span>] <span class="op">=</span> beta2 <span class="op">*</span> state[<span class="st">"v"</span>] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> (grad <span class="op">*</span> grad)</span>
<span id="cb28-35"><a href="#cb28-35"></a></span>
<span id="cb28-36"><a href="#cb28-36"></a>            <span class="co"># Compute bias-corrected first moment estimate</span></span>
<span id="cb28-37"><a href="#cb28-37"></a>            m_hat <span class="op">=</span> state[<span class="st">"m"</span>] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta1 <span class="op">**</span> state[<span class="st">"step"</span>])</span>
<span id="cb28-38"><a href="#cb28-38"></a>            <span class="co"># Compute bias-corrected second raw moment estimate</span></span>
<span id="cb28-39"><a href="#cb28-39"></a>            v_hat <span class="op">=</span> state[<span class="st">"v"</span>] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta2 <span class="op">**</span> state[<span class="st">"step"</span>])</span>
<span id="cb28-40"><a href="#cb28-40"></a></span>
<span id="cb28-41"><a href="#cb28-41"></a>            <span class="co"># Update parameters</span></span>
<span id="cb28-42"><a href="#cb28-42"></a>            p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> m_hat <span class="op">/</span> (torch.sqrt(v_hat) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb28-43"><a href="#cb28-43"></a></span>
<span id="cb28-44"><a href="#cb28-44"></a>            <span class="co"># Apply weight decay</span></span>
<span id="cb28-45"><a href="#cb28-45"></a>            <span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb28-46"><a href="#cb28-46"></a>                p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.weight_decay <span class="op">*</span> p.data</span>
<span id="cb28-47"><a href="#cb28-47"></a></span>
<span id="cb28-48"><a href="#cb28-48"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb28-49"><a href="#cb28-49"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-50"><a href="#cb28-50"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb28-51"><a href="#cb28-51"></a>                p.grad.detach_()</span>
<span id="cb28-52"><a href="#cb28-52"></a>                p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Adam Optimizer</p>
</div>
<div class="note-container foldable-content">
<p>对于不了的Adam的同学，也不用太担心，之后我们会有一系列的文章，专门介绍这些优化器的，包括Adam<span class="citation" data-cites="AdamMethodStochastic2017kingma">(<a href="#ref-AdamMethodStochastic2017kingma" role="doc-biblioref">Kingma and Ba 2017</a>)</span>，AdamW<span class="citation" data-cites="DecoupledWeightDecay2019loshchilov">(<a href="#ref-DecoupledWeightDecay2019loshchilov" role="doc-biblioref">Loshchilov and Hutter 2019</a>)</span>，以及最近比较火的Muon<span class="citation" data-cites="jordan2024muon">(<a href="#ref-jordan2024muon" role="doc-biblioref">Jordan et al. 2024</a>)</span>等。</p>
</div>
</div>
<section id="learning-rate-scheduler" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="learning-rate-scheduler"><span class="header-section-number">3.3.1</span> Learning Rate Scheduler</h3>
<p>在Transformer的论文中，作者利用了一个自定义的学习率调度器（Learning Rate Scheduler），它在训练的初始阶段逐渐增加学习率，然后在达到预设的步数后逐渐减小学习率。具体来说，学习率的计算公式如下:</p>
<p><span id="eq-transformer-learning-rate"><span class="math display">\[
\text{lrate} = d_{model}^{-0.5} \cdot \min\left(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5}\right)
\tag{33}\]</span></span></p>
<p>其中，<span class="math inline">\(d_{model}\)</span> 是模型的隐藏层维度，<span class="math inline">\(step\_num\)</span> 是当前的训练步数，<span class="math inline">\(warmup\_steps\)</span> 是预设的预热步数。在训练的前 <span class="math inline">\(warmup\_steps\)</span> 步中，学习率线性增加；在之后的训练过程中，学习率按照 <span class="math inline">\(step\_num^{-0.5}\)</span> 的比例逐渐减小。</p>
<div id="fig-learning-rate-scheduler" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-learning-rate-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/learning-rate-scheduler.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-learning-rate-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Visualization of Learning Rate Scheduler
</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb29" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">def</span> get_lr(cur_step, warmup_steps, d_model):</span>
<span id="cb29-2"><a href="#cb29-2"></a>    lrate <span class="op">=</span> (d_model<span class="op">**-</span><span class="fl">0.5</span>) <span class="op">*</span> <span class="bu">min</span>(cur_step<span class="op">**-</span><span class="fl">0.5</span>, cur_step <span class="op">*</span> (warmup_steps<span class="op">**-</span><span class="fl">1.5</span>))</span>
<span id="cb29-3"><a href="#cb29-3"></a>    <span class="cf">return</span> lrate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="loss-function" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">3.4</span> Loss Function</h2>
<p>在Transformer模型的训练过程中，通常使用交叉熵损失函数（Cross Entropy Loss）作为主要的损失函数。交叉熵损失函数用于衡量模型预测的概率分布与真实标签分布之间的差异，具体来说，给定一个包含 <span class="math inline">\(N\)</span> 个样本的训练集，每个样本的真实标签为 <span class="math inline">\(y_i\)</span>，模型预测的概率分布为 <span class="math inline">\(\hat{y}_i\)</span>，交叉熵损失函数的计算公式如下:</p>
<p><span id="eq-cross-entropy-loss"><span class="math display">\[
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
\tag{34}\]</span></span></p>
<p>其中，<span class="math inline">\(C\)</span> 是类别的总数，<span class="math inline">\(y_{i,c}\)</span> 是样本 <span class="math inline">\(i\)</span> 在类别 <span class="math inline">\(c\)</span> 上的真实标签（one-hot encoding），<span class="math inline">\(\hat{y}_{i,c}\)</span> 是模型对样本 <span class="math inline">\(i\)</span> 在类别 <span class="math inline">\(c\)</span> 上的预测概率。</p>
<p>结合Label Smoothing <a href="#eq-label-smoothing" class="quarto-xref">Equation&nbsp;31</a>, 交叉熵损失函数的计算公式可以调整为:</p>
<p><span id="eq-cross-entropy-loss-smooth"><span class="math display">\[
\mathcal{L}_{smooth} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c}^{smooth} \log(\hat{y}_{i,c})
\tag{35}\]</span></span> 其中，<span class="math inline">\(y_{i,c}^{smooth}\)</span> 是经过Label Smoothing处理后的目标标签。</p>
<div class="sourceCode" id="cb30" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="kw">def</span> cross_entropy_loss(logits, labels, ignore_index<span class="op">=</span><span class="dv">0</span>, label_smoothing<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="co"># Create mask for ignore_index</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>    mask <span class="op">=</span> labels <span class="op">!=</span> ignore_index</span>
<span id="cb30-4"><a href="#cb30-4"></a>    num_classes <span class="op">=</span> logits.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb30-5"><a href="#cb30-5"></a></span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="cf">if</span> label_smoothing <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb30-7"><a href="#cb30-7"></a>        smooth_labels <span class="op">=</span> F.one_hot(labels, num_classes).<span class="bu">float</span>()</span>
<span id="cb30-8"><a href="#cb30-8"></a>        smooth_labels <span class="op">=</span> smooth_labels <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> label_smoothing) <span class="op">+</span> label_smoothing <span class="op">/</span> num_classes</span>
<span id="cb30-9"><a href="#cb30-9"></a>    <span class="cf">else</span>:</span>
<span id="cb30-10"><a href="#cb30-10"></a>        smooth_labels <span class="op">=</span> F.one_hot(labels, num_classes).<span class="bu">float</span>()</span>
<span id="cb30-11"><a href="#cb30-11"></a></span>
<span id="cb30-12"><a href="#cb30-12"></a>    log_probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb30-13"><a href="#cb30-13"></a>    loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(smooth_labels <span class="op">*</span> log_probs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb30-14"><a href="#cb30-14"></a>    loss <span class="op">=</span> loss <span class="op">*</span> mask.<span class="bu">float</span>()</span>
<span id="cb30-15"><a href="#cb30-15"></a>    loss <span class="op">=</span> loss.<span class="bu">sum</span>() <span class="op">/</span> mask.<span class="bu">sum</span>()</span>
<span id="cb30-16"><a href="#cb30-16"></a></span>
<span id="cb30-17"><a href="#cb30-17"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="evaluation-metric" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evaluation-metric"><span class="header-section-number">3.5</span> Evaluation Metric</h2>
<section id="blue" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="blue"><span class="header-section-number">3.5.1</span> BLUE</h3>
<p>在评估Transformer模型的性能时，通常使用BLEU（Bilingual Evaluation Understudy）分数作为主要的评价指标。BLEU分数是一种用于评估机器翻译质量的自动化指标，通过比较机器生成的翻译与一个或多个参考翻译之间的相似度来衡量翻译的准确性。BLEU分数的计算过程包括以下几个步骤:</p>
<ol type="1">
<li><strong>N-gram匹配</strong>: 计算机器翻译输出与参考翻译之间的n-gram匹配数量，通常考虑1-gram到4-gram。</li>
<li><strong>精确率计算</strong>: 对每个n-gram，计算匹配的n-gram数量与机器翻译输出中n-gram总数的比值，得到精确率。</li>
<li><strong>几何平均</strong>: 将各个n-gram的精确率进行几何平均，以综合考虑不同长度的n-gram匹配情况。</li>
<li><strong>长度惩罚</strong>: 为了防止机器翻译输出过短，BLEU分数引入了长度惩罚项，根据机器翻译输出的长度与参考翻译的长度进行调整。</li>
<li><strong>最终计算</strong>: 将几何平均的精确率与长度惩罚相乘，得到最终的BLEU分数，范围在0到1之间，通常表示为百分比形式。</li>
</ol>
<p>计算BLEU分数的公式如下: <span id="eq-bleu-score"><span class="math display">\[
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\tag{36}\]</span></span></p>
<p>其中，<span class="math inline">\(\text{BP}\)</span> 是长度惩罚项，<span class="math inline">\(p_n\)</span> 是n-gram的精确率，<span class="math inline">\(w_n\)</span> 是n-gram的权重，通常均匀分配。</p>
<div class="sourceCode" id="cb31" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="im">import</span> nltk</span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="im">from</span> nltk.translate.bleu_score <span class="im">import</span> sentence_bleu, SmoothingFunction</span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="kw">def</span> compute_bleu(reference, candidate):</span>
<span id="cb31-4"><a href="#cb31-4"></a>    reference_tokens <span class="op">=</span> [reference.split()]</span>
<span id="cb31-5"><a href="#cb31-5"></a>    candidate_tokens <span class="op">=</span> candidate.split()</span>
<span id="cb31-6"><a href="#cb31-6"></a>    smoothing_function <span class="op">=</span> SmoothingFunction().method1</span>
<span id="cb31-7"><a href="#cb31-7"></a>    bleu_score <span class="op">=</span> sentence_bleu(reference_tokens, candidate_tokens,</span>
<span id="cb31-8"><a href="#cb31-8"></a>                               weights<span class="op">=</span>(<span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>),</span>
<span id="cb31-9"><a href="#cb31-9"></a>                               smoothing_function<span class="op">=</span>smoothing_function)</span>
<span id="cb31-10"><a href="#cb31-10"></a>    <span class="cf">return</span> bleu_score <span class="op">*</span> <span class="dv">100</span>  <span class="co"># Convert to percentage</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="perplexity" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="perplexity"><span class="header-section-number">3.5.2</span> Perplexity</h3>
<p>困惑度（Perplexity）是评估语言模型性能的常用指标，用于衡量模型对给定文本序列的预测能力。困惑度的定义是语言模型对测试集上每个词的平均不确定性，数值越低表示模型对文本的预测越准确。具体来说，给定一个测试集 <span class="math inline">\(W = w_1, w_2, \ldots, w_N\)</span>，语言模型计算该序列的概率 <span class="math inline">\(P(W)\)</span>，困惑度的计算公式如下:</p>
<p><span id="eq-perplexity"><span class="math display">\[
\text{Perplexity}(W) = P(W)^{-\frac{1}{N}} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, w_2, \ldots, w_{i-1})\right)
\tag{37}\]</span></span></p>
<p>其中，<span class="math inline">\(N\)</span> 是测试集中的词数，<span class="math inline">\(P(w_i | w_1, w_2, \ldots, w_{i-1})\)</span> 是语言模型预测第 <span class="math inline">\(i\)</span> 个词的条件概率。困惑度可以理解为模型在预测下一个词时面临的选择数量的指数级增长。</p>
<p>在实际计算中，困惑度通常通过交叉熵损失来间接计算: <span id="eq-perplexity-cross-entropy"><span class="math display">\[
\text{Perplexity}(W) = \exp(\text{CrossEntropyLoss})
\tag{38}\]</span></span></p>
<p>在之前，我们有提到Label Smoothing, 它会影响困惑度的计算，因为Label Smoothing会改变目标分布，从而影响交叉熵损失的计算，进而影响困惑度的数值。因此，在使用Label Smoothing时，困惑度的数值可能会有所偏差，需要谨慎解释。</p>
<div id="fig-perplexity-label-smoothing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-perplexity-label-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/perplexity-label-smoothing.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-perplexity-label-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: 从图中可以看出，随着 <span class="math inline">\(\epsilon\)</span> 的增加，Perplexity 也在增加。这是因为 Label Smoothing 引入了噪声，使得模型在训练过程中更难以准确预测目标词，从而导致交叉熵损失增加。
</figcaption>
</figure>
</div>
</section>
</section>
<section id="training" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="training"><span class="header-section-number">3.6</span> Training</h2>
<p>在训练Transformer模型，我们设置了以下超参数:</p>
<div class="sourceCode" id="cb32" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="at">@dataclass</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="kw">class</span> ModelConfig:</span>
<span id="cb32-3"><a href="#cb32-3"></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> VOCAB_SIZE</span>
<span id="cb32-4"><a href="#cb32-4"></a>    max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb32-5"><a href="#cb32-5"></a></span>
<span id="cb32-6"><a href="#cb32-6"></a>    d_model: <span class="bu">int</span> <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb32-7"><a href="#cb32-7"></a>    d_ff: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb32-8"><a href="#cb32-8"></a>    num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-9"><a href="#cb32-9"></a>    num_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb32-10"><a href="#cb32-10"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-11"><a href="#cb32-11"></a></span>
<span id="cb32-12"><a href="#cb32-12"></a><span class="at">@dataclass</span></span>
<span id="cb32-13"><a href="#cb32-13"></a><span class="kw">class</span> TrainConfig:</span>
<span id="cb32-14"><a href="#cb32-14"></a>    batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb32-15"><a href="#cb32-15"></a>    gradient_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-16"><a href="#cb32-16"></a>    total_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10_000</span>  <span class="co"># set to 0 for automatic calculation</span></span>
<span id="cb32-17"><a href="#cb32-17"></a>    warmup_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># will be set by total_steps // 10</span></span>
<span id="cb32-18"><a href="#cb32-18"></a></span>
<span id="cb32-19"><a href="#cb32-19"></a>    lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">5e-3</span></span>
<span id="cb32-20"><a href="#cb32-20"></a>    min_lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb32-21"><a href="#cb32-21"></a>    betas: <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="kw">lambda</span>: (<span class="fl">0.9</span>, <span class="fl">0.98</span>))</span>
<span id="cb32-22"><a href="#cb32-22"></a>    weight_decay: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb32-23"><a href="#cb32-23"></a>    optim_eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb32-24"><a href="#cb32-24"></a></span>
<span id="cb32-25"><a href="#cb32-25"></a>    label_smoothing: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-26"><a href="#cb32-26"></a></span>
<span id="cb32-27"><a href="#cb32-27"></a>    debug: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb32-28"><a href="#cb32-28"></a>    device <span class="op">=</span> get_device()</span>
<span id="cb32-29"><a href="#cb32-29"></a>    mixed_precision: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb32-30"><a href="#cb32-30"></a>    eval_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>在训练过程中，我们使用了Mixed Precision Training来加速训练过程并减少显存占用。混合精度训练通过在计算过程中使用16位浮点数（FP16）和32位浮点数（FP32）的组合，既保持了模型的精度，又提高了计算效率。具体来说，模型的前向传播和反向传播主要使用FP16进行计算，而关键的参数更新和梯度累积则使用FP32，以确保数值稳定性。</p>
<p>同时，我们还采用了Gradient Accumulation技术，以便在显存有限的情况下使用较大的有效批量大小进行训练。梯度累积的基本思想是将多个小批量的梯度累积起来，然后再进行一次参数更新。具体来说，假设我们希望使用一个较大的批量大小 <span class="math inline">\(B\)</span> 进行训练，但由于显存限制，我们只能使用一个较小的批量大小 <span class="math inline">\(b\)</span>，那么我们可以将 <span class="math inline">\(B/b\)</span> 个小批量的梯度累积起来，然后再进行一次参数更新。</p>
<p>简单来看，我们的训练循环如下:</p>
<div class="sourceCode" id="cb33" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb33-2"><a href="#cb33-2"></a>    optimizer.zero_grad()</span>
<span id="cb33-3"><a href="#cb33-3"></a>    <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(gradient_steps):</span>
<span id="cb33-4"><a href="#cb33-4"></a>        <span class="cf">with</span> torch.autocast(</span>
<span id="cb33-5"><a href="#cb33-5"></a>            device_type<span class="op">=</span>train_config.device.<span class="bu">type</span>, enabled<span class="op">=</span>train_config.mixed_precision, dtype<span class="op">=</span>torch.bfloat16</span>
<span id="cb33-6"><a href="#cb33-6"></a>        ):</span>
<span id="cb33-7"><a href="#cb33-7"></a>            logits <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb33-8"><a href="#cb33-8"></a>            loss <span class="op">=</span> cross_entropy_loss(</span>
<span id="cb33-9"><a href="#cb33-9"></a>                logits,</span>
<span id="cb33-10"><a href="#cb33-10"></a>                labels,</span>
<span id="cb33-11"><a href="#cb33-11"></a>                ignore_index<span class="op">=</span>translation_dataset.pad_id,</span>
<span id="cb33-12"><a href="#cb33-12"></a>                label_smoothing<span class="op">=</span>train_config.label_smoothing,</span>
<span id="cb33-13"><a href="#cb33-13"></a>            )</span>
<span id="cb33-14"><a href="#cb33-14"></a>            loss <span class="op">/=</span> train_config.gradient_steps</span>
<span id="cb33-15"><a href="#cb33-15"></a>        loss.backward()</span>
<span id="cb33-16"><a href="#cb33-16"></a>    </span>
<span id="cb33-17"><a href="#cb33-17"></a>    optimizer.update_lr()</span>
<span id="cb33-18"><a href="#cb33-18"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: OOM Error</p>
</div>
<div class="note-container foldable-content">
<p>如果大家在训练的过程中，遇到了OOM Error，我们可以调小我们的Batch Size，同时增大我们的Gradient Steps，这样可以保持最终的Batch Size不变。</p>
</div>
</div>
</section>
<section id="results" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="results"><span class="header-section-number">3.7</span> Results</h2>
<div id="fig-loss-curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/Transformer-loss.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: The Loss Curve of Transformer training for 10,000 steps
</figcaption>
</figure>
</div>
<p>有一个有趣的现象就是，Loss的下降呈现 zig-zag pattern，这个现象在很多NLP模型的训练中都会出现，但是我还没有找到一个很好的解释，可能是Learning Rate 调节的原因，也可能是Dataloader的问题，有时间我就探索这个问题的。欢迎大家在评论区留言讨论！</p>
<p>下面是训练完10,000步后一个翻译的例子:</p>
<div class="sourceCode" id="cb34" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb34-1"><a href="#cb34-1"></a>English Input: Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow</span>
<span id="cb34-2"><a href="#cb34-2"></a>challenge.</span>
<span id="cb34-3"><a href="#cb34-3"></a>Model Output: &lt;s&gt; 几年前,在TED, Peter Skillman介绍了一个设计挑战  叫做杨饼干的挑战-- &lt;/s&gt;</span>
<span id="cb34-4"><a href="#cb34-4"></a>Reference: 几年前，在这里的 TED 上，Peter Skillman 提出了一个名为“棉花糖挑战”的设计挑战。</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>一些简单的例子</p>
<div class="sourceCode" id="cb35" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb35-1"><a href="#cb35-1"></a>English Input: Who are you?</span>
<span id="cb35-2"><a href="#cb35-2"></a>Model Output: &lt;s&gt; 你是谁?&lt;/s&gt;</span>
<span id="cb35-3"><a href="#cb35-3"></a>English Input: What is your name?</span>
<span id="cb35-4"><a href="#cb35-4"></a>Model Output: &lt;s&gt; 你叫什么?&lt;/s&gt;</span>
<span id="cb35-5"><a href="#cb35-5"></a>English Input: I love Artificial Intelligence.</span>
<span id="cb35-6"><a href="#cb35-6"></a>Model Output: &lt;s&gt; 我喜欢魅力。&lt;/s&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>可以看到，我们的模型已经能够进行简单的英文到中文的翻译了，当然距离实际应用还有很大的差距，比如翻译的流畅度和准确度还需要提升，模型的规模也需要更大，训练的数据也需要更多等等。</p>
</section>
</section>
<section id="summary" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Summary</h1>
<p>在这篇文章中，我们详细介绍了Transformer模型：Transformer 是一种以注意力机制为核心的序列建模架构，它用 Attention 在全局范围内直接建模 token 之间的依赖关系，从而摆脱了 RNN/CNN 的顺序计算瓶颈并实现高效并行。在解码端通过 Masked Self-Attention 屏蔽未来信息，保证自回归生成的因果性；同时用 Positional Encoding 将顺序信息注入表示，使模型在无循环结构下仍能理解位置与相对次序。整体采用经典的 Encoder–Decoder 结构：编码器通过自注意力提取源序列上下文，解码器结合自注意力与编码器输出进行条件生成。为稳定深层训练与提升表达能力，Transformer 在每个子层引入 Residual Connections 与 Layer Normalization 来改善梯度传播与数值稳定性，并使用位置独立的 Feed Forward Network 提供非线性特征变换与表示增强。</p>
<p>同时，我们还介绍了Transformer模型在机器翻译任务中的具体实现细节，包括</p>
<ul>
<li>数据预处理、</li>
<li>模型训练</li>
<li>Adam 的实现</li>
<li>以及评估指标等内容。</li>
</ul>
<p>通过这些细节的介绍，可以更好地理解Transformer模型的工作原理以及如何在实际应用中进行训练和评估。</p>
<p>总之，关于Transformer以及后续的改进，有太多太多可以讲的了，比如：</p>
<ul>
<li>Position Encoding的变体</li>
<li>Normalization的变体，以及位置</li>
<li>Attention的优化</li>
<li>Feed Forward Network的改进</li>
<li>并行的训练与推理技术</li>
<li>如何微调预训练的Transformer模型</li>
</ul>
<p>在第一篇文章中，我们只能介绍Transformer的基础内容，后续我们会有更多的文章，来介绍这些内容，敬请期待！</p>
</section>
<section id="key-concepts" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Key Concepts</h1>
<div id="tbl-transformer-key-concepts" class="hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-transformer-key-concepts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table">
<colgroup>
<col style="width: 41%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Self-Attention</strong></td>
<td style="text-align: left;">Query、Key、Value 都来自同一序列，通过计算 <span class="math inline">\(QK^\top\)</span> 得到任意两个 token 之间的相似度，再对 Value 做加权求和（式 <a href="#eq-attention" class="quarto-xref">Equation&nbsp;11</a>）。它不依赖距离，一次矩阵乘法就能让每个 token 直接“看到”序列中所有位置，是 Transformer 能建模长距离依赖的核心原因。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Scaled Dot-Product Attention</strong></td>
<td style="text-align: left;">在点积注意力中引入 <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> 缩放（<a href="#eq-attention" class="quarto-xref">Equation&nbsp;11</a>），是因为当 <span class="math inline">\(d_k\)</span> 较大时，<span class="math inline">\(QK^\top\)</span> 的方差随维度线性增大（<a href="#eq-attention-dot-product-variance" class="quarto-xref">Equation&nbsp;13</a>），会导致 softmax 进入饱和区、梯度接近 0。缩放相当于对 logits 做方差归一化，保证梯度处在可学习区间（见 scaling-d_k 图）。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Multi-Head Attention</strong></td>
<td style="text-align: left;">将 <span class="math inline">\(d_{model}\)</span> 拆成多个 head，每个 head 都有独立的 <span class="math inline">\(W^Q,W^K,W^V\)</span>（<a href="#eq-multi-head-attention-head" class="quarto-xref">Equation&nbsp;17</a>），在不同表示子空间并行做注意力；单头只能学一种加权模式，多头能同时学局部对齐、长距依赖、语义聚合等多种关系，最后 concat 再线性映射（<a href="#eq-multi-head-attention" class="quarto-xref">Equation&nbsp;16</a>）。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Positional Encoding</strong></td>
<td style="text-align: left;">Transformer 本身是 permutation-invariant，需要显式注入位置信息。文章使用正弦/余弦绝对位置编码（<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>），不同维度对应不同频率：低维高频刻画局部位置，高维低频刻画全局位置；与词向量相加而非拼接，保持维度不变、计算复杂度不增加，并允许外推到更长序列。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Encoder–Decoder Attention</strong></td>
<td style="text-align: left;">Cross-Attention 中 Query 来自解码器，Key/Value 来自编码器输出（<a href="#eq-cross-attention" class="quarto-xref">Equation&nbsp;21</a>）。这一步本质是在生成每个目标词时，对源句做一次信息检索与对齐，是机器翻译中“看源句哪里最相关”的数学实现。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Masked (Causal) Self-Attention</strong></td>
<td style="text-align: left;">在解码器自注意力中加入上三角 mask（<a href="#eq-mask-matrix" class="quarto-xref">Equation&nbsp;20</a>），把未来位置的 logits 设为 <span class="math inline">\(-\infty\)</span>，softmax 后概率为 0（<a href="#eq-causal-attention" class="quarto-xref">Equation&nbsp;19</a>）。这样保证模型在位置 <span class="math inline">\(i\)</span> 只能访问 <span class="math inline">\(j\le i\)</span> 的信息，严格满足自回归生成，不发生信息泄露（见 causal mask 图）。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Position-wise Feed Forward Network</strong></td>
<td style="text-align: left;">FFN 是对每个 token 独立应用的两层 MLP（<a href="#eq-feed-forward-network" class="quarto-xref">Equation&nbsp;24</a>），中间维度 <span class="math inline">\(d_{ff}\approx 4d_{model}\)</span>。Attention 负责“跨 token 混信息”，FFN 负责“在单 token 维度上做非线性特征变换”，两者分工明确、互补。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Residual Connection + LayerNorm</strong></td>
<td style="text-align: left;">每个子层采用 <span class="math inline">\(x + \text{sublayer}(x)\)</span> 的残差结构，再做 LayerNorm（<a href="#eq-residual-connection" class="quarto-xref">Equation&nbsp;25</a>）。反向传播时梯度包含一条“直通路径”（<a href="#eq-backprop-residual" class="quarto-xref">Equation&nbsp;26</a>），有效缓解梯度消失；LayerNorm 在特征维归一化，稳定激活分布，使深层 Transformer 可训练。</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Attention Time Complexity</strong></td>
<td style="text-align: left;">Self-Attention 的主成本来自 <span class="math inline">\(QK^\top\)</span> 和 <span class="math inline">\(AV\)</span>，时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>（<a href="#eq-self-attention-time-complexity" class="quarto-xref">Equation&nbsp;22</a>）。相比 RNN 的 <span class="math inline">\(\mathcal{O}(nd^2)\)</span>，Transformer 的优势不在理论阶数，而在矩阵化并行计算，这也是后续 FlashAttention、Sparse/Linear Attention 研究的根源。</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-transformer-key-concepts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Key Concepts in Transformer
</figcaption>
</figure>
</div>
</section>
<section id="q-a" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Q &amp; A</h1>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 1: 为什么注意力要除以 <span class="math inline">\(\sqrt{d_k}\)</span> ？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 因为 <span class="math inline">\(d_k\)</span> 变大时点积的方差会变大，softmax 更容易饱和 <a href="#fig-scaling-d-k" class="quarto-xref">Figure&nbsp;3</a>（某些位置权重接近 1、其余接近 0），导致梯度小 <a href="#fig-scaling-d-k-gradient" class="quarto-xref">Figure&nbsp;4</a>、训练不稳定；缩放能把数值拉回合适范围。</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 2: 多头注意力相比单头注意力“本质上”多了什么能力？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 单头注意力在一个表示子空间里做一次全局加权；多头把表示拆成多个子空间并行做 attention，等价于同时学习多种关系模式（例如局部依存、长距离指代、语义聚合等），再融合，表达力更强且更稳健。</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 3: 没有 RNN/CNN，Transformer 怎么知道“顺序”？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 通过把位置编码（Positional Encoding） <a href="#sec-postion-embedding" class="quarto-xref">Section&nbsp;2.2</a> 加到输入 embedding 上，让模型在注意力计算中可利用位置信息；论文用正弦/余弦的固定编码实现。</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 4: Decoder 为什么必须 Mask？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 训练时目标序列是已知的，但生成时必须逐步预测；如果不 mask，模型训练时能看到未来 token，会造成“训练-推理不一致”，并破坏自回归建模。</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 5: Transformer 的计算复杂度瓶颈在哪里？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 自注意力需要构造 <span class="math inline">\(n \times n\)</span> 的注意力矩阵 <a href="#sec-attention-complexity" class="quarto-xref">Section&nbsp;2.3.5</a>，时间与内存对序列长度 <span class="math inline">\(n\)</span> 是二次的；这也是后来 Longformer/Performer/FlashAttention/线性注意力等工作的动机之一。</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 6: FFN 为什么是“对每个位置独立”？</p>
</div>
<div class="question-container foldable-content">
<p>Answer: 注意力负责“跨位置的信息混合”，FFN 负责“每个位置的非线性特征变换”；二者分工清晰，且位置独立计算更易并行与实现。</p>
</div>
</div>
</section>
<section id="related-resource-further-reading" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Related resource &amp; Further Reading</h1>
<ul>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a>: A step-by-step explanation of the Transformer model with code snippets.</li>
<li><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a>: A comprehensive overview of the Transformer architecture and its variants.</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>: A visual and intuitive explanation of the Transformer model.</li>
</ul>
<hr>
<p>我们知道，Transformer的核心是Attention机制，后续有许多工作，都是围绕着优化Attention机制展开的，下面是一些比较重要的Transformer的变种:</p>
<ul>
<li>Flash-Attention<span class="citation" data-cites="FlashAttentionFastMemoryEfficient2022dao">(<a href="#ref-FlashAttentionFastMemoryEfficient2022dao" role="doc-biblioref">Dao et al. 2022</a>)</span>: 一种高效的注意力计算方法，利用分块和内存优化技术，显著减少计算时间和内存使用。</li>
<li>Multi-Query Attention / Grouped Query Attention: 一种改进的多头注意力机制，通过共享查询向量来减少计算复杂度，同时保持模型性能。</li>
<li>Linear Attention: 一种线性时间复杂度的注意力机制，通过将注意力计算从二次复杂度降低到线性复杂度，提高了长序列处理的效率。</li>
<li>Native Sparse Attention <span class="citation" data-cites="NativeSparseAttention2025yuan">(<a href="#ref-NativeSparseAttention2025yuan" role="doc-biblioref">Yuan et al. 2025</a>)</span>: 一种稀疏注意力机制，通过只计算部分注意力权重来提高效率，适用于长序列处理。</li>
<li>KV Cache: 一种缓存机制，通过存储过去的键值对来加速自回归生成过程，减少重复计算。</li>
</ul>
<p>除此之外，还有一些Transformer的改进方向:</p>
<ul>
<li>Feed Forward Network
<ul>
<li>Mixture of Expert <span class="citation" data-cites="SwitchTransformersScaling2022fedus">(<a href="#ref-SwitchTransformersScaling2022fedus" role="doc-biblioref">Fedus, Zoph, and Shazeer 2022</a>)</span>: 一种基于专家模型的前馈网络，通过动态选择专家子网络来提高模型的表达能力和计算效率。</li>
</ul></li>
<li>Normalization:
<ul>
<li>RMS-Normalization <span class="citation" data-cites="RootMeanSquare2019zhang">(<a href="#ref-RootMeanSquare2019zhang" role="doc-biblioref">Zhang and Sennrich 2019</a>)</span>: 一种归一化方法，通过计算输入的均方根值来进行归一化，具有更好的数值稳定性和训练效果。</li>
</ul></li>
<li>Position Embedding:
<ul>
<li>RoPE<span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span>: 一种旋转位置编码方法，通过旋转嵌入向量来捕捉相对位置信息，提高模型对长序列的处理能力。</li>
</ul></li>
</ul>
<p>我们也可以将Transformer运用到不同的 Modality中，比如Vision， 这就是我们下一篇要学习的Vision Transformer<span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>。</p>
<p>总之，Transformer最为一个强大的网络架构，已经被广泛应用于自然语言处理、计算机视觉等多个领域，并且仍在不断发展和演进中。希望通过这篇文章，大家能够对Transformer有一个全面的了解，并能够在实际应用中灵活运用这一强大的工具。在2026年的AI中，Transformer无疑将继续发挥其重要作用，推动人工智能技术的进一步发展。</p>
</section>
<section id="in-the-end" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> In the end</h1>
<p>创作不易，如果你觉得内容对你有帮助，欢迎请我 <span class="hilite-teal">喝杯咖啡/支付宝红包</span>，支持我继续创作！你们的支持是我最大的动力！ :) <br> <img src="../../../style/AliPay.jpg" class="img-fluid" width="300"></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-LayerNormalization2016ba" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>“Layer <span>Normalization</span>.”</span> July 21, 2016. <a href="https://doi.org/10.48550/arXiv.1607.06450">https://doi.org/10.48550/arXiv.1607.06450</a>.
</div>
<div id="ref-FlashAttentionFastMemoryEfficient2022dao" class="csl-entry" role="listitem">
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. <span>“<span>FlashAttention</span>: <span>Fast</span> and <span>Memory-Efficient Exact Attention</span> with <span>IO-Awareness</span>.”</span> June 23, 2022. <a href="https://doi.org/10.48550/arXiv.2205.14135">https://doi.org/10.48550/arXiv.2205.14135</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.”</span> June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-SwitchTransformersScaling2022fedus" class="csl-entry" role="listitem">
Fedus, William, Barret Zoph, and Noam Shazeer. 2022. <span>“Switch <span>Transformers</span>: <span>Scaling</span> to <span>Trillion Parameter Models</span> with <span>Simple</span> and <span>Efficient Sparsity</span>.”</span> June 16, 2022. <a href="https://doi.org/10.48550/arXiv.2101.03961">https://doi.org/10.48550/arXiv.2101.03961</a>.
</div>
<div id="ref-DeepResidualLearning2015he" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep <span>Residual Learning</span> for <span>Image Recognition</span>.”</span> December 10, 2015. <a href="https://doi.org/10.48550/arXiv.1512.03385">https://doi.org/10.48550/arXiv.1512.03385</a>.
</div>
<div id="ref-jordan2024muon" class="csl-entry" role="listitem">
Jordan, Keller, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. 2024. <span>“Muon: An Optimizer for Hidden Layers in Neural Networks.”</span> <a href="https://kellerjordan.github.io/posts/muon/">https://kellerjordan.github.io/posts/muon/</a>.
</div>
<div id="ref-AdamMethodStochastic2017kingma" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>“Adam: <span>A Method</span> for <span>Stochastic Optimization</span>.”</span> January 30, 2017. <a href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</a>.
</div>
<div id="ref-DecoupledWeightDecay2019loshchilov" class="csl-entry" role="listitem">
Loshchilov, Ilya, and Frank Hutter. 2019. <span>“Decoupled <span>Weight Decay Regularization</span>.”</span> January 4, 2019. <a href="https://doi.org/10.48550/arXiv.1711.05101">https://doi.org/10.48550/arXiv.1711.05101</a>.
</div>
<div id="ref-NeuralMachineTranslation2016sennrich" class="csl-entry" role="listitem">
Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. <span>“Neural <span>Machine Translation</span> of <span>Rare Words</span> with <span>Subword Units</span>.”</span> June 10, 2016. <a href="https://doi.org/10.48550/arXiv.1508.07909">https://doi.org/10.48550/arXiv.1508.07909</a>.
</div>
<div id="ref-RoFormerEnhancedTransformer2023su" class="csl-entry" role="listitem">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. <span>“<span>RoFormer</span>: <span>Enhanced Transformer</span> with <span>Rotary Position Embedding</span>.”</span> November 8, 2023. <a href="https://doi.org/10.48550/arXiv.2104.09864">https://doi.org/10.48550/arXiv.2104.09864</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-NativeSparseAttention2025yuan" class="csl-entry" role="listitem">
Yuan, Jingyang, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, et al. 2025. <span>“Native <span>Sparse Attention</span>: <span>Hardware-Aligned</span> and <span>Natively Trainable Sparse Attention</span>.”</span> February 27, 2025. <a href="https://doi.org/10.48550/arXiv.2502.11089">https://doi.org/10.48550/arXiv.2502.11089</a>.
</div>
<div id="ref-RootMeanSquare2019zhang" class="csl-entry" role="listitem">
Zhang, Biao, and Rico Sennrich. 2019. <span>“Root <span>Mean Square Layer Normalization</span>.”</span> October 16, 2019. <a href="https://doi.org/10.48550/arXiv.1910.07467">https://doi.org/10.48550/arXiv.1910.07467</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/sta210-s22\.github\.io\/website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="pagination-link" aria-label="Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="pagination-link" aria-label="Vision Transformer">
        <span class="nav-page-text">Vision Transformer</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>