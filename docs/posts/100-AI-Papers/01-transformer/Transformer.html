<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="YY Zhang">
<meta name="description" content="Transformer æ˜¯ä¸€ç§åŸºäº è‡ªæ³¨æ„åŠ›æœºåˆ¶ çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›å¹¶ä¸”ä½œä¸º GPTã€BERT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ Transformer çš„åŸºæœ¬åŸç†ï¼Œä»¥åŠå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬ Word Embeddingã€Position Embeddingã€Attentionã€Normalization Layer å’Œ Feed Forward Layerã€‚å¹¶é€šè¿‡åœ¨ Ted Talks æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œå±•ç¤º Transformer åœ¨å®é™…ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœã€‚">

<title>01: Attention is All You Need (Transformer) â€“ Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" rel="next">
<link href="../../../posts/100-AI-Papers/100_Papers_index.html" rel="prev">
<link href="../../.././style/icon.avif" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-01e1ab90bb0902c54ef2dc3889c8698d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-987788bda8b0cf97ae4c62c0f40f22af.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-01e1ab90bb0902c54ef2dc3889c8698d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-E8EJCZTZG1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-E8EJCZTZG1', { 'anonymize_ip': true});
</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // å¯è®¿é—®æ€§ï¼ˆé”®ç›˜ï¼‰
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">01: Attention is All You Need (<tag style="color:orange">Transformer</tag>)</h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../.././style/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/YYZhang2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/zhang-yuyang/" title="LinkedIn" class="quarto-navigation-tool px-1" aria-label="LinkedIn"><i class="bi bi-linkedin"></i></a>
    <a href="https://yyzhang2025.github.io/posts/Blogs/blogs_index.html" title="Personal Blogs" class="quarto-navigation-tool px-1" aria-label="Personal Blogs"><i class="bi bi-globe"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About website</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">ğŸ“ 100 AI Papers with Code</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/100_Papers_index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">001: Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">002: Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">ğŸ“ Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this course</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Ass01/ass01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 01: BPE Tokenizer &amp; Transformer LM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Ass02/ass02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 02: Flash Attention &amp; Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Ass05/ass05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 05: SFT &amp; GRPO</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">ğŸ“– Deep Learning Foundation &amp; Concepts</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/DLFaC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this book</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/DLFaC/Chapter01/Chapter01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 01: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/DLFaC/Chapter02/Chapter02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 02: Probability and Information Theory</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function"><span class="header-section-number">1.1</span> Softmax Function</a></li>
  <li><a href="#vector-similarity" id="toc-vector-similarity" class="nav-link" data-scroll-target="#vector-similarity"><span class="header-section-number">1.2</span> Vector Similarity</a></li>
  </ul></li>
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer"><span class="header-section-number">2</span> Transformer</a>
  <ul>
  <li><a href="#sec-word-embedding" id="toc-sec-word-embedding" class="nav-link" data-scroll-target="#sec-word-embedding"><span class="header-section-number">2.1</span> Word Embedding Layer</a></li>
  <li><a href="#sec-postion-embedding" id="toc-sec-postion-embedding" class="nav-link" data-scroll-target="#sec-postion-embedding"><span class="header-section-number">2.2</span> Position Embedding Layer</a>
  <ul>
  <li><a href="#why-sinusoidal-position-embedding" id="toc-why-sinusoidal-position-embedding" class="nav-link" data-scroll-target="#why-sinusoidal-position-embedding"><span class="header-section-number">2.2.1</span> Why Sinusoidal Position Embedding?</a></li>
  </ul></li>
  <li><a href="#sec-attention" id="toc-sec-attention" class="nav-link" data-scroll-target="#sec-attention"><span class="header-section-number">2.3</span> Attention Layer</a>
  <ul>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">2.3.1</span> Multi-Head Attention</a></li>
  <li><a href="#self-attention-layer" id="toc-self-attention-layer" class="nav-link" data-scroll-target="#self-attention-layer"><span class="header-section-number">2.3.2</span> Self-Attention Layer</a></li>
  <li><a href="#causal-self-attention-layer" id="toc-causal-self-attention-layer" class="nav-link" data-scroll-target="#causal-self-attention-layer"><span class="header-section-number">2.3.3</span> Causal Self-Attention Layer</a></li>
  <li><a href="#cross-attention-layer" id="toc-cross-attention-layer" class="nav-link" data-scroll-target="#cross-attention-layer"><span class="header-section-number">2.3.4</span> Cross Attention Layer</a></li>
  <li><a href="#sec-attention-complexity" id="toc-sec-attention-complexity" class="nav-link" data-scroll-target="#sec-attention-complexity"><span class="header-section-number">2.3.5</span> Time Complexity of Attention</a></li>
  </ul></li>
  <li><a href="#sec-normalization" id="toc-sec-normalization" class="nav-link" data-scroll-target="#sec-normalization"><span class="header-section-number">2.4</span> Normalization Layer</a></li>
  <li><a href="#sec-feed-forward" id="toc-sec-feed-forward" class="nav-link" data-scroll-target="#sec-feed-forward"><span class="header-section-number">2.5</span> Feed Forward Layer</a></li>
  <li><a href="#residual-connection" id="toc-residual-connection" class="nav-link" data-scroll-target="#residual-connection"><span class="header-section-number">2.6</span> Residual Connection</a>
  <ul>
  <li><a href="#backpropagation-through-residual-connection" id="toc-backpropagation-through-residual-connection" class="nav-link" data-scroll-target="#backpropagation-through-residual-connection"><span class="header-section-number">2.6.1</span> Backpropagation through Residual Connection</a></li>
  </ul></li>
  <li><a href="#sec-output-layer" id="toc-sec-output-layer" class="nav-link" data-scroll-target="#sec-output-layer"><span class="header-section-number">2.7</span> Output Layer</a></li>
  <li><a href="#encoder-decoder-layer" id="toc-encoder-decoder-layer" class="nav-link" data-scroll-target="#encoder-decoder-layer"><span class="header-section-number">2.8</span> Encoder &amp; Decoder Layer</a></li>
  <li><a href="#others" id="toc-others" class="nav-link" data-scroll-target="#others"><span class="header-section-number">2.9</span> Others</a>
  <ul>
  <li><a href="#dropout-layer" id="toc-dropout-layer" class="nav-link" data-scroll-target="#dropout-layer"><span class="header-section-number">2.9.1</span> Dropout Layer</a></li>
  <li><a href="#label-smoothing" id="toc-label-smoothing" class="nav-link" data-scroll-target="#label-smoothing"><span class="header-section-number">2.9.2</span> Label Smoothing</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#experiment" id="toc-experiment" class="nav-link" data-scroll-target="#experiment"><span class="header-section-number">3</span> Experiment</a>
  <ul>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset"><span class="header-section-number">3.1</span> Dataset</a>
  <ul>
  <li><a href="#tokenizer-vocabulary" id="toc-tokenizer-vocabulary" class="nav-link" data-scroll-target="#tokenizer-vocabulary"><span class="header-section-number">3.1.1</span> Tokenizer &amp; Vocabulary</a></li>
  <li><a href="#padding-samples" id="toc-padding-samples" class="nav-link" data-scroll-target="#padding-samples"><span class="header-section-number">3.1.2</span> Padding Samples</a></li>
  </ul></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization"><span class="header-section-number">3.2</span> Weight Initialization</a></li>
  <li><a href="#optimizer" id="toc-optimizer" class="nav-link" data-scroll-target="#optimizer"><span class="header-section-number">3.3</span> Optimizer</a>
  <ul>
  <li><a href="#learning-rate-scheduler" id="toc-learning-rate-scheduler" class="nav-link" data-scroll-target="#learning-rate-scheduler"><span class="header-section-number">3.3.1</span> Learning Rate Scheduler</a></li>
  </ul></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">3.4</span> Loss Function</a></li>
  <li><a href="#evaluation-metric" id="toc-evaluation-metric" class="nav-link" data-scroll-target="#evaluation-metric"><span class="header-section-number">3.5</span> Evaluation Metric</a>
  <ul>
  <li><a href="#blue" id="toc-blue" class="nav-link" data-scroll-target="#blue"><span class="header-section-number">3.5.1</span> BLUE</a></li>
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity"><span class="header-section-number">3.5.2</span> Perplexity</a></li>
  </ul></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">3.6</span> Training</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3.7</span> Results</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4</span> Summary</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">5</span> Key Concepts</a></li>
  <li><a href="#q-a" id="toc-q-a" class="nav-link" data-scroll-target="#q-a"><span class="header-section-number">6</span> Q &amp; A</a></li>
  <li><a href="#related-resource-further-reading" id="toc-related-resource-further-reading" class="nav-link" data-scroll-target="#related-resource-further-reading"><span class="header-section-number">7</span> Related resource &amp; Further Reading</a></li>
  <li><a href="#in-the-end" id="toc-in-the-end" class="nav-link" data-scroll-target="#in-the-end"><span class="header-section-number">8</span> In the end</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">01: Attention is All You Need (<tag style="color:orange">Transformer</tag>)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">Architecture</div>
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸</div>
  </div>
  </div>

<div>
  <div class="description">
    Transformer æ˜¯ä¸€ç§åŸºäº <strong>è‡ªæ³¨æ„åŠ›æœºåˆ¶</strong> çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œåœ¨è¯­è¨€ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›å¹¶ä¸”ä½œä¸º <strong>GPT</strong>ã€<strong>BERT</strong> ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŸºç¡€ï¼Œæ¨åŠ¨äº†å½“ä»Šç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ã€‚åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ Transformer çš„åŸºæœ¬åŸç†ï¼Œä»¥åŠå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬ <strong>Word Embedding</strong>ã€<strong>Position Embedding</strong>ã€<strong>Attention</strong>ã€<strong>Normalization Layer</strong> å’Œ <strong>Feed Forward Layer</strong>ã€‚å¹¶é€šè¿‡åœ¨ <a href="https://huggingface.co/datasets/IWSLT/ted_talks_iwslt">Ted Talks</a> æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œå±•ç¤º Transformer åœ¨å®é™…ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœã€‚
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>YY Zhang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>æˆ‘ä»¬å¼€å§‹ç¬¬ä¸€ç¯‡è®ºæ–‡çš„å­¦ä¹ ï¼š ã€ŠAttention is All You Needã€‹ <span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>ï¼Œä¹Ÿå°±æ˜¯ä¼ è¯´ä¸­çš„Transformeræ¨¡å‹ã€‚Transformeræ¨¡å‹çš„æå‡ºï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»¥åŠæ›´å¹¿æ³›çš„é¢†åŸŸã€‚è¯¥æ¶æ„å®Œå…¨åŸºäº<strong>æ³¨æ„åŠ›æœºåˆ¶(Attention)</strong>ï¼Œä¸å†ä¾èµ–å¾ªç¯ï¼ˆRNNï¼‰æˆ–å·ç§¯ï¼ˆCNNï¼‰ï¼Œå› æ­¤åœ¨è®­ç»ƒæ—¶<em>æ›´æ˜“å¹¶è¡ŒåŒ–ã€æ•ˆç‡æ›´é«˜</em>ã€‚Transformer å·²æˆä¸ºä¼—å¤šå‰æ²¿æ¨¡å‹çš„åŸºç¡€ï¼Œä¸ä»…åœ¨ NLP ä¸­è¡¨ç°çªå‡ºï¼Œä¹Ÿæ‰©å±•åˆ°è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚æ¯”å¦‚ ChatGPTã€DeepSeek ç­‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ½ä»¥ Transformer ä¸ºæ ¸å¿ƒæ¶æ„ã€‚æ‰€ä»¥æˆ‘ä»¬è‡ªç„¶å°±æŠŠå®ƒå½“ä½œæˆ‘ä»¬ç¬¬ä¸€ç¯‡æ–‡ç« çš„é¦–é€‰ã€‚</p>
<section id="preliminary" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preliminary</h1>
<p>åœ¨å¼€å§‹å­¦ä¹ Transformerä¹‹å‰ï¼Œæˆ‘ä»¬é¢„ä¹ ä¸€ä¸‹ä¸€äº›éœ€è¦çš„çŸ¥è¯†ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ›´å¥½çš„ç†è§£è¿™ä¸ªæ¨¡å‹ã€‚</p>
<section id="softmax-function" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="softmax-function"><span class="header-section-number">1.1</span> Softmax Function</h2>
<p>Softmax Function æ˜¯ä¸€ä¸ª <span class="hilite-teal">å°†å®æ•°å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ</span> çš„å‡½æ•°ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š</p>
<p><span id="eq-softmax"><span class="math display">\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\tag{1}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(z_i\)</span> æ˜¯è¾“å…¥å‘é‡çš„ç¬¬ <span class="math inline">\(i\)</span> ä¸ªå…ƒç´ ï¼Œ<span class="math inline">\(e\)</span> æ˜¯è‡ªç„¶å¯¹æ•°çš„åº•æ•°ã€‚Softmax å‡½æ•°çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€æœ‰è¾“å‡ºå€¼çš„å’Œä¸º 1ã€‚</p>
<div class="sourceCode" id="cb1" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> softmax(z):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    exp_z <span class="op">=</span> torch.exp(z)</span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="cf">return</span> exp_z <span class="op">/</span> torch.<span class="bu">sum</span>(exp_z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="vector-similarity" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="vector-similarity"><span class="header-section-number">1.2</span> Vector Similarity</h2>
<p>åœ¨Transformerä¸­ï¼Œè®¡ç®—å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„æ­¥éª¤ï¼Œå¸¸ç”¨çš„æ–¹æ³•æœ‰ç‚¹ç§¯ï¼ˆDot Productï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰ã€‚åœ¨Transformerä¸­ï¼Œä¸»è¦ä½¿ç”¨Dot Productæ¥è¡¡é‡å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥ç®€å•å›é¡¾ä¸€ä¸‹Dot Productçš„è®¡ç®—æ–¹æ³•ï¼š</p>
<p><span id="eq-dot-product"><span class="math display">\[
\text{Dot Product}(A, B) = \sum_{i=1}^{n} A_i \cdot B_i
\tag{2}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(A\)</span> å’Œ <span class="math inline">\(B\)</span> æ˜¯ä¸¤ä¸ªå‘é‡ï¼Œ<span class="math inline">\(n\)</span> æ˜¯å‘é‡çš„ç»´åº¦ï¼Œ<span class="math inline">\(A_i\)</span> å’Œ <span class="math inline">\(B_i\)</span> åˆ†åˆ«æ˜¯å‘é‡ <span class="math inline">\(A\)</span> å’Œ <span class="math inline">\(B\)</span> åœ¨ç¬¬ <span class="math inline">\(i\)</span> ä¸ªç»´åº¦çš„åˆ†é‡ï¼š</p>
<ul>
<li>Dot Product çš„å€¼è¶Š<strong>å¤§</strong>ï¼Œè¡¨ç¤ºä¸¤ä¸ªå‘é‡è¶Š<strong>ç›¸ä¼¼</strong>ã€‚</li>
<li>Dot Product çš„å€¼è¶Š<strong>å°</strong>ï¼Œè¡¨ç¤ºä¸¤ä¸ªå‘é‡è¶Š<strong>ä¸ç›¸ä¼¼</strong>ã€‚</li>
</ul>
<p>Dot Productä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ <strong>Unnormalized Cosine Similarity</strong>ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¯¹å‘é‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚</p>
<p>æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—å¤šä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼š</p>
<p><span id="eq-dot-product-matrix"><span class="math display">\[
\text{Dot Product Matrix}(A, B) = A B^\top
\tag{3}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(A\)</span> æ˜¯ä¸€ä¸ª <span class="math inline">\(m \times n\)</span> çš„çŸ©é˜µï¼Œ<span class="math inline">\(B\)</span> æ˜¯ä¸€ä¸ª <span class="math inline">\(p \times n\)</span> çš„çŸ©é˜µï¼Œ<span class="math inline">\(B^\top\)</span> æ˜¯ <span class="math inline">\(B\)</span> çš„è½¬ç½®çŸ©é˜µï¼Œç»“æœæ˜¯ä¸€ä¸ª <span class="math inline">\(m \times p\)</span> çš„çŸ©é˜µï¼Œè¡¨ç¤º <span class="math inline">\(A\)</span> ä¸­çš„æ¯ä¸ªå‘é‡ä¸ <span class="math inline">\(B\)</span> ä¸­çš„æ¯ä¸ªå‘é‡ä¹‹é—´çš„ç‚¹ç§¯ã€‚</p>
<div class="sourceCode" id="cb2" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> dot_product_matrix(A, B):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="cf">return</span> torch.matmul(A, B.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="transformer" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Transformer</h1>
<p>ç®€å•å›é¡¾äº†ä¸€ä¸‹è¿™äº›æ•°å­¦çŸ¥è¯†ï¼Œæ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹Transformeråˆ°åº•æ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ã€‚</p>
<div id="fig-transformer-model-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-model-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/transformer-model-overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-model-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Transformeræ¨¡å‹ä¸»è¦ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ã€‚ç¼–ç å™¨è´Ÿè´£å¤„ç†è¾“å…¥åºåˆ—ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸€ç»„è¿ç»­çš„è¡¨ç¤ºï¼ˆé€šå¸¸ç§°ä¸ºâ€œContextâ€ï¼‰ã€‚è§£ç å™¨åˆ™æ ¹æ®è¿™äº›è¡¨ç¤ºç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ç”±å¤šä¸ªç›¸åŒçš„å±‚ï¼ˆLayerï¼‰å †å è€Œæˆ. å¹¶ä¸”åœ¨æœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆLinear Layerï¼‰å’Œ Softmax å±‚å°†è§£ç å™¨çš„è¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹çš„è¯æ±‡åˆ†å¸ƒã€‚
</figcaption>
</figure>
</div>
<p>Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span> æ˜¯ Google åœ¨2017å¹´æå‡ºçš„æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ƒçš„æå‡ºä¸»è¦æ˜¯ä¸ºäº†è§£å†³ï¼Œ</p>
<ul>
<li><span class="hilite-pink">Sequence Modelingçš„æ•ˆç‡é—®é¢˜</span>:
<ul>
<li>åœ¨ Transformer å‡ºç°ä¹‹å‰ï¼Œä¸»æµæ–¹æ³•æ˜¯ <strong>RNN</strong>å’Œ <strong>CNN</strong>ã€‚
<ul>
<li>RNN éœ€è¦<u>æŒ‰é¡ºåºé€æ­¥å¤„ç†åºåˆ—ï¼Œæ— æ³•å¹¶è¡ŒåŒ–</u>ï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>CNN è™½ç„¶æœ‰ä¸€å®šçš„å¹¶è¡Œæ€§ï¼Œä½†<u>æ•æ‰é•¿è·ç¦»ä¾èµ–éœ€è¦å †å å¾ˆå¤šå±‚ï¼Œè®¡ç®—å¼€é”€å¤§</u>ã€‚</li>
</ul></li>
</ul></li>
<li><span class="hilite-pink">Long Distance Dependency Modeling</span>:
<ul>
<li>RNN åœ¨æ•æ‰é•¿è·ç¦»ä¾èµ–æ—¶å®¹æ˜“å‡ºç°<strong>Gradient Vanish</strong> æˆ–<strong>Gradient Explosion</strong>ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ è¿œè·ç¦»çš„ä¿¡æ¯ã€‚</li>
</ul></li>
</ul>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Gradient Vanish &amp; Gradient Explosion</p>
</div>
<div class="note-container foldable-content">
<ul>
<li><strong>Gradient Vanish</strong> é—®é¢˜æ˜¯æŒ‡åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œéšç€æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€å±‚ä¼ é€’ï¼Œ<u>æ¢¯åº¦å€¼é€æ¸å˜å°ï¼Œæœ€ç»ˆè¶‹è¿‘äºé›¶</u>çš„ç°è±¡ã€‚è¿™ä¼šå¯¼è‡´å‰é¢çš„å±‚å‡ ä¹æ²¡æœ‰æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚</li>
<li><strong>Gradient Explosion</strong> åˆ™æ˜¯æŒ‡åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œéšç€æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€å±‚ä¼ é€’ï¼Œ<u>æ¢¯åº¦å€¼é€æ¸å˜å¤§ï¼Œæœ€ç»ˆå˜å¾—éå¸¸å¤§</u>ï¼Œå¯¼è‡´æ¨¡å‹å‚æ•°æ›´æ–°è¿‡å¤§ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šï¼Œç”šè‡³å‘æ•£ã€‚</li>
</ul>
</div>
</div>
<blockquote class="blockquote">
<p>This <u>inherently sequential nature precludes parallelization</u> within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. â€¦ In these models, <u>the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions</u>, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions <cite> Attention is all you need, p.2 </cite></p>
</blockquote>
<p>Transformerçš„åŸºæœ¬æ¡†æ¶å¦‚ <a href="#fig-transformer-model-overview" class="quarto-xref">Figure&nbsp;1</a> æ‰€ç¤ºã€‚æ•´ä½“ç»“æ„æ¯”è¾ƒç®€å•æ¸…æ™°ï¼Œä¸»è¦åŒ…æ‹¬Encoderå’ŒDecoderä¸¤å¤§éƒ¨åˆ†:</p>
<div class="sourceCode" id="cb3" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(config)</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(config)</span>
<span id="cb3-7"><a href="#cb3-7"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.d_model, config.tgt_vocab_size)</span>
<span id="cb3-8"><a href="#cb3-8"></a>        ...</span>
<span id="cb3-9"><a href="#cb3-9"></a>    </span>
<span id="cb3-10"><a href="#cb3-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, original, target):</span>
<span id="cb3-11"><a href="#cb3-11"></a>        enc_output <span class="op">=</span> <span class="va">self</span>.encoder(original)</span>
<span id="cb3-12"><a href="#cb3-12"></a>        dec_output <span class="op">=</span> <span class="va">self</span>.decoder(target, enc_output)</span>
<span id="cb3-13"><a href="#cb3-13"></a>        output <span class="op">=</span> <span class="va">self</span>.output_layer(dec_output)</span>
<span id="cb3-14"><a href="#cb3-14"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>æ¥ä¸‹æ¥è®©æˆ‘ä»¬ä»ä¸‹è‡³ä¸Šï¼Œæ¥æ·±åº¦è§£åˆ¨Transformerçš„æ¨¡å‹ç»“æ„ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®ç»„ä»¶:</p>
<ol type="1">
<li>Word Embedding Layer (<a href="#sec-word-embedding" class="quarto-xref">Section&nbsp;2.1</a>): å°†è¯æ±‡è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º</li>
<li>Position Embedding Layer (<a href="#sec-postion-embedding" class="quarto-xref">Section&nbsp;2.2</a>): æ³¨å…¥ä½ç½®ä¿¡æ¯</li>
<li>Attention Layer (<a href="#sec-attention" class="quarto-xref">Section&nbsp;2.3</a>): Transformerçš„æ ¸å¿ƒç»„ä»¶
<ol type="1">
<li>Self-Attention Layer: å¤„ç†è¾“å…¥åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»</li>
<li>Causal Self-Attention Layer: å¤„ç†è§£ç å™¨ä¸­çš„Auto-Regressiveä¾èµ–å…³ç³»</li>
<li>Cross-Attention Layer: å¤„ç†è¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—ä¹‹é—´çš„ä¾èµ–å…³ç³»</li>
</ol></li>
<li>Normalization Layer (<a href="#sec-normalization" class="quarto-xref">Section&nbsp;2.4</a>): æ ‡å‡†åŒ–è¾“å…¥ï¼Œç¨³å®šè®­ç»ƒ</li>
<li>Feed Forward Layer (<a href="#sec-feed-forward" class="quarto-xref">Section&nbsp;2.5</a>): éçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›</li>
<li>Output Layer(<a href="#sec-output-layer" class="quarto-xref">Section&nbsp;2.7</a>): ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æœ</li>
</ol>
<section id="sec-word-embedding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-word-embedding"><span class="header-section-number">2.1</span> Word Embedding Layer</h2>
<p>Word Embedding åŸºæœ¬æ˜¯æ‰€æœ‰è¯­è¨€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œå®ƒçš„ä½œç”¨æ˜¯ <span class="hilite-teal">å°†ç¦»æ•£çš„è¯æ±‡è½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤º</span>ã€‚è¿™æ ·ï¼Œæ¨¡å‹å°±å¯ä»¥åœ¨ä¸€ä¸ª<strong>é«˜ç»´ç©ºé—´</strong>ä¸­å¤„ç†è¯æ±‡ä¹‹é—´çš„å…³ç³»å’Œç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼ˆEmbedding Matrixï¼‰æ¥å®ç°è¿™ä¸€ç‚¹:</p>
<p><span id="eq-word-embedding"><span class="math display">\[
\text{Embedding}(w) = W_{e}[w]
\tag{4}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(W_{e} \in \mathbb{R}^{V \times d}\)</span> æ˜¯åµŒå…¥çŸ©é˜µ, <span class="math inline">\(w \in {0, 1, \dots, V-1}\)</span> æ˜¯è¯æ±‡åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•ï¼Œ<span class="math inline">\(d\)</span> æ˜¯åµŒå…¥ç»´åº¦ï¼Œ<span class="math inline">\(V\)</span> æ˜¯è¯æ±‡è¡¨å¤§å°ã€‚ è¯¥æ“ä½œç­‰ä»·äºå°†è¯æ±‡ <span class="math inline">\(w\)</span> çš„ One-Hot Encoding ä¸åµŒå…¥çŸ©é˜µç›¸ä¹˜ï¼Œå³ï¼š</p>
<p><span id="eq-word-embedding-one-hot"><span class="math display">\[
\text{Embedding}(w) = W_{e}^{\top} \cdot \text{one hot}(w), \quad \text{one hot}(w) \in \mathbb{R}^{V}
\tag{5}\]</span></span></p>
<p>ä»å®ç°è§’åº¦çœ‹ï¼Œè¿™ä¸€è¿‡ç¨‹å¯ä»¥ç›´æ¥ç†è§£ä¸ºï¼šé€šè¿‡è¯æ±‡ç´¢å¼• <span class="math inline">\(w\)</span>ï¼Œä»åµŒå…¥çŸ©é˜µ <span class="math inline">\(W_e\)</span> ä¸­å–å‡ºç¬¬ <span class="math inline">\(w\)</span> è¡Œä½œä¸ºè¯¥è¯çš„å‘é‡è¡¨ç¤ºã€‚</p>
<p>æ›´ç›´è§‚çš„æ–¹å¼å°±æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒçœ‹ä½œä¸€ä¸ªæŸ¥æ‰¾è¡¨ï¼ˆLookup Tableï¼‰ï¼Œ<u>é€šè¿‡è¯æ±‡çš„ç´¢å¼•ç›´æ¥è·å–å¯¹åº”çš„åµŒå…¥å‘é‡</u>ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»£ç å®ç°ï¼š</p>
<div class="sourceCode" id="cb4" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> navie_embedding(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, v, d):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Parameter(torch.randn(v, d)) <span class="co"># åˆå§‹åŒ–Embedding Table</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>    </span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="co"># x: (batch_size, seq_len)</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>        </span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="co"># ç¬¬ä¸€ç§æ–¹æ³•: </span></span>
<span id="cb4-10"><a href="#cb4-10"></a>        <span class="co"># return self.embedding[x]  # ç›´æ¥ç´¢å¼•è·å–åµŒå…¥å‘é‡</span></span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>        <span class="co"># ç¬¬äºŒç§æ–¹æ³•: One Hot Encoding</span></span>
<span id="cb4-13"><a href="#cb4-13"></a>        <span class="co"># x_one_hot = F.one_hot(x, num_classes=self.embedding.size(0)).float() # (batch_size, seq_len, v)</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>        <span class="co"># return torch.matmul(x_one_hot, self.embedding) # (batch_size, seq_len, d)</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>        <span class="co"># ç¬¬ä¸‰ç§æ–¹æ³•ï¼Œåˆ©ç”¨Gatherå‡½æ•°</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>        <span class="co"># batch_size, seq_len = x.size()</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>        <span class="co"># x = x.unsqueeze(-1).expand(-1, -1, self.embedding.size(1)) # (batch_size, seq_len, d)</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>        <span class="co"># return torch.gather(self.embedding.unsqueeze(0).expand(batch_size, -1, -1), 1, x) # (batch_size, seq_len, d)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç®€å•çš„åµŒå…¥å±‚ <code>navie_embedding</code>ï¼Œå®ƒæ¥å—è¯æ±‡è¡¨å¤§å° <code>v</code> å’ŒåµŒå…¥ç»´åº¦ <code>d</code> ä½œä¸ºå‚æ•°ã€‚æˆ‘ä»¬åˆå§‹åŒ–äº†ä¸€ä¸ªåµŒå…¥çŸ©é˜µ <code>self.embedding</code>ï¼Œå¹¶åœ¨å‰å‘ä¼ æ’­ä¸­é€šè¿‡<strong>ç´¢å¼•</strong>ã€<strong>One-Hot ç¼–ç </strong>æˆ– <strong><code>gather</code> å‡½æ•°</strong>æ¥è·å–å¯¹åº”çš„åµŒå…¥å‘é‡ã€‚</p>
<p>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨ PyTorch æä¾›çš„ <code>nn.Embedding</code> ç±»æ¥ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼š</p>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> WordEmbedding(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, d_model):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb5-5"><a href="#cb5-5"></a>    </span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>åœ¨ Transformer ä¸­ï¼Œè¯åµŒå…¥å±‚ä¸ä»…ç”¨äºå°†è¾“å…¥è¯æ±‡è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œè¿˜ç”¨äºå°†è§£ç å™¨çš„è¾“å‡ºè¯æ±‡è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚ä¸ºäº†ä¿æŒè¾“å…¥å’Œè¾“å‡ºçš„ä¸€è‡´æ€§ï¼ŒTransformer é‡‡ç”¨äº†<strong>Weight Tying</strong>çš„ç­–ç•¥: å³Output Layerçš„æƒé‡çŸ©é˜µä¸Embedding Layerçš„æƒé‡çŸ©é˜µå…±äº«:</p>
<p><span id="eq-weight-tying"><span class="math display">\[
\text{Output Layer Weight} = \text{Embedding Layer Weight}^\top
\tag{6}\]</span></span></p>
<p>å¹¶ä¸”åœ¨åˆå§‹åŒ–æ—¶ï¼Œå¯¹åµŒå…¥å‘é‡è¿›è¡Œäº†ç¼©æ”¾å¤„ç†ï¼Œå³ä¹˜ä»¥ <span class="math inline">\(\sqrt{d_{model}}\)</span>ï¼Œä»¥ç¡®ä¿åµŒå…¥å‘é‡çš„å°ºåº¦é€‚åˆåç»­çš„æ³¨æ„åŠ›è®¡ç®—</p>
<blockquote class="blockquote">
<p>In our model, <u>we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation</u>. In the embedding layers, we multiply those weights by <span class="math inline">\(\sqrt{d_{model}}\)</span>. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
</section>
<section id="sec-postion-embedding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-postion-embedding"><span class="header-section-number">2.2</span> Position Embedding Layer</h2>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add â€œpositional encodingsâ€ to the input embeddings at the bottoms of the encoder and decoder stacks. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>Transformeræ¨¡å‹ä¸­æ²¡æœ‰ä½¿ç”¨RNNæˆ–CNNï¼Œå› æ­¤ç¼ºä¹å¯¹åºåˆ—ä¸­è¯æ±‡é¡ºåºçš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒTransformeræ˜¯ <u>Permutation Invariant</u> çš„æ¨¡å‹ï¼Œå®ƒæ— æ³•åŒºåˆ†è¾“å…¥åºåˆ—ä¸­è¯æ±‡çš„é¡ºåºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTransformerå¼•å…¥äº†ä½ç½®ç¼–ç ï¼ˆPosition Embeddingï¼‰æ¥æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥è¯æ±‡åœ¨åºåˆ—ä¸­çš„ä½ç½®ã€‚ å…¶ä¸­ï¼Œä½ç½®ç¼–ç æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•: <strong>ç»å¯¹ä½ç½®ç¼–ç </strong>å’Œ<strong>ç›¸å¯¹ä½ç½®ç¼–ç </strong>ã€‚åœ¨åŸå§‹çš„Transformerè®ºæ–‡ä¸­ï¼Œä½¿ç”¨çš„æ˜¯<strong>ç»å¯¹ä½ç½®ç¼–ç </strong>:</p>
<p><span id="eq-position-embedding"><span class="math display">\[
\begin{split}
PE_{(pos, 2i)} &amp; = \sin (pos / 10,000^{2i / d_{model}}) \\
PE_{(pos, 2i+1)} &amp; = \cos (pos / 10,000^{2i+1 / d_{model}})
\end{split}
\tag{7}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(pos\)</span> æ˜¯è¯æ±‡åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œ<span class="math inline">\(i \in [0, d_{model} / 2 )\)</span> æ˜¯åµŒå…¥ç»´åº¦çš„ç´¢å¼•ï¼Œ<span class="math inline">\(d_{model}\)</span> æ˜¯åµŒå…¥ç»´åº¦çš„å¤§å°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å‘é‡è¡¨ç¤ºã€‚</p>
<p>ä»”ç»†è§‚å¯Ÿä¸Šé¢çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°:</p>
<ul>
<li>ä½ç½®ç¼–ç çš„ç»´åº¦ä¸è¯æ±‡åµŒå…¥çš„ <span class="hilite-purple">ç»´åº¦ç›¸åŒ</span>ï¼Œè¿™æ ·å¯ä»¥ <span class="hilite-purple">æ–¹ä¾¿åœ°å°†ä¸¤è€…ç›¸åŠ </span>ã€‚</li>
<li>ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°å¯ä»¥ç¡®ä¿ä¸åŒä½ç½®çš„ç¼–ç å…·æœ‰ä¸åŒçš„é¢‘ç‡ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„ä½ç½®ä¿¡æ¯ã€‚</li>
<li>è¿™ç§æ–¹æ³•è¿˜å…·æœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼Œå³å®ƒå¯ä»¥æ¨å¹¿åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ï¼Œå› ä¸ºä½ç½®ç¼–ç æ˜¯åŸºäºä½ç½®è®¡ç®—çš„ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå…·ä½“çš„è¯æ±‡ã€‚</li>
</ul>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question: ä¸ºä»€ä¹ˆæ˜¯ä¸Word Vectorç›¸åŠ ï¼Œè€Œä¸æ˜¯ç›¸ä¹˜æˆ–è€… concat å‘¢ï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>å¦‚æœç”¨ç›¸ä¹˜ <span class="math inline">\(\odot\)</span>, é‚£ä¹ˆä½ç½®ç¼–ç ä¸­ä¸º0çš„ç»´åº¦ä¼šç›´æ¥å°†è¯å‘é‡çš„å¯¹åº”ç»´åº¦ç½®ä¸º0ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚</p>
<p>å¦‚æœç”¨concat, é‚£ä¹ˆè¯å‘é‡å’Œä½ç½®ç¼–ç çš„ç»´åº¦ä¼šå¢åŠ ä¸€å€ï¼Œå¯¼è‡´åç»­çš„Attentionè®¡ç®—å¤æ‚åº¦å¢åŠ ï¼ŒåŒæ—¶ä¹Ÿä¼šæ”¹å˜æ¨¡å‹çš„å‚æ•°è§„æ¨¡ï¼Œå½±å“è®­ç»ƒæ•ˆæœã€‚</p>
<p>ç”¨ç›¸åŠ çš„æ–¹å¼ï¼Œå¯ä»¥ä¿æŒè¯å‘é‡çš„ç»´åº¦ä¸å˜ï¼ŒåŒæ—¶å°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°è¯å‘é‡ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶åˆ©ç”¨è¯æ±‡ä¿¡æ¯å’Œä½ç½®ä¿¡æ¯è¿›è¡Œå­¦ä¹ ã€‚</p>
</div>
</div>
<p>æˆ‘ä»¬æ¥ä»”ç»†çœ‹ä¸€ä¸‹<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>ï¼Œå‡è®¾æˆ‘ä»¬å›ºå®šä½ç½® <span class="math inline">\(pos=1\)</span>ï¼Œå¹¶ä¸”åµŒå…¥ç»´åº¦ <span class="math inline">\(d_{model}=6\)</span>ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºå¯¹åº”çš„ä½ç½®ä¿¡æ¯ï¼š</p>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>pos <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>d_model <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>pe <span class="op">=</span> torch.zeros(d_model)</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d_model <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb6-5"><a href="#cb6-5"></a>    pe[<span class="dv">2</span> <span class="op">*</span> i] <span class="op">=</span> torch.sin(pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (<span class="dv">2</span> <span class="op">*</span> i <span class="op">/</span> d_model)))</span>
<span id="cb6-6"><a href="#cb6-6"></a>    pe[<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> torch.cos(pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> d_model)))</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="bu">print</span>(pe)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>è®¡ç®—çš„æ–¹å¼å¾ˆç®€å•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹åœ¨Transformerä¸­ï¼Œæˆ‘ä»¬å¦‚ä½•å®ç°å®ƒã€‚åœ¨å®é™…çš„å®ç°å½“ä¸­ï¼Œä¼šåˆ©ç”¨ä¸€äº›æ•°å­¦çš„æŠ€å·§æ¥é˜²æ­¢Overflow:</p>
<p><span id="eq-prevent-overflow"><span class="math display">\[
\frac{1}{(10000^{2i/d_{model}})} = e^{\ln(10000^{- 2i/d_{model}})} = e^{-(2i/d_{model}) \cdot \ln(10000)}
\tag{8}\]</span></span></p>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> PositionalEncoding(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, max_len<span class="op">=</span><span class="dv">5000</span>):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len).unsqueeze(<span class="dv">1</span>) <span class="co"># (max_len, 1)</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>        i <span class="op">=</span> torch.arange(<span class="dv">0</span>, d_model, <span class="dv">2</span>)  <span class="co"># (d_model/2,)</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>        div_term <span class="op">=</span> torch.exp(i <span class="op">*</span> (<span class="op">-</span>math.log(<span class="fl">10000.0</span>) <span class="op">/</span> d_model)) <span class="co"># (d_model/2,)</span></span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>        pe <span class="op">=</span> torch.zeros(max_len, d_model)</span>
<span id="cb7-10"><a href="#cb7-10"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb7-11"><a href="#cb7-11"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb7-12"><a href="#cb7-12"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># (1, max_len, d_model)</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="va">self</span>.register_buffer(<span class="st">'pe'</span>, pe)</span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-16"><a href="#cb7-16"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>), :]</span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-pos-embedding" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pos-embedding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pos-embedding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pos-embedding-low-dimension" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pos-embedding-low-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/pos-embedding-low-dimension-viz.png" class="img-fluid figure-img" data-ref-parent="fig-pos-embedding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pos-embedding-low-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Display Position Embedding in Low Dimension, with sequence length 100
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pos-embedding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pos-embedding-high-dimension" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pos-embedding-high-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/low-dim-viz-18-19.png" class="img-fluid figure-img" data-ref-parent="fig-pos-embedding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pos-embedding-high-dimension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pos-embedding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Display Position Embedding in High Dimension, with sequence length 100
</figcaption>
</figure>
</div>
<p>ä» <a href="#fig-pos-embedding" class="quarto-xref">Figure&nbsp;2</a> ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼šSinusoidal PE æ˜¯ä¸€ä¸ªã€Œå¤šå°ºåº¦è¡¨ç¤ºã€ï¼Œ<span class="hilite-blue">ä¸åŒçš„ç»´åº¦å¯¹åº”ä¸åŒçš„é¢‘ç‡ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„ä½ç½®ä¿¡æ¯</span>ï¼š</p>
<ul>
<li>ä½ç»´ â†’ é«˜é¢‘ â†’ å±€éƒ¨ã€ç²¾ç»†ä½ç½®ä¿¡æ¯</li>
<li>é«˜ç»´ â†’ ä½é¢‘ â†’ å…¨å±€ã€é•¿è·ç¦»ä½ç½®ä¿¡æ¯</li>
</ul>
<section id="why-sinusoidal-position-embedding" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="why-sinusoidal-position-embedding"><span class="header-section-number">2.2.1</span> Why Sinusoidal Position Embedding?</h3>
<blockquote class="blockquote">
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. We also experimented with using learned positional embeddings. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>è®ºæ–‡ä¸­æåˆ°çš„ç¬¬ä¸€ä¸ªå¥½å¤„å°±æ˜¯ï¼š<span class="hilite-blue">ç›¸å¯¹ä½ç½®ç¼–ç </span>ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªä½ç½® <span class="math inline">\(pos\)</span> å’Œ <span class="math inline">\(pos + k\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(k\)</span> æ˜¯ä¸€ä¸ªå›ºå®šçš„åç§»é‡ã€‚é‚£ä¹ˆæ ¹æ®<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºä¸º: <span id="eq-pos-embedding-relative-1"><span class="math display">\[
\begin{split}
PE_{(pos \textcolor{orange}{+ k}, 2i:2i+1)}
&amp;=
\begin{bmatrix}
\sin\big((pos+k)\,\omega_i\big)\\
\cos\big((pos+k)\,\omega_i\big)
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
\sin(pos\,\omega_i)\cos(k\,\omega_i)+\cos(pos\,\omega_i)\sin(k\,\omega_i)\\
\cos(pos\,\omega_i)\cos(k\,\omega_i)-\sin(pos\,\omega_i)\sin(k\,\omega_i)
\end{bmatrix} \\
&amp;=
\begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix}
\textcolor{orange}{
    \begin{bmatrix}
    \sin(pos\,\omega_i)\\
    \cos(pos\,\omega_i)
    \end{bmatrix}
} \\
&amp;= \begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix} \textcolor{orange}{PE_{(pos, 2i:2i+1)}}
\end{split}
\tag{9}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ <span class="math inline">\(\omega_i = 1 / 10,000^{2i/d_{model}}\)</span>ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä½ç½® <span class="math inline">\(pos + k\)</span> çš„ç¼–ç å¯ä»¥è¡¨ç¤ºä¸ºä½ç½® <span class="math inline">\(pos\)</span> çš„ç¼–ç é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°çš„ç»“æœã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥é€šè¿‡å­¦ä¹ è¿™ä¸ª<strong>çº¿æ€§å˜æ¢</strong>æ¥æ•æ‰ç›¸å¯¹ä½ç½®å…³ç³»ã€‚ <span class="math inline">\(\begin{bmatrix}
\cos(k\,\omega_i) &amp; \sin(k\,\omega_i)\\
-\sin(k\,\omega_i) &amp; \cos(k\,\omega_i)
\end{bmatrix}\)</span> æ˜¯ä¸€ä¸ªæ—‹è½¬çŸ©é˜µï¼Œè¡¨ç¤ºåœ¨äºŒç»´ç©ºé—´ä¸­çš„æ—‹è½¬å˜æ¢.</p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Rotate Matrix</p>
</div>
<div class="note-container foldable-content">
<p>Rotate Matrix æ˜¯ä¸€ç§äºŒç»´ç©ºé—´ä¸­çš„çº¿æ€§å˜æ¢ï¼Œç”¨äºè¡¨ç¤ºç‚¹ç»•åŸç‚¹æ—‹è½¬ä¸€å®šè§’åº¦çš„æ“ä½œã€‚å¯¹äºä¸€ä¸ªè§’åº¦ <span class="math inline">\(\theta\)</span>ï¼Œå…¶æ—‹è½¬çŸ©é˜µå®šä¹‰å¦‚ä¸‹: <span class="math display">\[
R(\theta) = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta)\\
\sin(\theta) &amp; \cos(\theta)
\end{bmatrix}
\]</span></p>
<p>å½“æˆ‘ä»¬å°†ä¸€ä¸ªäºŒç»´å‘é‡ <span class="math inline">\(v = \begin{bmatrix} x \\ y \end{bmatrix}\)</span> ä¹˜ä»¥æ—‹è½¬çŸ©é˜µ <span class="math inline">\(R(\theta)\)</span> æ—¶ï¼Œå¾—åˆ°çš„æ–°å‘é‡ <span class="math inline">\(v'\)</span> è¡¨ç¤ºåŸå§‹å‘é‡ç»•åŸç‚¹æ—‹è½¬äº† <span class="math inline">\(\theta\)</span> è§’åº¦: <span class="math display">\[
v' = R(\theta) \cdot v = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta)\\
\sin(\theta) &amp; \cos(\theta)
\end{bmatrix} \cdot \begin{bmatrix}
x \\ y
\end{bmatrix} = \begin{bmatrix}
x \cos(\theta) - y \sin(\theta) \\
x \sin(\theta) + y \cos(\theta)
\end{bmatrix}
\]</span></p>
<p>ä¹‹åæˆ‘ä»¬è¦å­¦ä¹ çš„RoPE <span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span>ï¼Œä¹Ÿæ˜¯åŸºäºè¿™ä¸ªæ€§è´¨æ¥è®¾è®¡çš„ã€‚</p>
</div>
</div>
<hr>
<p>ç¬¬äºŒä¸ªå¥½å¤„æ˜¯ï¼š<span class="hilite-blue">å¯æ¨å¹¿æ€§</span>ã€‚ç”±äºä½ç½®ç¼–ç æ˜¯åŸºäºä½ç½®è®¡ç®—çš„ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå…·ä½“çš„è¯æ±‡ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ã€‚è¿™å¯¹äºå¤„ç†é•¿æ–‡æœ¬æˆ–é•¿åºåˆ—ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚</p>
<blockquote class="blockquote">
<p>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. <cite> Attention is all you need, p.6 </cite></p>
</blockquote>
<p>å‡è®¾æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶ï¼Œæ¨¡å‹è§è¿‡çš„æœ€å¤§åºåˆ—é•¿åº¦æ˜¯ <span class="math inline">\(L_{train}\)</span>ï¼Œé‚£ä¹ˆåœ¨æµ‹è¯•æ—¶ï¼Œå¦‚æœé‡åˆ°ä¸€ä¸ªæ›´é•¿çš„åºåˆ—ï¼Œé•¿åº¦ä¸º <span class="math inline">\(L_{test} &gt; L_{train}\)</span>ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å…¬å¼æ¥è®¡ç®—ä½ç½®ç¼–ç : <span id="eq-pos-embedding-generalization"><span class="math display">\[
PE_{(pos, 2i)} = \sin (pos / 10,000^{2i / d_{model}}), \quad pos \in [0, L_{test}-1]
\tag{10}\]</span></span></p>
<p>ä¸è¿‡ä¸ªäººè®¤ä¸ºï¼Œå¯æ‹“å±•çš„è¿˜æœ‰ä¸€ä¸ªåŸå› æ˜¯ï¼š<u>ç”±äº (<span class="math inline">\(pos+k\)</span>) çš„ç¼–ç å¯ä»¥è¡¨ç¤ºä¸ºä»…ä¾èµ– <span class="math inline">\(k\)</span> çš„çº¿æ€§å˜æ¢ä½œç”¨åœ¨ <span class="math inline">\(pos\)</span> çš„ç¼–ç ä¸Šï¼Œæ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ â€œç›¸å¯¹ä½ç§»â€çš„è§„å¾‹ï¼Œä»è€Œåœ¨æ›´é•¿åºåˆ—ä¸Šå…·å¤‡ä¸€å®šå¤–æ¨èƒ½åŠ›</u>ï¼ˆè®ºæ–‡ç”¨è¯ä¸º may allowï¼Œè¡¨ç¤ºå€¾å‘æ€§è€Œéä¸¥æ ¼ä¿è¯ï¼‰ã€‚</p>
</section>
</section>
<section id="sec-attention" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-attention"><span class="header-section-number">2.3</span> Attention Layer</h2>
<p>Attentionæœºåˆ¶æ˜¯Transformerçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ã€‚Attentionæœºåˆ¶çš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡è®¡ç®—æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ä¹‹é—´çš„ç›¸ä¼¼æ€§<a href="#eq-dot-product-matrix" class="quarto-xref">Equation&nbsp;3</a> æ¥å†³å®šå¦‚ä½•åŠ æƒè¾“å…¥ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒAttentionçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-attention"><span class="math display">\[
\boxed{\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V}
\tag{11}\]</span></span></p>
<p>ç”¨ä»£ç æ¥è¡¨ç¤ºå°±æ˜¯:</p>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> scaled_dot_product_attention(q, k, v):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    d_k <span class="op">=</span> q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3"></a>    scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb8-4"><a href="#cb8-4"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a>    output <span class="op">=</span> torch.matmul(attn, v)</span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="cf">return</span> output, attn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>æˆ‘ä»¬æ¥æ‹†çœ‹çœ‹ä¸€ä¸‹è¿™ä¸ªå…¬å¼ï¼Œç”±å››éƒ¨åˆ†ç»„æˆ:</p>
<ol type="1">
<li><span class="math inline">\(Q K^\top\)</span>: è®¡ç®—Queryå’ŒKeyä¹‹é—´çš„ç‚¹ç§¯ï¼Œå¾—åˆ°ç›¸ä¼¼æ€§çŸ©é˜µï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢ä¸æ‰€æœ‰é”®çš„ç›¸å…³æ€§ã€‚</li>
<li><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>: è¿™æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œç”¨äºé˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§ï¼Œå¯¼è‡´Softmaxå‡½æ•°çš„æ¢¯åº¦å˜å¾—éå¸¸å°ï¼Œä»è€Œå½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚è¿™é‡Œï¼Œ<span class="math inline">\(d_k\)</span> æ˜¯é”®å‘é‡çš„ç»´åº¦ã€‚</li>
<li><span class="math inline">\(\text{softmax}(\cdot)\)</span>: å¯¹ç›¸ä¼¼æ€§çŸ©é˜µè¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¯ä¸ªæŸ¥è¯¢å¯¹æ‰€æœ‰é”®çš„æ³¨æ„åŠ›æƒé‡ã€‚</li>
<li><span class="math inline">\(\cdot V\)</span>: ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚</li>
</ol>
<hr>
<p><tag style="color:blue"><span class="math inline">\(Q K^\top\)</span></tag></p>
<p>Attentionçš„ç¬¬ä¸€æ­¥ï¼Œå°±æ˜¯è®¡ç®—Queryå’ŒKeyä¹‹é—´çš„ç‚¹ç§¯ (<a href="#eq-dot-product-matrix" class="quarto-xref">Equation&nbsp;3</a>) ï¼Œå¾—åˆ°ç›¸ä¼¼æ€§çŸ©é˜µ, è¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢ä¸æ‰€æœ‰é”®çš„ç›¸å…³æ€§ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸ¥è¯¢çŸ©é˜µ <span class="math inline">\(Q \in \mathbb{R}^{n \times d_k}\)</span> å’Œä¸€ä¸ªé”®çŸ©é˜µ <span class="math inline">\(K \in \mathbb{R}^{m \times d_k}\)</span>ï¼Œé‚£ä¹ˆç‚¹ç§¯çŸ©é˜µ <span class="math inline">\(Q K^\top\)</span> çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-attention-dot-product-matrix"><span class="math display">\[
Q K^\top = \begin{bmatrix}
q_1 \\
q_2 \\
\vdots \\
q_n
\end{bmatrix}
\begin{bmatrix}
k_1^\top &amp; k_2^\top &amp; \cdots &amp; k_m^\top
\end{bmatrix} =
\begin{bmatrix}
q_1 k_1^\top &amp; q_1 k_2^\top &amp; \cdots &amp; q_1 k_m^\top \\
q_2 k_1^\top &amp; q_2 k_2^\top &amp; \cdots &amp; q_2 k_m^\top \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_n k_1^\top &amp; q_n k_2^\top &amp; \cdots &amp; q_n k_m^\top
\end{bmatrix}
\tag{12}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(q_i \in \mathbb{R}^{1 \times d_k}\)</span> æ˜¯æŸ¥è¯¢çŸ©é˜µ <span class="math inline">\(Q\)</span> çš„ç¬¬ <span class="math inline">\(i\)</span> è¡Œï¼Œ<span class="math inline">\(k_j \in \mathbb{R}^{1 \times d_k}\)</span> æ˜¯é”®çŸ©é˜µ <span class="math inline">\(K\)</span> çš„ç¬¬ <span class="math inline">\(j\)</span> è¡Œã€‚ç»“æœçŸ©é˜µ <span class="math inline">\(Q K^\top \in \mathbb{R}^{n \times m}\)</span> çš„æ¯ä¸ªå…ƒç´  <span class="math inline">\((i, j)\)</span> è¡¨ç¤ºæŸ¥è¯¢ <span class="math inline">\(q_i\)</span> ä¸é”® <span class="math inline">\(k_j\)</span> ä¹‹é—´çš„ç‚¹ç§¯ã€‚ <span class="math inline">\(QK^\top\)</span> çš„ä½œç”¨å°±æ˜¯å‘Šè¯‰æˆ‘ä»¬ï¼Œ<u>æ¯ä¸ªæŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰é”®å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§</u>ï¼Œç”¨äºä¹‹åä»å€¼å‘é‡ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚</p>
<blockquote class="blockquote">
<p>Dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. <cite> Attention is all you need, p.4 </cite></p>
</blockquote>
<hr>
<p><tag style="color:blue"><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span></tag></p>
<p>Attentionçš„ç¬¬äºŒæ­¥ï¼Œæ˜¯å¯¹ç‚¹ç§¯çŸ©é˜µè¿›è¡Œç¼©æ”¾ï¼Œä½¿ç”¨ <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> ä½œä¸ºç¼©æ”¾å› å­ã€‚å‡è®¾ <span class="math inline">\(q, k \sim \mathcal{N}(0, I)\)</span>ï¼Œé‚£ä¹ˆç‚¹ç§¯ <span class="math inline">\(q k^\top\)</span> çš„æœŸæœ›å’Œæ–¹å·®åˆ†åˆ«ä¸º:</p>
<p><span id="eq-attention-dot-product-variance"><span class="math display">\[
\mathbb{E}[q k^\top] = 0, \quad \text{Var}(q k^\top) = d_k
\tag{13}\]</span></span></p>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç‚¹ç§¯çš„æ–¹å·®ä¸é”®å‘é‡çš„ç»´åº¦ <span class="math inline">\(d_k\)</span> æˆæ­£æ¯”ã€‚éšç€ <span class="math inline">\(d_k\)</span> çš„å¢åŠ ï¼Œç‚¹ç§¯ logits çš„å°ºåº¦ä¼šä¸æ–­æ”¾å¤§ï¼Œä½¿å¾— softmax çš„è¾“å…¥æ›´å®¹æ˜“è¿›å…¥é¥±å’ŒåŒºï¼ˆsaturation regimeï¼‰ï¼Œæ­¤æ—¶æŸäº›ä½ç½®çš„æ¦‚ç‡æ¥è¿‘ 1ï¼Œå…¶ä½™æ¥è¿‘ 0ã€‚åœ¨è¯¥åŒºåŸŸå†…ï¼Œsoftmax çš„æ¢¯åº¦ä¼šæ˜¾è‘—å˜å°ï¼Œä»è€Œå¯¼è‡´åå‘ä¼ æ’­ä¸ç¨³å®šã€è®­ç»ƒæ•ˆç‡ä¸‹é™ã€‚é€šè¿‡é™¤ä»¥ <span class="math inline">\(\sqrt{d_k}\)</span>ï¼Œå¯ä»¥å°†ç‚¹ç§¯ logits çš„æ–¹å·®é‡æ–°å½’ä¸€åŒ–åˆ° <span class="math inline">\(O(1)\)</span> çš„å°ºåº¦ï¼Œä½¿ softmax å§‹ç»ˆå·¥ä½œåœ¨æ¢¯åº¦è¾ƒä¸ºæ•æ„Ÿçš„åŒºåŸŸï¼Œä»è€Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<div id="fig-scaling-d-k" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-d-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/scaling-d_k.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-d-k-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Effect of different scaling factor <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> on the distribution of dot-product valuesã€‚å¯ä»¥çœ‹åˆ°ï¼Œæœªç¼©æ”¾çš„ç‚¹ç§¯å€¼åˆ†å¸ƒåœ¨å‡ ä¸ªç‚¹ä¸Šï¼Œéšç€ç»´åº¦å¢åŠ ï¼Œåˆ†å¸ƒå˜å¾—æ›´åˆ†æ•£ï¼Œå¯¼è‡´softmaxæ›´å®¹æ˜“é¥±å’Œã€‚å¼•å…¥ç¼©æ”¾å› å­åï¼Œç‚¹ç§¯å€¼åˆ†å¸ƒä¿æŒç¨³å®šï¼Œæœ‰åŠ©äºsoftmaxçš„ç¨³å®šæ€§ã€‚
</figcaption>
</figure>
</div>
<p>å› æ­¤è¿™ä¸ªæœ‰æ—¶å€™æˆ‘ä»¬ç§°ä¹‹ä¸º â€œScale Dot-Product Attentionâ€ã€‚é€šè¿‡å¼•å…¥ç¼©æ”¾å› å­ <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç‚¹ç§¯çš„æ–¹å·®æ§åˆ¶åœ¨ä¸€ä¸ªåˆç†çš„èŒƒå›´å†…ï¼Œä»è€Œç¨³å®šSoftmaxå‡½æ•°çš„è¾“å‡ºï¼Œæ”¹å–„æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚</p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE Gradient of Softmax</p>
</div>
<div class="note-container foldable-content">
<p><span class="math display">\[
\frac{\partial \,\text{softmax}_i}{\partial z_j} = \text{softmax}_i (\delta_{ij} - \text{softmax}_j)
\]</span></p>
<p>å…¶ä¸­ï¼Œ <span class="math inline">\(\delta_{ij}\)</span> æ˜¯ <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker Delta</a>ï¼Œå½“ <span class="math inline">\(i=j\)</span> æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚</p>
<p>å½“æŸä¸€é¡¹ <span class="math inline">\(\text{softmax}_i \approx 1\)</span> æ—¶ï¼š</p>
<ul>
<li><span class="math inline">\(\text{softmax}_i (1 - \text{softmax}_i) \approx 0\)</span></li>
<li>å…¶ä»–é¡¹ <span class="math inline">\(\text{softmax}_j \approx 0\)</span></li>
</ul>
<p>æ‰€ä»¥æ‰€æœ‰çš„æ¢¯åº¦éƒ½æ¥è¿‘äº0</p>
</div>
</div>
<div id="fig-scaling-d-k-gradient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-d-k-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/scaling-d-k-gradient.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-d-k-gradient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Gradient of Softmax with different scaling factor <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>
</figcaption>
</figure>
</div>
<hr>
<p><tag style="color:blue">ç¬¬ä¸‰é¡¹ <span class="math inline">\(\text{softmax}(\cdot)\)</span></tag></p>
<p>Attentionçš„ç¬¬ä¸‰æ­¥ï¼Œæ˜¯å¯¹ç¼©æ”¾åçš„ç‚¹ç§¯çŸ©é˜µè¿›è¡ŒSoftmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¯ä¸ªæŸ¥è¯¢å¯¹æ‰€æœ‰é”®çš„æ³¨æ„åŠ›æƒé‡ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¼©æ”¾åçš„ç‚¹ç§¯çŸ©é˜µ <span class="math inline">\(S = \frac{Q K^\top}{\sqrt{d_k}}\)</span>ã€‚ è¿™éƒ¨åˆ†å¾ˆç›´è§‚ï¼Œæˆ‘ä»¬å¯¹çŸ©é˜µ <span class="math inline">\(S\)</span> çš„æ¯ä¸€è¡Œåº”ç”¨Softmaxå‡½æ•°ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡çŸ©é˜µ <span class="math inline">\(A\)</span>:</p>
<p><span id="eq-attention-softmax"><span class="math display">\[
A_{ij} = \frac{e^{S_{ij}}}{\sum_{k} e^{S_{ik}}}
\tag{14}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(A_{ij}\)</span> è¡¨ç¤ºæŸ¥è¯¢ <span class="math inline">\(q_i\)</span> å¯¹é”® <span class="math inline">\(k_j\)</span> çš„æ³¨æ„åŠ›æƒé‡ã€‚é€šè¿‡Softmaxå½’ä¸€åŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿æ¯ä¸ªæŸ¥è¯¢çš„æ³¨æ„åŠ›æƒé‡ä¹‹å’Œä¸º1ï¼Œä»è€Œå¯ä»¥å°†å…¶è§£é‡Šä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚è¿™äº›æ³¨æ„åŠ›æƒé‡åæ˜ äº†æ¯ä¸ªæŸ¥è¯¢ä¸æ‰€æœ‰é”®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œ<u>å¸®åŠ©æ¨¡å‹åŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†</u>ã€‚</p>
<hr>
<p><tag style="color:blue">ç¬¬å››é¡¹ <span class="math inline">\(\cdot V\)</span></tag></p>
<p>Attentionçš„æœ€åä¸€æ­¥ï¼Œæ˜¯ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå€¼çŸ©é˜µ <span class="math inline">\(V \in \mathbb{R}^{m \times d_v}\)</span> å’Œæ³¨æ„åŠ›æƒé‡çŸ©é˜µ <span class="math inline">\(A \in \mathbb{R}^{n \times m}\)</span>ï¼Œé‚£ä¹ˆè¾“å‡ºçŸ©é˜µ <span class="math inline">\(O \in \mathbb{R}^{n \times d_v}\)</span> çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-attention-weighted-sum"><span class="math display">\[
O = A V = \begin{bmatrix}
A_{11} &amp; A_{12} &amp; \cdots &amp; A_{1m} \\
A_{21} &amp; A_{22} &amp; \cdots &amp; A_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n1} &amp; A_{n2} &amp; \cdots &amp; A_{nm}
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_m
\end{bmatrix} =
\begin{bmatrix}
\sum_{j=1}^{m} A_{1j} v_j \\
\sum_{j=1}^{m} A_{2j} v_j \\
\vdots \\       
\sum_{j=1}^{m} A_{nj} v_j
\end{bmatrix}
\tag{15}\]</span></span> å…¶ä¸­ï¼Œ<span class="math inline">\(v_j \in \mathbb{R}^{1 \times d_v}\)</span> æ˜¯å€¼çŸ©é˜µ <span class="math inline">\(V\)</span> çš„ç¬¬ <span class="math inline">\(j\)</span> è¡Œã€‚ç»“æœçŸ©é˜µ <span class="math inline">\(O \in \mathbb{R}^{n \times d_v}\)</span> çš„æ¯ä¸€è¡Œè¡¨ç¤ºå¯¹åº”æŸ¥è¯¢çš„åŠ æƒå€¼å‘é‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ³¨æ„åŠ›æƒé‡åŠ¨æ€åœ°èšåˆè¾“å…¥ä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆæ›´å…·è¡¨è¾¾åŠ›çš„è¾“å‡ºè¡¨ç¤ºã€‚</p>
<div class="highlight">
<p>æ‹†çœ‹æ¥çœ‹ï¼ŒAttentionä¹Ÿæ²¡æœ‰æƒ³è±¡çš„è¿™ä¹ˆå¤æ‚ã€‚å®ƒä¸»è¦æ˜¯é€šè¿‡è®¡ç®—æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ¥å†³å®šå¦‚ä½•åŠ æƒè¾“å…¥çš„å€¼ï¼Œä»è€Œç”Ÿæˆè¾“å‡ºè¡¨ç¤ºã€‚</p>
<p>Attention å°±åƒåšèœï¼š</p>
<ul>
<li>Query å†³å®šä½ æƒ³åšä»€ä¹ˆï¼Œ</li>
<li>Key å†³å®šæ¯ä¸ªé£Ÿæçš„ç‰¹ç‚¹ï¼Œ</li>
<li><span class="math inline">\(QK^\top\)</span> æ˜¯æŠŠæ‰€æœ‰é£Ÿææ‘†åœ¨æ¡Œä¸Šï¼Œçœ‹çœ‹å“ªäº›æ¯”è¾ƒMatchä½ çš„éœ€æ±‚ï¼Œ</li>
<li><span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> æ˜¯è°ƒæ•´é£Ÿæçš„åˆ†é‡ï¼Œ</li>
<li>Softmax å†³å®šç”¨å¤šå°‘ï¼Œ</li>
<li>Value å†³å®šæœ€ç»ˆå‘³é“ã€‚</li>
</ul>
<p>å®ƒä¸åƒ RNN æˆ–è€… CNN</p>
<ul>
<li>ä½ ä¸æ˜¯æŒ‰â€œé£Ÿæé¡ºåºâ€å¤„ç†ï¼ˆä¸æ˜¯ RNNï¼‰</li>
<li>ä¹Ÿä¸æ˜¯åªçœ‹ç›¸é‚»å‡ æ ·ï¼ˆä¸æ˜¯ CNNï¼‰</li>
<li>è€Œæ˜¯ ä¸€æ¬¡æ€§çœ‹å®Œæ•´æ¡Œé£Ÿæï¼Œå†å†³å®šé‡ç‚¹</li>
</ul>
</div>
<section id="multi-head-attention" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">2.3.1</span> Multi-Head Attention</h3>
<p>Multi Head Attention å°±æ˜¯åœ¨ Attention çš„åŸºç¡€ä¸Šï¼Œ<u>å¹¶è¡Œåœ°è®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆAttention Headï¼‰</u>ï¼Œä»è€Œæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒå­ç©ºé—´ä¿¡æ¯ã€‚å…¶ä¸­æ¯ä¸€ä¸ªHeadï¼Œéƒ½æ˜¯ç‹¬ç«‹çš„ Self-Attention æœºåˆ¶ã€‚Multi-Head Attention çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-multi-head-attention"><span class="math display">\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W^O
\tag{16}\]</span></span></p>
<p>å…¶ä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´ <span class="math inline">\(\text{head}_i\)</span> çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹: <span id="eq-multi-head-attention-head"><span class="math display">\[
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
\tag{17}\]</span></span></p>
<p>æ¯ä¸ªHeadç‹¬ç«‹çš„è¿è¡Œï¼Œç„¶åå°†æ‰€æœ‰Headçš„è¾“å‡ºè¿›è¡Œæ‹¼æ¥ï¼ˆConcatï¼‰ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ <span class="math inline">\(W^O\)</span> å¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMulti-Head Attention èƒ½å¤ŸåŒæ—¶å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<blockquote class="blockquote">
<p>Multi-head attention allows the model to <u>jointly attend to information from different representation subspaces at different positions</u>. With a single attention head, averaging inhibits this. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
<div class="sourceCode" id="cb9" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig, is_causal: <span class="bu">bool</span>):</span>
<span id="cb9-3"><a href="#cb9-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>.is_causal <span class="op">=</span> is_causal</span>
<span id="cb9-6"><a href="#cb9-6"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_heads</span>
<span id="cb9-7"><a href="#cb9-7"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads</span>
<span id="cb9-8"><a href="#cb9-8"></a>        <span class="cf">assert</span> <span class="va">self</span>.head_dim <span class="op">*</span> <span class="va">self</span>.num_heads <span class="op">==</span> config.d_model, <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb9-9"><a href="#cb9-9"></a></span>
<span id="cb9-10"><a href="#cb9-10"></a>        <span class="va">self</span>.q_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-11"><a href="#cb9-11"></a>        <span class="va">self</span>.k_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-12"><a href="#cb9-12"></a>        <span class="va">self</span>.v_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb9-13"><a href="#cb9-13"></a>    </span>
<span id="cb9-14"><a href="#cb9-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, q, k, v):</span>
<span id="cb9-15"><a href="#cb9-15"></a>        b, q_len, _ <span class="op">=</span> q.size()</span>
<span id="cb9-16"><a href="#cb9-16"></a>        kv_len <span class="op">=</span> k.size(<span class="dv">1</span>)</span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>        <span class="co"># é€šè¿‡åˆ›å»ºviewå’Œtransposeå°†q, k, væ‹†åˆ†æˆå¤šä¸ªhead</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>        q <span class="op">=</span> <span class="va">self</span>.q_proj(q).view(b, q_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-20"><a href="#cb9-20"></a>        k <span class="op">=</span> <span class="va">self</span>.k_proj(k).view(b, kv_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-21"><a href="#cb9-21"></a>        v <span class="op">=</span> <span class="va">self</span>.v_proj(v).view(b, kv_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="self-attention-layer" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="self-attention-layer"><span class="header-section-number">2.3.2</span> Self-Attention Layer</h3>
<p>Self-Attention, é¡¾åæ€ä¹‰ï¼Œæ˜¯æŒ‡åœ¨è®¡ç®—Attentionæ—¶ï¼Œ<u>æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—</u>ã€‚è¿™ç§æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨åºåˆ—ä¸­çš„ä¸åŒä½ç½®ï¼Œä»è€Œæ•æ‰åˆ°åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚Self-Attentionçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-self-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
\tag{18}\]</span></span></p>
<p>åœ¨æ²¡æœ‰ä»»ä½•æ©ç çš„æƒ…å†µä¸‹ï¼ŒSelf-Attentionå…è®¸æ¯ä¸ªä½ç½®çš„æŸ¥è¯¢å‘é‡å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼ŒåŒ…æ‹¬å½“å‰ä½ç½®å’Œæœªæ¥ä½ç½®çš„ä¿¡æ¯ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<div class="sourceCode" id="cb10" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>out <span class="op">=</span> <span class="va">self</span>.attention(x, x, x) <span class="co"># Self-Attention</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="causal-self-attention-layer" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="causal-self-attention-layer"><span class="header-section-number">2.3.3</span> Causal Self-Attention Layer</h3>
<p><span id="eq-causal-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}} \textcolor{red}{+ M}\right) V
\tag{19}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(M \in \mathbb{R}^{n \times n}\)</span> æ˜¯ä¸€ä¸ªæ©ç çŸ©é˜µï¼ˆMask Matrixï¼‰ï¼Œç”¨äºé˜»æ­¢æ¨¡å‹åœ¨ç”Ÿæˆåºåˆ—æ—¶è®¿é—®æœªæ¥çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæ©ç çŸ©é˜µ <span class="math inline">\(M\)</span> çš„å®šä¹‰å¦‚ä¸‹:</p>
<p><span id="eq-mask-matrix"><span class="math display">\[
M_{ij} = \begin{cases}
-\infty, &amp; \text{if } j &gt; i \\
0, &amp; \text{otherwise}
\end{cases}
\tag{20}\]</span></span> è¿™ä¸ªæ©ç çŸ©é˜µç¡®ä¿äº†åœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ï¼ŒæŸ¥è¯¢ä½ç½® <span class="math inline">\(i\)</span> åªèƒ½å…³æ³¨åˆ°é”®ä½ç½® <span class="math inline">\(j \leq i\)</span> çš„ä¿¡æ¯ï¼Œä»è€Œå®ç°äº†è‡ªå›å½’ï¼ˆAuto-Regressiveï¼‰çš„ç‰¹æ€§ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚</p>
<div class="sourceCode" id="cb11" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">def</span> create_causal_mask(q_len: <span class="bu">int</span>, k_len: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="cf">return</span> torch.tril(torch.ones((q_len, k_len), dtype<span class="op">=</span>torch.<span class="bu">bool</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <span class="math inline">\(-\infty\)</span>) all values in the input of the softmax which correspond to illegal connections. <cite> Attention is all you need, p.5 </cite></p>
</blockquote>
<div id="fig-mask-in-attention" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-in-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mask-in-attention" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-attention-causal-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-attention-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/causal-mask.png" class="img-fluid figure-img" data-ref-parent="fig-mask-in-attention">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-attention-causal-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Causal Mask in Attention
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mask-in-attention" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-attention-causal-padding-mask" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-attention-causal-padding-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/causal-padding-mask.png" class="img-fluid figure-img" data-ref-parent="fig-mask-in-attention">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-attention-causal-padding-mask-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Casual Mask with Padding Positions
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-in-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: åœ¨ <a href="#fig-attention-causal-mask" class="quarto-xref">Figure&nbsp;5 (a)</a> ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ©ç çŸ©é˜µ <span class="math inline">\(M\)</span> çš„ç»“æ„ã€‚ä¸Šä¸‰è§’éƒ¨åˆ†è¢«è®¾ç½®ä¸º <span class="math inline">\(-\infty\)</span>ï¼Œè¡¨ç¤ºè¿™äº›ä½ç½®çš„æ³¨æ„åŠ›æƒé‡åœ¨ç»è¿‡ Softmax å½’ä¸€åŒ–åå°†å˜ä¸º 0ï¼Œä»è€Œé˜»æ­¢æ¨¡å‹å…³æ³¨æœªæ¥çš„ä¿¡æ¯ã€‚åœ¨ <a href="#fig-attention-causal-padding-mask" class="quarto-xref">Figure&nbsp;5 (b)</a> ä¸­ï¼Œ<span class="math inline">\((t_5, t_6, t_7)\)</span> æ˜¯å¡«å……ä½ç½®ï¼ˆPadding Positionsï¼‰ï¼Œè¿™äº›ä½ç½®åŒæ ·è¢«æ©ç æ‰ï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šå…³æ³¨åˆ°è¿™äº›æ— æ•ˆçš„ä¿¡æ¯ã€‚
</figcaption>
</figure>
</div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Padding Mask</p>
</div>
<div class="note-container foldable-content">
<p>é™¤äº†Causal Maskä¹‹å¤–, åœ¨å®é™…åº”ç”¨ä¸­, æˆ‘ä»¬è¿˜éœ€è¦å¤„ç†å˜é•¿åºåˆ—ä¸­çš„å¡«å……ä½ç½®(Padding Positions)ã€‚è¿™äº›ä½ç½®é€šå¸¸ç”¨ç‰¹æ®Šçš„å¡«å……å€¼(å¦‚0)è¡¨ç¤º, ä¸åŒ…å«æœ‰æ•ˆä¿¡æ¯ã€‚åœ¨è®¡ç®—Attentionæ—¶, æˆ‘ä»¬éœ€è¦ç¡®ä¿æ¨¡å‹ä¸ä¼šå…³æ³¨åˆ°è¿™äº›å¡«å……ä½ç½®, å› æ­¤æˆ‘ä»¬å¼•å…¥äº†Padding Maskã€‚ åœ¨è®¡ç®—Attentionä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†Padding Maskä¸Causal Maskç»“åˆä½¿ç”¨, å½¢æˆä¸€ä¸ªç»¼åˆçš„æ©ç çŸ©é˜µ <a href="#fig-attention-causal-padding-mask" class="quarto-xref">Figure&nbsp;5 (b)</a> ã€‚å…·ä½“æ¥è¯´, å¯¹äºå¡«å……ä½ç½®, æˆ‘ä»¬åŒæ ·å°†å¯¹åº”çš„æ³¨æ„åŠ›æƒé‡è®¾ç½®ä¸º <span class="math inline">\(-\infty\)</span>ï¼Œç¡®ä¿è¿™äº›ä½ç½®åœ¨ç»è¿‡Softmaxå½’ä¸€åŒ–åä¸ä¼šè¢«å…³æ³¨åˆ°ã€‚</p>
<div class="sourceCode" id="cb12" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    ...</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="kw">def</span> construct_mask(<span class="va">self</span>, pad_mask, q_len: <span class="bu">int</span>, k_len: <span class="bu">int</span>, device):</span>
<span id="cb12-4"><a href="#cb12-4"></a>        <span class="co"># True=allowed, False=masked</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>        mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a>        <span class="co"># causal mask (decoder self-attention only)</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>        <span class="cf">if</span> <span class="va">self</span>.is_causal:</span>
<span id="cb12-9"><a href="#cb12-9"></a>            causal_mask <span class="op">=</span> create_causal_mask(q_len, k_len)</span>
<span id="cb12-10"><a href="#cb12-10"></a>            causal_mask <span class="op">=</span> causal_mask.to(device)</span>
<span id="cb12-11"><a href="#cb12-11"></a>            mask <span class="op">=</span> causal_mask[<span class="va">None</span>, <span class="va">None</span>, :, :]</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a>        <span class="cf">if</span> pad_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-14"><a href="#cb12-14"></a>            <span class="co"># True means allowed</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>            pad_mask <span class="op">=</span> pad_mask[:, <span class="va">None</span>, <span class="va">None</span>, :].to(device)  <span class="co"># Shape: (batch, 1, 1, kv_len)</span></span>
<span id="cb12-16"><a href="#cb12-16"></a>            mask <span class="op">=</span> pad_mask <span class="cf">if</span> mask <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> (mask <span class="op">&amp;</span> pad_mask)</span>
<span id="cb12-17"><a href="#cb12-17"></a></span>
<span id="cb12-18"><a href="#cb12-18"></a>        <span class="cf">return</span> mask</span>
<span id="cb12-19"><a href="#cb12-19"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, q, k, v, pad_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-20"><a href="#cb12-20"></a>        ...</span>
<span id="cb12-21"><a href="#cb12-21"></a>        mask <span class="op">=</span> <span class="va">self</span>.construct_mask(pad_mask, q_len, kv_len, q.device)</span>
<span id="cb12-22"><a href="#cb12-22"></a>        out, attn <span class="op">=</span> scaled_dot_product_attention(q, k, v, mask)</span>
<span id="cb12-23"><a href="#cb12-23"></a>        ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>ç”¨ä»£ç æ¥è¡¨ç¤ºCausal Self-Attentionï¼Œ æˆ‘ä»¬åªéœ€è¦åšåŸæ¥çš„åŸºç¡€ä¸Šï¼Œ<span class="hilite-purple">åœ¨è®¡ç®—Softmaxä¹‹å‰</span>ï¼Œæ·»åŠ æ©ç çŸ©é˜µ <span class="math inline">\(M\)</span> å³å¯:</p>
<div class="sourceCode" id="cb13" data-code-line-numbers="5,10,11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> scaled_dot_product_attention(</span>
<span id="cb13-2"><a href="#cb13-2"></a>    q, </span>
<span id="cb13-3"><a href="#cb13-3"></a>    k, </span>
<span id="cb13-4"><a href="#cb13-4"></a>    v, </span>
<span id="cb13-5"><a href="#cb13-5"></a>    mask<span class="op">=</span><span class="va">None</span> </span>
<span id="cb13-6"><a href="#cb13-6"></a>    ):</span>
<span id="cb13-7"><a href="#cb13-7"></a>    d_k <span class="op">=</span> q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-8"><a href="#cb13-8"></a>    scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb13-9"><a href="#cb13-9"></a>    </span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: </span>
<span id="cb13-11"><a href="#cb13-11"></a>        scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>)) </span>
<span id="cb13-12"><a href="#cb13-12"></a>        </span>
<span id="cb13-13"><a href="#cb13-13"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb13-14"><a href="#cb13-14"></a>    output <span class="op">=</span> torch.matmul(attn, v)</span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="cf">return</span> output, attn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>å…¶ä¸­<code>mask</code>å‚æ•°æ˜¯ Padding Mask ä¸ Causal Mask çš„ç»“åˆã€‚</p>
</section>
<section id="cross-attention-layer" class="level3" data-number="2.3.4">
<h3 data-number="2.3.4" class="anchored" data-anchor-id="cross-attention-layer"><span class="header-section-number">2.3.4</span> Cross Attention Layer</h3>
<p>Cross-Attention æ˜¯æŒ‡åœ¨è®¡ç®— Attention æ—¶ï¼Œ<u>æŸ¥è¯¢ï¼ˆQueryï¼‰æ¥è‡ªè§£ç å™¨çš„è¾“å…¥åºåˆ—ï¼Œè€Œé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰æ¥è‡ªç¼–ç å™¨çš„è¾“å‡ºåºåˆ—</u>ã€‚è¿™ç§æœºåˆ¶å…è®¸è§£ç å™¨åœ¨ç”Ÿæˆè¾“å‡ºåºåˆ—æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œæ•æ‰åˆ°è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚Cross-Attention çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-cross-attention"><span class="math display">\[
\text{Attention}(Q_{dec}, \textcolor{red}{K_{enc}}, \textcolor{red}{V_{enc}}) = \text{softmax}\left(\frac{Q_{dec} \textcolor{red}{K_{enc}^\top}}{\sqrt{d_k}}\right) \textcolor{red}{V_{enc}}
\tag{21}\]</span></span></p>
<div class="sourceCode" id="cb14" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>out <span class="op">=</span> <span class="va">self</span>.attention(decoder_x, encoder_x, encoder_x) <span class="co"># Cross-Att</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-attention-complexity" class="level3" data-number="2.3.5">
<h3 data-number="2.3.5" class="anchored" data-anchor-id="sec-attention-complexity"><span class="header-section-number">2.3.5</span> Time Complexity of Attention</h3>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥åˆ†æä¸€ä¸‹ Self-Attention çš„æ—¶é—´å¤æ‚åº¦ã€‚å‡è®¾è¾“å…¥åºåˆ—çš„é•¿åº¦ä¸º <span class="math inline">\(n\)</span>ï¼ŒåµŒå…¥ç»´åº¦ä¸º <span class="math inline">\(d\)</span>ï¼Œé‚£ä¹ˆ Self-Attention çš„æ—¶é—´å¤æ‚åº¦ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:</p>
<ol type="1">
<li>è®¡ç®—ç‚¹ç§¯çŸ©é˜µ <span class="math inline">\(Q K^\top\)</span> çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰é”®å‘é‡è¿›è¡Œç‚¹ç§¯è®¡ç®—ï¼Œå…±æœ‰ <span class="math inline">\(n\)</span> ä¸ªæŸ¥è¯¢å’Œ <span class="math inline">\(n\)</span> ä¸ªé”®ï¼Œæ¯ä¸ªç‚¹ç§¯è®¡ç®—çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(d)\)</span>ã€‚</li>
<li>è®¡ç®— Softmax çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2)\)</span>ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡çš„ç‚¹ç§¯ç»“æœè¿›è¡Œå½’ä¸€åŒ–ï¼Œå…±æœ‰ <span class="math inline">\(n\)</span> ä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢éœ€è¦å¯¹ <span class="math inline">\(n\)</span> ä¸ªé”®è¿›è¡Œå½’ä¸€åŒ–ã€‚</li>
<li>è®¡ç®—åŠ æƒå’Œ $ V$ çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰å€¼å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œ å…±æœ‰ <span class="math inline">\(n\)</span> ä¸ªæŸ¥è¯¢å’Œ <span class="math inline">\(n\)</span> ä¸ªå€¼ï¼Œæ¯ä¸ªåŠ æƒæ±‚å’Œçš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(d)\)</span>ã€‚</li>
</ol>
<p><span id="eq-self-attention-time-complexity"><span class="math display">\[
\begin{array}{|l|l|}
\hline
\textbf{Step} &amp; \textbf{Time Complexity} \\
\hline
QK^\top &amp; \mathcal{O}(n^2 d) \\
\text{softmax}(QK^\top) &amp; \mathcal{O}(n^2) \\
\text{attention} \times V &amp; \mathcal{O}(n^2 d) \\
\hline
\textbf{Total} &amp; \mathcal{O}(n^2 d) \\
\hline
\end{array}
\tag{22}\]</span></span></p>
<p>ç»¼ä¸Šæ‰€è¿°ï¼ŒSelf-Attention çš„æ€»æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ã€‚éšç€è¾“å…¥åºåˆ—é•¿åº¦ <span class="math inline">\(n\)</span> çš„å¢åŠ ï¼Œæ—¶é—´å¤æ‚åº¦å‘ˆäºŒæ¬¡å¢é•¿ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´åœ¨å¤„ç†é•¿åºåˆ—æ—¶è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å„ç§ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰ã€å±€éƒ¨æ³¨æ„åŠ›ï¼ˆLocal Attentionï¼‰ç­‰ï¼Œä»¥é™ä½ Self-Attention çš„æ—¶é—´å¤æ‚åº¦ï¼Œæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
<div class="warning foldable is-open">
<div class="warning-header foldable-header">
<p>Warning: ç†è§£Attention Complexityçš„é‡è¦æ€§</p>
</div>
<div class="warning-container foldable-content">
<p>ç†è§£Attentionçš„Complexityå¾ˆé‡è¦, å› ä¸ºå®ƒç›´æ¥å½±å“åˆ°Transformeræ¨¡å‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´, å½“å¤„ç†é•¿åºåˆ—æ—¶, Attentionçš„è®¡ç®—å¤æ‚åº¦ä¼šæ˜¾è‘—å¢åŠ , è¿™å¯èƒ½å¯¼è‡´è®­ç»ƒå’Œæ¨ç†çš„æ—¶é—´æˆæœ¬å˜å¾—éå¸¸é«˜ã€‚å› æ­¤, ç ”ç©¶äººå‘˜æå‡ºäº†å„ç§ä¼˜åŒ–æ–¹æ³•, å¦‚ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰ã€å±€éƒ¨æ³¨æ„åŠ›ï¼ˆLocal Attentionï¼‰ï¼ŒLinear Attentionï¼ŒFlash Attentionï¼ŒåŒ…æ‹¬Deep Sparse Attentionç­‰ï¼Œéƒ½æ˜¯ä¸ºäº†é™ä½Attentionçš„è®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œæå‡Transformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚å¯ä»¥è¯´ï¼Œç†è§£äº†Attentionçš„Complexity, å°±ç†è§£äº†Transformerçš„æ•ˆç‡ç“¶é¢ˆæ‰€åœ¨ã€‚</p>
</div>
</div>
<div id="fig-time-complexity-of-self-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-time-complexity-of-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/time-complexity-of-self-attention.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-time-complexity-of-self-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Discussion on Time Complexity and Maximum Sequence Length of Self-Attention
</figcaption>
</figure>
</div>
<div class="columns">
<div class="column">
<p><img src="./assets/wait-meme.png" class="img-fluid" style="width:90.0%" data-align="center"></p>
</div><div class="column">
<p><span class="hilite-pink">ç­‰ä¸€ç­‰ï¼Œç¨³ä¸€ç¨³ï¼Œå¿ä¸€å¿</span></p>
<p>RNNçš„æ—¶é—´å¤æ‚åº¦æ˜¯<span class="math inline">\(\mathcal{O}(n d^2)\)</span>ï¼ŒTransformerçš„æ—¶é—´å¤æ‚åº¦æ˜¯ <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ï¼Œé‚£RNNä¸æ˜¯æ›´å¿«å—ï¼Ÿ</p>
<p>ä¸ä¸€å®š! Transformer å¾€å¾€æ›´å¿«çš„å…³<u>é”®ä¸åœ¨äºæŠŠæ€»å¤æ‚åº¦å˜æˆ <span class="math inline">\(\mathcal{O}(1)\)</span>ï¼Œè€Œåœ¨äºæŠŠâ€œåºåˆ—ç»´åº¦ä¸Šçš„è®¡ç®—â€ä»å¿…é¡»ä¸²è¡Œï¼Œå˜æˆå¯å¹¶è¡Œçš„çŸ©é˜µè¿ç®—ä¹Ÿå°±æ˜¯<a href="#fig-time-complexity-of-self-attention" class="quarto-xref">Figure&nbsp;6</a>é‡Œçš„ <em>Sequential Operations</em> å¯¹æ¯”ï¼‰</u>ã€‚å› æ­¤åœ¨ GPU/TPU ä¸Šï¼ŒTransformer çš„ååé€šå¸¸æ›´é«˜ã€‚</p>
<ol type="1">
<li><strong>å¹¶è¡Œè®¡ç®— / å¹¶è¡Œæ·±åº¦ï¼ˆcritical pathï¼‰</strong>ï¼šRNN å­˜åœ¨ä¸¥æ ¼çš„æ—¶é—´æ­¥ä¾èµ–ï¼Œå¿…é¡»æŒ‰æ­¥è®¡ç®—ï¼Œå¯¼è‡´å¹¶è¡Œæ·±åº¦éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼ˆ<span class="math inline">\(\mathcal{O}(n)\)</span>ï¼‰ï¼›è€Œ Self-Attention åœ¨ä¸€ä¸ªå±‚å†…å¯ä»¥ç”¨å‡ æ¬¡çŸ©é˜µä¹˜æ³•åŒæ—¶å¤„ç†æ‰€æœ‰ä½ç½®ï¼Œå› æ­¤å¹¶è¡Œæ·±åº¦æ˜¯å¸¸æ•°çº§<span class="math inline">\(\mathcal{O}(1)\)</span>ã€‚</li>
<li><strong>ç“¶é¢ˆä¸åŒï¼ˆ<span class="math inline">\(d\)</span> vs <span class="math inline">\(n\)</span>ï¼‰</strong>ï¼šRNN å¯¹éšè—ç»´ <span class="math inline">\(d\)</span> çš„ä¸»è¦æˆæœ¬æ˜¯äºŒæ¬¡ï¼ˆ<span class="math inline">\(\mathcal{O}(n d^2)\)</span>ï¼‰ï¼Œè€Œ attention å¯¹ <span class="math inline">\(d\)</span> è¿‘ä¼¼ä¸€æ¬¡ã€ä½†å¯¹åºåˆ—é•¿åº¦ <span class="math inline">\(n\)</span> æ˜¯äºŒæ¬¡ï¼ˆ<span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ï¼‰ã€‚æ‰€ä»¥å½“åºåˆ—éå¸¸é•¿æ—¶ï¼Œattention çš„ <span class="math inline">\(n^2\)</span> ä¼šæˆä¸ºç“¶é¢ˆï¼Œå®è·µä¸­å¸¸ç”¨
<ul>
<li>FlashAttention<span class="citation" data-cites="FlashAttentionFastMemoryEfficient2022dao">(<a href="#ref-FlashAttentionFastMemoryEfficient2022dao" role="doc-biblioref">Dao et al. 2022</a>)</span> (ä¼˜åŒ–å¸¸æ•°ä¸æ˜¾å­˜/IO)</li>
<li><strong>Window / restricted attention</strong>ï¼ˆå°†å…¨å±€æ³¨æ„åŠ›æ”¹ä¸ºå±€éƒ¨çª—å£ï¼Œç±»ä¼¼å›¾<a href="#fig-time-complexity-of-self-attention" class="quarto-xref">Figure&nbsp;6</a> ä¸­çš„â€œrestricted self-attentionâ€é‚£ä¸€è¡Œï¼‰æ¥è¿›ä¸€æ­¥æå‡é•¿åºåˆ—æ•ˆç‡ã€‚</li>
</ul></li>
</ol>
</div>
</div>
<p>ä¸€å¥è¯æ€»ç»“Attentionå°±æ˜¯:</p>
<div class="highlight">
<p><tag style="color:green">Attention is Weighted Sum of Values, where Weights are from Softmax of Scaled Dot-Product of Queries and Keys.</tag></p>
</div>
</section>
</section>
<section id="sec-normalization" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-normalization"><span class="header-section-number">2.4</span> Normalization Layer</h2>
<p>Layer Normalization <span class="citation" data-cites="LayerNormalization2016ba">(<a href="#ref-LayerNormalization2016ba" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span> æ˜¯ä¸€ç§ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œçš„å½’ä¸€åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œé€Ÿåº¦ã€‚ä¸æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ä¸åŒï¼ŒLayer Normalization æ˜¯åœ¨<u>æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦ä¸Š</u> (<span class="math inline">\(d_{model}\)</span>) è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨æ‰¹é‡ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–ã€‚è¿™ä½¿å¾— Layer Normalization ç‰¹åˆ«é€‚ç”¨äºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œ Transformer ç­‰æ¨¡å‹ã€‚</p>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Questionï¼šä¸ºä»€ä¹ˆLayer Normalizationæ›´é€‚åˆSequence Modelingï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<ol type="1">
<li><strong>åºåˆ—é•¿åº¦å˜åŒ–</strong>: åœ¨å¤„ç†å˜é•¿åºåˆ—æ—¶ï¼Œæ‰¹é‡å½’ä¸€åŒ–å¯èƒ½ä¼šå—åˆ°ä¸åŒé•¿åº¦åºåˆ—çš„å½±å“ï¼Œè€Œ Layer Normalization å¯ä»¥ç‹¬ç«‹äºåºåˆ—é•¿åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚</li>
<li><strong>æ—¶é—´æ­¥ä¾èµ–</strong>: åœ¨ RNN ä¸­ï¼Œæ—¶é—´æ­¥ä¹‹é—´å­˜åœ¨ä¾èµ–å…³ç³»ï¼Œæ‰¹é‡å½’ä¸€åŒ–å¯èƒ½ä¼šç ´åè¿™ç§ä¾èµ–å…³ç³»ï¼Œè€Œ Layer Normalization ä¿æŒäº†æ—¶é—´æ­¥ä¹‹é—´çš„ç‹¬ç«‹æ€§ã€‚</li>
<li><strong>å°æ‰¹é‡å¤§å°</strong>: åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ‰¹é‡å¤§å°å¯èƒ½éå¸¸å°ï¼Œç”šè‡³ä¸º1ï¼Œè¿™ä½¿å¾—æ‰¹é‡å½’ä¸€åŒ–æ•ˆæœä¸ä½³ï¼Œè€Œ Layer Normalization ä¸ä¾èµ–äºæ‰¹é‡å¤§å°ã€‚</li>
</ol>
</div>
</div>
<p>Layer Normalization çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-layer-normalization"><span class="math display">\[
\begin{split}
\mu &amp; = \frac{1}{d} \sum_{i=1}^{d} x_i \\
\sigma^2 &amp; = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2 \\
\hat{x_i} &amp; = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
y_i &amp; = \gamma \odot \hat{x_i} + \beta
\end{split}
\tag{23}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(x_i\)</span> æ˜¯è¾“å…¥å‘é‡çš„ç¬¬ <span class="math inline">\(i\)</span> ä¸ªå…ƒç´ ï¼Œ<span class="math inline">\(d\)</span> æ˜¯å‘é‡çš„ç»´åº¦ï¼Œ<span class="math inline">\(\mu\)</span> å’Œ <span class="math inline">\(\sigma^2\)</span> åˆ†åˆ«æ˜¯å‡å€¼å’Œæ–¹å·®ï¼Œ<span class="math inline">\(\epsilon\)</span> æ˜¯ä¸€ä¸ªå°å¸¸æ•°ï¼Œç”¨äºé˜²æ­¢é™¤é›¶é”™è¯¯ï¼Œ<span class="math inline">\(\gamma \in \mathbb{R}^d\)</span> å’Œ <span class="math inline">\(\beta \in \mathbb{R}^d\)</span> æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œç”¨äºç¼©æ”¾å’Œå¹³ç§»å½’ä¸€åŒ–åçš„è¾“å‡ºã€‚</p>
<div class="sourceCode" id="cb15" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span>, eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb15-3"><a href="#cb15-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-4"><a href="#cb15-4"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb15-5"><a href="#cb15-5"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(dim))</span>
<span id="cb15-6"><a href="#cb15-6"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(dim))</span>
<span id="cb15-7"><a href="#cb15-7"></a></span>
<span id="cb15-8"><a href="#cb15-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb15-9"><a href="#cb15-9"></a>        mean <span class="op">=</span> x.mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-10"><a href="#cb15-10"></a>        var <span class="op">=</span> x.var(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-11"><a href="#cb15-11"></a></span>
<span id="cb15-12"><a href="#cb15-12"></a>        x_hat <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> torch.sqrt(var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb15-13"><a href="#cb15-13"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> x_hat <span class="op">+</span> <span class="va">self</span>.beta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question: ä¸ºä»€ä¹ˆ <span class="math inline">\(\gamma\)</span> å’Œ <span class="math inline">\(\beta\)</span> æ˜¯å¿…è¦çš„? å¹¶ä¸”è¦åˆå§‹åŒ–ä¸º1å’Œ0?</p>
</div>
<div class="question-container foldable-content">
<p>ä¸ºä»€ä¹ˆæ˜¯å¿…è¦çš„ï¼Ÿ</p>
<p>åœ¨Normalizationä¹‹å <span class="math inline">\(\hat{x_i}\)</span> æ˜¯å½’ä¸€åŒ–çš„è¾“å‡º, å…¶å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ã€‚è¿™å¾ˆç¨³å®šï¼Œä½†ä¹Ÿæœ‰ä¸€ä¸ªå‰¯ä½œç”¨ï¼šæ¨¡å‹å¤±å»äº†â€œæƒ³è¦å¤šå¤§å°ºåº¦/ä»€ä¹ˆå‡å€¼â€çš„è‡ªç”±åº¦ã€‚å¦‚æœæ²¡æœ‰ <span class="math inline">\(\gamma\)</span> å’Œ <span class="math inline">\(\beta\)</span>ï¼Œæ¨¡å‹å°†å¤±å»å¯¹åŸå§‹æ•°æ®åˆ†å¸ƒçš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ <span class="math inline">\(\gamma\)</span> å’Œ <span class="math inline">\(\beta\)</span>ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°é€‚åˆå½“å‰ä»»åŠ¡çš„ç¼©æ”¾å’Œå¹³ç§»ï¼Œä»è€Œæ¢å¤æˆ–è°ƒæ•´æ•°æ®çš„åˆ†å¸ƒã€‚</p>
<p>ä¸ºä»€ä¹ˆåˆå§‹åŒ–ä¸º1å’Œ0ï¼Ÿ</p>
<p>åˆå§‹åŒ–æˆ <span class="math inline">\(\gamma=1, \beta=0\)</span> æ—¶ï¼Œ<span class="math inline">\(y_i = \hat{x_i}\)</span>ï¼Œå³åˆå§‹æ—¶ Layer Normalization çš„è¾“å‡ºä¸å½’ä¸€åŒ–åçš„è¾“å…¥ç›¸åŒã€‚è¿™ç§åˆå§‹åŒ–æ–¹å¼ç¡®ä¿äº†åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼ŒLayer Normalization ä¸ä¼šå¯¹æ•°æ®è¿›è¡Œä»»ä½•ç¼©æ”¾æˆ–å¹³ç§»ï¼Œä»è€Œé¿å…äº†å¯¹æ¨¡å‹è®­ç»ƒçš„å¹²æ‰°ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ <span class="math inline">\(\gamma\)</span> å’Œ <span class="math inline">\(\beta\)</span> çš„å€¼ï¼Œä»¥é€‚åº”å…·ä½“ä»»åŠ¡çš„éœ€æ±‚ã€‚</p>
</div>
</div>
</section>
<section id="sec-feed-forward" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-feed-forward"><span class="header-section-number">2.5</span> Feed Forward Layer</h2>
<p>åœ¨Transformerä¸­ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰æ˜¯æ¯ä¸ªç¼–ç å™¨å’Œè§£ç å™¨å±‚ä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚å‰é¦ˆç¥ç»ç½‘ç»œé€šå¸¸ç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ç»„æˆï¼Œå…·ä½“è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹: <span id="eq-feed-forward-network"><span class="math display">\[
\text{FFN}(\mathrm{x}) = \underset{}{\max} (0, \mathrm{x} W_{1} + b_{1}) W_{2} + b_{2}
\tag{24}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\mathrm{x} \in \mathbb{R}^{d_{model}}\)</span> æ˜¯è¾“å…¥å‘é‡ï¼Œ<span class="math inline">\(W_{1} \in \mathbb{R}^{d_{model} \times d_{ff}}\)</span> å’Œ <span class="math inline">\(W_{2} \in \mathbb{R}^{d_{ff} \times d_{model}}\)</span> æ˜¯æƒé‡çŸ©é˜µï¼Œ<span class="math inline">\(b_{1} \in \mathbb{R}^{d_{ff}}\)</span> å’Œ <span class="math inline">\(b_{2} \in \mathbb{R}^{d_{model}}\)</span> æ˜¯åç½®å‘é‡ï¼Œ<span class="math inline">\(d_{ff}\)</span> æ˜¯å‰é¦ˆç½‘ç»œçš„éšè—å±‚ç»´åº¦ï¼Œé€šå¸¸å¤§äº <span class="math inline">\(d_{model}\)</span>ï¼Œåœ¨åŸå§‹çš„Transformerè®ºæ–‡ä¸­ï¼Œ<span class="math inline">\(d_{ff}\)</span> é€šå¸¸è®¾ç½®ä¸º <span class="math inline">\(4 \times d_{model}\)</span>ã€‚</p>
<p>ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å‰é¦ˆç¥ç»ç½‘ç»œï¼Ÿä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› :</p>
<ol type="1">
<li><strong>éçº¿æ€§å˜æ¢</strong>: å‰é¦ˆç¥ç»ç½‘ç»œå¼•å…¥äº†éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li><strong>ä½ç½®ç‹¬ç«‹æ€§</strong>: å‰é¦ˆç¥ç»ç½‘ç»œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œç‹¬ç«‹çš„å˜æ¢ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹æ•æ‰æ¯ä¸ªä½ç½®çš„ç‰¹å¾ï¼Œè€Œä¸å—å…¶ä»–ä½ç½®çš„å½±å“ã€‚</li>
<li><strong>å¢åŠ æ¨¡å‹å®¹é‡</strong>: é€šè¿‡å¢åŠ å‰é¦ˆç¥ç»ç½‘ç»œçš„éšè—å±‚ç»´åº¦ <span class="math inline">\(d_{ff}\)</span>ï¼Œå¯ä»¥æ˜¾è‘—å¢åŠ æ¨¡å‹çš„å®¹é‡ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<div class="sourceCode" id="cb16" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="kw">class</span> FFN(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb16-3"><a href="#cb16-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-4"><a href="#cb16-4"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(config.d_model, config.d_ff)</span>
<span id="cb16-5"><a href="#cb16-5"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(config.d_ff, config.d_model)</span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-8"><a href="#cb16-8"></a>        <span class="cf">return</span> <span class="va">self</span>.fc2(F.relu(<span class="va">self</span>.fc1(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="residual-connection" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="residual-connection"><span class="header-section-number">2.6</span> Residual Connection</h2>
<p>å½“ç„¶ï¼Œå¦‚æœè¦è®­ç»ƒä¸€ä¸ªDEEP Transformeræ¨¡å‹ï¼Œé¿ä¸å¼€çš„å°±æ˜¯Residual Connection <span class="citation" data-cites="DeepResidualLearning2015he">(<a href="#ref-DeepResidualLearning2015he" role="doc-biblioref">He et al. 2015</a>)</span>, å®ƒçš„ä½œç”¨æ˜¯ç¼“è§£æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œä½¿å¾—æ›´æ·±å±‚çš„ç½‘ç»œèƒ½å¤Ÿè¢«æœ‰æ•ˆè®­ç»ƒã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥ï¼ˆSkip Connectionï¼‰ï¼Œå°†è¾“å…¥ç›´æ¥æ·»åŠ åˆ°è¾“å‡ºä¸Šï¼Œä»è€Œå½¢æˆä¸€ä¸ªâ€œæ·å¾„â€ï¼Œä½¿å¾—æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ é€’åˆ°æ›´æ—©çš„å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå­å±‚ï¼ˆSublayerï¼‰ï¼Œå…¶è¾“å…¥ä¸º <span class="math inline">\(\mathbf   {x}\)</span>ï¼Œè¾“å‡ºä¸º <span class="math inline">\(\mathrm{Sublayer}(\mathbf{x})\)</span>ï¼Œé‚£ä¹ˆå¼•å…¥æ®‹å·®è¿æ¥åçš„è¾“å‡º <span class="math inline">\(\mathbf{y}\)</span> å¯ä»¥è¡¨ç¤ºä¸º:</p>
<p><span class="math display">\[
\mathbf{y} = \text{LayerNorm}(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x}))
\]</span></p>
<section id="backpropagation-through-residual-connection" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="backpropagation-through-residual-connection"><span class="header-section-number">2.6.1</span> Backpropagation through Residual Connection</h3>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥ç®€å•åˆ†æä¸€ä¸‹ï¼Œæ®‹å·®è¿æ¥æ˜¯å¦‚ä½•å¸®åŠ©ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸå¤±å‡½æ•° <span class="math inline">\(\mathcal{L}\)</span>ï¼Œ</p>
<p><span id="eq-residual-connection"><span class="math display">\[
\mathbf{y} = \mathbf{x} + \mathrm{Sublayer}(\mathbf{x})
\tag{25}\]</span></span></p>
<p>æˆ‘ä»¬æƒ³è¦è®¡ç®—æŸå¤±å‡½æ•°å¯¹è¾“å…¥ <span class="math inline">\(\mathbf{x}\)</span> çš„æ¢¯åº¦ <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span>ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°:</p>
<p><span id="eq-backprop-residual"><span class="math display">\[
\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial}{\partial \mathbf{x}}\left(\mathbf{x}+\mathrm{Sublayer}(\mathbf{x})\right) \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left(\frac{\partial \mathbf{x}}{\partial \mathbf{x}} +
\frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left( \mathbf{I} + \frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}_{\text{straight path}} +
\underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot
\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}}
\end{split}
\tag{26}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(y\)</span> æ˜¯æ®‹å·®è¿æ¥çš„è¾“å‡ºï¼Œ<span class="math inline">\(\mathbf{I}\)</span> æ˜¯å•ä½çŸ©é˜µã€‚å¯ä»¥çœ‹åˆ°ï¼Œæ¢¯åº¦ <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span> åŒ…å«äº†ä¸¤éƒ¨åˆ†:</p>
<ul>
<li><strong>Straight Path</strong>: è¿™éƒ¨åˆ†æ¢¯åº¦ç›´æ¥æ¥è‡ªäºæŸå¤±å‡½æ•°å¯¹è¾“å‡º <span class="math inline">\(\mathbf{y}\)</span> çš„æ¢¯åº¦ <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\)</span>ï¼Œå®ƒä¸ç»è¿‡ä»»ä½•å­å±‚çš„å˜æ¢ï¼Œå› æ­¤ä¸ä¼šå—åˆ°æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ã€‚</li>
<li><strong>Through the Sub-layer</strong>: è¿™éƒ¨åˆ†æ¢¯åº¦é€šè¿‡å­å±‚çš„å˜æ¢ä¼ æ’­ï¼Œå¯èƒ½ä¼šå—åˆ°æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ã€‚</li>
</ul>
<p>é€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥ï¼Œæ¨¡å‹å¯ä»¥ç¡®ä¿æ¢¯åº¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è‡³å°‘æœ‰ä¸€éƒ¨åˆ†ï¼ˆStraight Pathï¼‰èƒ½å¤Ÿç›´æ¥ä¼ é€’åˆ°æ›´æ—©çš„å±‚ï¼Œä»è€Œç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚è¿™ä½¿å¾—æ·±å±‚ç½‘ç»œèƒ½å¤Ÿè¢«æœ‰æ•ˆè®­ç»ƒï¼Œä»è€Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</section>
</section>
<section id="sec-output-layer" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="sec-output-layer"><span class="header-section-number">2.7</span> Output Layer</h2>
<p>åœ¨Transformerçš„è¾“å‡ºå±‚ï¼Œé€šå¸¸ä¼šä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆLinear Layerï¼‰å°†è§£ç å™¨çš„è¾“å‡ºè½¬æ¢ä¸ºè¯æ±‡è¡¨å¤§å°çš„å‘é‡ï¼Œç„¶åé€šè¿‡Softmaxå‡½æ•°<a href="#eq-softmax" class="quarto-xref">Equation&nbsp;1</a> å°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾è§£ç å™¨çš„è¾“å‡ºä¸º <span class="math inline">\(\mathbf{h} \in \mathbb{R}^{d_{model}}\)</span>ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º <span class="math inline">\(V\)</span>ï¼Œé‚£ä¹ˆè¾“å‡ºå±‚çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-output-layer"><span class="math display">\[
\mathbf{y} = \text{Softmax}(\mathbf{h} W_{o} + b_o)
\tag{27}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(W_{o} \in \mathbb{R}^{d_{model} \times V}\)</span> æ˜¯çº¿æ€§å±‚çš„æƒé‡çŸ©é˜µï¼Œ<span class="math inline">\(b_o \in \mathbb{R}^{V}\)</span> æ˜¯åç½®å‘é‡ï¼Œ<span class="math inline">\(\mathbf{y} \in \mathbb{R}^{V}\)</span> æ˜¯æœ€ç»ˆçš„é¢„æµ‹ç»“æœï¼Œè¡¨ç¤ºæ¯ä¸ªè¯æ±‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<div class="sourceCode" id="cb17" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb17-2"><a href="#cb17-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb17-3"><a href="#cb17-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-4"><a href="#cb17-4"></a>        ...</span>
<span id="cb17-5"><a href="#cb17-5"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(config.d_model, config.tgt_vocab_size)</span>
<span id="cb17-6"><a href="#cb17-6"></a>    </span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, original, target):</span>
<span id="cb17-8"><a href="#cb17-8"></a>        ...</span>
<span id="cb17-9"><a href="#cb17-9"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_layer(y_dec)</span>
<span id="cb17-10"><a href="#cb17-10"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="encoder-decoder-layer" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="encoder-decoder-layer"><span class="header-section-number">2.8</span> Encoder &amp; Decoder Layer</h2>
<p>æœ‰äº†è¿™äº›åŸºç¡€ç»„ä»¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥å’Œå ç§¯æœ¨ä¸€æ ·ï¼Œæ¥æ­å»ºTransformerçš„Encoderå’ŒDecoderå±‚äº†ã€‚</p>
<p>Encoder å¯ä»¥è¡¨ç¤ºä¸º:</p>
<p><span id="eq-encoder-layer"><span class="math display">\[
\begin{split}
\text{EncoderLayer}_{i}(\mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{x} + \text{MultiHeadSelfAttention}_{i}(\mathrm{x}, \mathrm{x},\mathrm{x})\right) \\
\text{EncoderLayer}_{i}(\mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{x} + \text{FFN}_{i}(\mathrm{x})\right) \\
\end{split}
\tag{28}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\mathrm{x}\)</span> æ˜¯è¾“å…¥å‘é‡ï¼Œ<span class="math inline">\(\text{MultiHeadSelfAttention}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªç¼–ç å™¨å±‚çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œ<span class="math inline">\(\text{FFN}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªç¼–ç å™¨å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œ<span class="math inline">\(\text{LayerNorm}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªç¼–ç å™¨å±‚çš„å±‚å½’ä¸€åŒ–ã€‚</p>
<div class="sourceCode" id="cb18" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb18-3"><a href="#cb18-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-6"><a href="#cb18-6"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb18-7"><a href="#cb18-7"></a></span>
<span id="cb18-8"><a href="#cb18-8"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb18-9"><a href="#cb18-9"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb18-10"><a href="#cb18-10"></a></span>
<span id="cb18-11"><a href="#cb18-11"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor, pad_mask<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-14"><a href="#cb18-14"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.attn(x, x, x, pad_mask<span class="op">=</span>pad_mask)</span>
<span id="cb18-15"><a href="#cb18-15"></a>        x <span class="op">=</span> <span class="va">self</span>.ln1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))</span>
<span id="cb18-16"><a href="#cb18-16"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb18-17"><a href="#cb18-17"></a>        x <span class="op">=</span> <span class="va">self</span>.ln2(x <span class="op">+</span> ffn_output)</span>
<span id="cb18-18"><a href="#cb18-18"></a></span>
<span id="cb18-19"><a href="#cb18-19"></a>        <span class="cf">return</span> x</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb18-22"><a href="#cb18-22"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb18-23"><a href="#cb18-23"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-24"><a href="#cb18-24"></a></span>
<span id="cb18-25"><a href="#cb18-25"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([EncoderBlock(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_layers)])</span>
<span id="cb18-26"><a href="#cb18-26"></a></span>
<span id="cb18-27"><a href="#cb18-27"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor, pad_mask<span class="op">=</span><span class="va">None</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-28"><a href="#cb18-28"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-29"><a href="#cb18-29"></a>            x <span class="op">=</span> layer(x, pad_mask<span class="op">=</span>pad_mask)</span>
<span id="cb18-30"><a href="#cb18-30"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ä¸‹å›¾å±•ç¤ºäº†Encoder Layerçš„è¿‡ç¨‹ï¼š</p>
<div id="fig-encoder-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/transformer-encoding.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Encoding Layer Process
</figcaption>
</figure>
</div>
<p>Decoder å¯ä»¥è¡¨ç¤ºä¸º: <span id="eq-decoder-layer"><span class="math display">\[
\begin{split}
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{CausalMultiHeadSelfAttention}_{i}(\mathrm{y}, \mathrm{y}, \mathrm{y})\right) \\
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{CrossAttention}_{i}(\mathrm{y}, \mathrm{x}_{enc}, \mathrm{x}_{enc})\right) \\
\text{DecoderLayer}_{i}(\mathrm{y}, \mathrm{x}) &amp; = \text{LayerNorm}_{i}\left(\mathrm{y} + \text{FFN}_{i}(\mathrm{y})\right) \\
\end{split}
\tag{29}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\mathrm{y}\)</span> æ˜¯è§£ç å™¨çš„è¾“å…¥å‘é‡ï¼Œ<span class="math inline">\(\mathrm{x}_{enc}\)</span> æ˜¯ç¼–ç å™¨çš„è¾“å‡ºå‘é‡ï¼Œ<span class="math inline">\(\text{CausalMultiHeadSelfAttention}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªè§£ç å™¨å±‚çš„å› æœå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œ<span class="math inline">\(\text{CrossAttention}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªè§£ç å™¨å±‚çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œ<span class="math inline">\(\text{FFN}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªè§£ç å™¨å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œ<span class="math inline">\(\text{LayerNorm}_{i}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span> ä¸ªè§£ç å™¨å±‚çš„å±‚å½’ä¸€åŒ–ã€‚</p>
<div class="sourceCode" id="cb19" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">class</span> DecoderBlock(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb19-3"><a href="#cb19-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>        <span class="va">self</span>.self_attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-9"><a href="#cb19-9"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb19-12"><a href="#cb19-12"></a>        <span class="va">self</span>.ln3 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb19-13"><a href="#cb19-13"></a></span>
<span id="cb19-14"><a href="#cb19-14"></a>    <span class="kw">def</span> forward(</span>
<span id="cb19-15"><a href="#cb19-15"></a>        <span class="va">self</span>,</span>
<span id="cb19-16"><a href="#cb19-16"></a>        x: torch.Tensor,</span>
<span id="cb19-17"><a href="#cb19-17"></a>        enc_output: torch.Tensor,</span>
<span id="cb19-18"><a href="#cb19-18"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-19"><a href="#cb19-19"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-20"><a href="#cb19-20"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb19-21"><a href="#cb19-21"></a>        self_attn_output, _ <span class="op">=</span> <span class="va">self</span>.self_attn(x, x, x, pad_mask<span class="op">=</span>tgt_pad_mask)</span>
<span id="cb19-22"><a href="#cb19-22"></a>        x <span class="op">=</span> <span class="va">self</span>.ln1(x <span class="op">+</span> self_attn_output)</span>
<span id="cb19-23"><a href="#cb19-23"></a></span>
<span id="cb19-24"><a href="#cb19-24"></a>        cross_attn_output, _ <span class="op">=</span> <span class="va">self</span>.cross_attn(x, enc_output, enc_output, pad_mask<span class="op">=</span>src_pad_mask)</span>
<span id="cb19-25"><a href="#cb19-25"></a>        x <span class="op">=</span> <span class="va">self</span>.ln2(x <span class="op">+</span> cross_attn_output)</span>
<span id="cb19-26"><a href="#cb19-26"></a></span>
<span id="cb19-27"><a href="#cb19-27"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb19-28"><a href="#cb19-28"></a>        x <span class="op">=</span> <span class="va">self</span>.ln3(x <span class="op">+</span> ffn_output)</span>
<span id="cb19-29"><a href="#cb19-29"></a></span>
<span id="cb19-30"><a href="#cb19-30"></a>        <span class="cf">return</span> x</span>
<span id="cb19-31"><a href="#cb19-31"></a></span>
<span id="cb19-32"><a href="#cb19-32"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb19-33"><a href="#cb19-33"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb19-34"><a href="#cb19-34"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-35"><a href="#cb19-35"></a></span>
<span id="cb19-36"><a href="#cb19-36"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([DecoderBlock(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_layers)])</span>
<span id="cb19-37"><a href="#cb19-37"></a></span>
<span id="cb19-38"><a href="#cb19-38"></a>    <span class="kw">def</span> forward(</span>
<span id="cb19-39"><a href="#cb19-39"></a>        <span class="va">self</span>,</span>
<span id="cb19-40"><a href="#cb19-40"></a>        x: torch.Tensor,</span>
<span id="cb19-41"><a href="#cb19-41"></a>        enc_output: torch.Tensor,</span>
<span id="cb19-42"><a href="#cb19-42"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-43"><a href="#cb19-43"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb19-44"><a href="#cb19-44"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb19-45"><a href="#cb19-45"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb19-46"><a href="#cb19-46"></a>            x <span class="op">=</span> layer(</span>
<span id="cb19-47"><a href="#cb19-47"></a>                x,</span>
<span id="cb19-48"><a href="#cb19-48"></a>                enc_output,</span>
<span id="cb19-49"><a href="#cb19-49"></a>                src_pad_mask<span class="op">=</span>src_pad_mask,</span>
<span id="cb19-50"><a href="#cb19-50"></a>                tgt_pad_mask<span class="op">=</span>tgt_pad_mask,</span>
<span id="cb19-51"><a href="#cb19-51"></a>            )</span>
<span id="cb19-52"><a href="#cb19-52"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ä¸‹å›¾å±•ç¤ºäº†Decoder Layerçš„è¿‡ç¨‹ï¼š</p>
<div id="fig-decoder-layer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/transformer-decoding.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decoder-layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Decoding Layer Process
</figcaption>
</figure>
</div>
<p>é€šè¿‡å †å å¤šä¸ªç¼–ç å™¨å±‚å’Œè§£ç å™¨å±‚ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ„å»ºå‡ºå®Œæ•´çš„Transformeræ¨¡å‹ã€‚</p>
<div class="sourceCode" id="cb20" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb20-3"><a href="#cb20-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4"></a></span>
<span id="cb20-5"><a href="#cb20-5"></a>        <span class="va">self</span>.vocab_embedding <span class="op">=</span> WordEmbedding(config.vocab_size, config.d_model)</span>
<span id="cb20-6"><a href="#cb20-6"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> PositionalEmbedding(config)</span>
<span id="cb20-7"><a href="#cb20-7"></a></span>
<span id="cb20-8"><a href="#cb20-8"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(config)</span>
<span id="cb20-9"><a href="#cb20-9"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(config)</span>
<span id="cb20-10"><a href="#cb20-10"></a></span>
<span id="cb20-11"><a href="#cb20-11"></a>        <span class="va">self</span>.output_proj <span class="op">=</span> nn.Linear(config.d_model, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-12"><a href="#cb20-12"></a></span>
<span id="cb20-13"><a href="#cb20-13"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb20-14"><a href="#cb20-14"></a>        <span class="va">self</span>._tie_weights()</span>
<span id="cb20-15"><a href="#cb20-15"></a></span>
<span id="cb20-16"><a href="#cb20-16"></a>    <span class="kw">def</span> _tie_weights(<span class="va">self</span>):</span>
<span id="cb20-17"><a href="#cb20-17"></a>        <span class="va">self</span>.output_proj.weight <span class="op">=</span> <span class="va">self</span>.vocab_embedding.embedding.weight</span>
<span id="cb20-18"><a href="#cb20-18"></a></span>
<span id="cb20-19"><a href="#cb20-19"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb20-20"><a href="#cb20-20"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb20-21"><a href="#cb20-21"></a>            nn.init.xavier_uniform_(module.weight)</span>
<span id="cb20-22"><a href="#cb20-22"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb20-23"><a href="#cb20-23"></a>                nn.init.zeros_(module.bias)</span>
<span id="cb20-24"><a href="#cb20-24"></a></span>
<span id="cb20-25"><a href="#cb20-25"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb20-26"><a href="#cb20-26"></a>            nn.init.xavier_uniform_(module.weight)</span>
<span id="cb20-27"><a href="#cb20-27"></a></span>
<span id="cb20-28"><a href="#cb20-28"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, LayerNorm):</span>
<span id="cb20-29"><a href="#cb20-29"></a>            nn.init.ones_(module.gamma)</span>
<span id="cb20-30"><a href="#cb20-30"></a>            nn.init.zeros_(module.beta)</span>
<span id="cb20-31"><a href="#cb20-31"></a></span>
<span id="cb20-32"><a href="#cb20-32"></a>    <span class="kw">def</span> forward(</span>
<span id="cb20-33"><a href="#cb20-33"></a>        <span class="va">self</span>,</span>
<span id="cb20-34"><a href="#cb20-34"></a>        src_input: torch.Tensor,</span>
<span id="cb20-35"><a href="#cb20-35"></a>        tgt_input: torch.Tensor,</span>
<span id="cb20-36"><a href="#cb20-36"></a>        src_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb20-37"><a href="#cb20-37"></a>        tgt_pad_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb20-38"><a href="#cb20-38"></a>    ) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb20-39"><a href="#cb20-39"></a>        <span class="co"># Get Src and Tgt embeddings</span></span>
<span id="cb20-40"><a href="#cb20-40"></a>        src_embeddings <span class="op">=</span> <span class="va">self</span>.vocab_embedding(src_input) <span class="op">*</span> math.sqrt(</span>
<span id="cb20-41"><a href="#cb20-41"></a>            <span class="va">self</span>.vocab_embedding.embedding.embedding_dim</span>
<span id="cb20-42"><a href="#cb20-42"></a>        ) <span class="op">+</span> <span class="va">self</span>.positional_embedding(src_input)</span>
<span id="cb20-43"><a href="#cb20-43"></a>        tgt_embeddings <span class="op">=</span> <span class="va">self</span>.vocab_embedding(tgt_input) <span class="op">*</span> math.sqrt(</span>
<span id="cb20-44"><a href="#cb20-44"></a>            <span class="va">self</span>.vocab_embedding.embedding.embedding_dim</span>
<span id="cb20-45"><a href="#cb20-45"></a>        ) <span class="op">+</span> <span class="va">self</span>.positional_embedding(tgt_input)</span>
<span id="cb20-46"><a href="#cb20-46"></a></span>
<span id="cb20-47"><a href="#cb20-47"></a>        <span class="co"># Feed through Encoder</span></span>
<span id="cb20-48"><a href="#cb20-48"></a>        enc_output <span class="op">=</span> <span class="va">self</span>.encoder(src_embeddings, pad_mask<span class="op">=</span>src_pad_mask)</span>
<span id="cb20-49"><a href="#cb20-49"></a></span>
<span id="cb20-50"><a href="#cb20-50"></a>        <span class="co"># Feed through Decoder with Encoder output</span></span>
<span id="cb20-51"><a href="#cb20-51"></a>        dec_output <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb20-52"><a href="#cb20-52"></a>            tgt_embeddings,</span>
<span id="cb20-53"><a href="#cb20-53"></a>            enc_output,</span>
<span id="cb20-54"><a href="#cb20-54"></a>            src_pad_mask<span class="op">=</span>src_pad_mask,</span>
<span id="cb20-55"><a href="#cb20-55"></a>            tgt_pad_mask<span class="op">=</span>tgt_pad_mask,</span>
<span id="cb20-56"><a href="#cb20-56"></a>        )</span>
<span id="cb20-57"><a href="#cb20-57"></a></span>
<span id="cb20-58"><a href="#cb20-58"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_proj(dec_output)</span>
<span id="cb20-59"><a href="#cb20-59"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="others" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="others"><span class="header-section-number">2.9</span> Others</h2>
<p>å½“ç„¶ï¼Œé™¤äº†ä»¥ä¸Šçš„ä¸€ä¸ªéƒ¨åˆ†ï¼ŒTransformerä¸­è¿˜æœ‰å‡ ä¸ªå€¼å¾—ä¸€æçš„éƒ¨åˆ†,æ¯”å¦‚:</p>
<ul>
<li>Dropout Layer: ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ</li>
<li>Label Smoothing: ç”¨äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›</li>
</ul>
<section id="dropout-layer" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="dropout-layer"><span class="header-section-number">2.9.1</span> Dropout Layer</h3>
<p>Dropout <span class="citation" data-cites="Dropout2014srivastava">(<a href="#ref-Dropout2014srivastava" role="doc-biblioref"><strong>Dropout2014srivastava?</strong></a>)</span> æ˜¯ä¸€ç§å¸¸ç”¨çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨é˜²æ­¢ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡æ‹Ÿåˆã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ<u>éšæœºåœ°â€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†ç¥ç»å…ƒ</u>ï¼Œå³å°†å®ƒä»¬çš„è¾“å‡ºè®¾ç½®ä¸ºé›¶ï¼Œä»è€Œå‡å°‘ç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚çš„è¾“å…¥å‘é‡ <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{d}\)</span>ï¼ŒDropout çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-dropout"><span class="math display">\[
\begin{split}
\mathbf{r} &amp; \sim \text{Bernoulli}(p) \\
\hat{\mathbf{x}} &amp; = \mathbf{x} \odot \mathbf{r} \\
\mathbf{y} &amp; = \frac{1}{p} \hat{\mathbf{x}}
\end{split}
\tag{30}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\mathbf{r} \in \mathbb{R}^{d}\)</span> æ˜¯ä¸€ä¸ªä¸è¾“å…¥å‘é‡ <span class="math inline">\(\mathbf{x}\)</span> å½¢çŠ¶ç›¸åŒçš„äºŒè¿›åˆ¶æ©ç å‘é‡ï¼Œå…¶æ¯ä¸ªå…ƒç´ ç‹¬ç«‹åœ°æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå–å€¼ä¸º1çš„æ¦‚ç‡ä¸º <span class="math inline">\(p\)</span>ï¼ˆä¿ç•™æ¦‚ç‡ï¼‰ï¼Œå–å€¼ä¸º0çš„æ¦‚ç‡ä¸º <span class="math inline">\(1-p\)</span>ï¼ˆä¸¢å¼ƒæ¦‚ç‡ï¼‰ã€‚<span class="math inline">\(\odot\)</span> è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•æ“ä½œï¼Œ<span class="math inline">\(\hat{\mathbf{x}}\)</span> æ˜¯ç»è¿‡ Dropout å¤„ç†åçš„è¾“å…¥å‘é‡ï¼Œ<span class="math inline">\(\mathbf{y}\)</span> æ˜¯æœ€ç»ˆçš„è¾“å‡ºå‘é‡ï¼Œé€šè¿‡é™¤ä»¥ä¿ç•™æ¦‚ç‡ <span class="math inline">\(p\)</span> æ¥è¿›è¡Œç¼©æ”¾ï¼Œä»¥ä¿æŒè¾“å‡ºçš„æœŸæœ›å€¼ä¸å˜ã€‚</p>
<p>ç”¨Pythonå®ç°Dropoutå¦‚ä¸‹:</p>
<div class="sourceCode" id="cb21" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb21-3"><a href="#cb21-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-4"><a href="#cb21-4"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb21-6"><a href="#cb21-6"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb21-7"><a href="#cb21-7"></a>            mask <span class="op">=</span> (torch.rand_like(x) <span class="op">&lt;</span> <span class="va">self</span>.p).<span class="bu">float</span>()</span>
<span id="cb21-8"><a href="#cb21-8"></a>            <span class="cf">return</span> (x <span class="op">*</span> mask) <span class="op">/</span> <span class="va">self</span>.p</span>
<span id="cb21-9"><a href="#cb21-9"></a>        <span class="cf">else</span>:</span>
<span id="cb21-10"><a href="#cb21-10"></a>            <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="label-smoothing" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="label-smoothing"><span class="header-section-number">2.9.2</span> Label Smoothing</h3>
<p>Label Smoothing <span class="citation" data-cites="RethinkingInception2016szegedy">(<a href="#ref-RethinkingInception2016szegedy" role="doc-biblioref"><strong>RethinkingInception2016szegedy?</strong></a>)</span> æ˜¯ä¸€ç§ç”¨äºåˆ†ç±»ä»»åŠ¡çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯å°†ç›®æ ‡æ ‡ç­¾ä»â€œç¡¬æ ‡ç­¾â€ï¼ˆone-hot encodingï¼‰è½¬æ¢ä¸ºâ€œè½¯æ ‡ç­¾â€ï¼Œå³åœ¨ç›®æ ‡æ ‡ç­¾ä¸­å¼•å…¥ä¸€å®šçš„å¹³æ»‘åº¦ï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è¿‡äºè‡ªä¿¡åœ°é¢„æµ‹æŸä¸ªç±»åˆ«ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç±»åˆ«æ€»æ•°ä¸º <span class="math inline">\(C\)</span>ï¼ŒåŸå§‹çš„ç›®æ ‡æ ‡ç­¾ä¸º <span class="math inline">\(\mathbf{y} \in \mathbb{R}^{C}\)</span>ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ ä¸º1ï¼Œå…¶ä½™å…ƒç´ ä¸º0ï¼ˆone-hot encodingï¼‰ã€‚Label Smoothing çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:</p>
<p><span id="eq-label-smoothing"><span class="math display">\[
\mathbf{y}_{smooth} = (1 - \epsilon) \mathbf{y} + \frac{\epsilon}{C}
\tag{31}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\epsilon\)</span> æ˜¯å¹³æ»‘å‚æ•°ï¼Œæ§åˆ¶æ ‡ç­¾çš„å¹³æ»‘ç¨‹åº¦ï¼Œ<span class="math inline">\(\mathbf{y}_{smooth} \in \mathbb{R}^{C}\)</span> æ˜¯ç»è¿‡ Label Smoothing å¤„ç†åçš„ç›®æ ‡æ ‡ç­¾ã€‚</p>
</section>
</section>
</section>
<section id="experiment" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Experiment</h1>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹Transformeræ¨¡å‹åœ¨è®­ç»ƒæ—¶çš„ä¸€äº›ç»†èŠ‚è®¾ç½®ã€‚</p>
<section id="dataset" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="dataset"><span class="header-section-number">3.1</span> Dataset</h2>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨çš„<a href="https://huggingface.co/datasets/IWSLT/iwslt2017">Ted Talks</a>çš„æ•°æ®é›†ä¸­çš„è‹±æ–‡-ä¸­æ–‡Pairsï¼Œå®ƒåŒ…å«äº†å¤§é‡çš„TEDæ¼”è®²è§†é¢‘çš„å­—å¹•æ–‡æœ¬ï¼Œæ¶µç›–äº†å¤šä¸ªé¢†åŸŸå’Œä¸»é¢˜ã€‚æˆ‘ä»¬çœ‹ä¸€ä¸‹å…¶ä¸­å‡ ä¸ªä¾‹å­ï¼š</p>
<div class="sourceCode" id="cb22" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>[{<span class="st">'en'</span>: <span class="st">"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful."</span>,</span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="st">'zh'</span>: <span class="st">'éå¸¸è°¢è°¢ï¼Œå…‹é‡Œæ–¯ã€‚çš„ç¡®éå¸¸è£å¹¸ èƒ½æœ‰ç¬¬äºŒæ¬¡ç«™åœ¨è¿™ä¸ªå°ä¸Šçš„æœºä¼šï¼Œæˆ‘çœŸæ˜¯éå¸¸æ„Ÿæ¿€ã€‚'</span>},</span>
<span id="cb22-3"><a href="#cb22-3"></a> {<span class="st">'en'</span>: <span class="st">'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.'</span>,</span>
<span id="cb22-4"><a href="#cb22-4"></a>  <span class="st">'zh'</span>: <span class="st">'è¿™ä¸ªä¼šè®®çœŸæ˜¯è®©æˆ‘æ„Ÿåˆ°æƒŠå¹ä¸å·²ï¼Œæˆ‘è¿˜è¦è°¢è°¢ä½ ä»¬ç•™ä¸‹çš„ å…³äºæˆ‘ä¸Šæ¬¡æ¼”è®²çš„ç²¾å½©è¯„è®º'</span>},</span>
<span id="cb22-5"><a href="#cb22-5"></a> {<span class="st">'en'</span>: <span class="st">'And I say that sincerely, partly because  I need that.  Put yourselves in my position.'</span>,</span>
<span id="cb22-6"><a href="#cb22-6"></a>  <span class="st">'zh'</span>: <span class="st">'æˆ‘æ˜¯éå¸¸çœŸè¯šçš„ï¼Œéƒ¨åˆ†åŸå› æ˜¯å› ä¸º----æˆ‘çš„ç¡®éå¸¸éœ€è¦ï¼ ä½ è®¾èº«å¤„åœ°ä¸ºæˆ‘æƒ³æƒ³ï¼'</span>},</span>
<span id="cb22-7"><a href="#cb22-7"></a> {<span class="st">'en'</span>: <span class="st">'I flew on Air Force Two for eight years.'</span>, <span class="st">'zh'</span>: <span class="st">'æˆ‘åäº†8å¹´çš„ç©ºå†›äºŒå·ã€‚'</span>},</span>
<span id="cb22-8"><a href="#cb22-8"></a> {<span class="st">'en'</span>: <span class="st">'Now I have to take off my shoes or boots to get on an airplane!'</span>,</span>
<span id="cb22-9"><a href="#cb22-9"></a>  <span class="st">'zh'</span>: <span class="st">'ä¸è¿‡ç°åœ¨ä¸Šé£æœºå‰æˆ‘åˆ™è¦è„±æ‰æˆ‘çš„é‹å­'</span>}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>å…¶ä¸­ <code>train_dataset</code> æœ‰231,266æ¡æ•°æ®ï¼Œ<code>test_dataset</code> æœ‰ 8,549 æ¡æ•°æ®ã€‚</p>
<section id="tokenizer-vocabulary" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="tokenizer-vocabulary"><span class="header-section-number">3.1.1</span> Tokenizer &amp; Vocabulary</h3>
<p>åœ¨è®ºæ–‡ä¸­ï¼ŒTarget å’Œ Source ä½¿ç”¨åŒä¸€ä¸ªTokenizerï¼Œå¹¶ä¸”å…±äº«åŒä¸€ä¸ªè¯è¡¨ï¼ˆVocabularyï¼‰ã€‚å¹¶ä¸”ä½¿ç”¨ BPE (Byte Pair Encoding) <span class="citation" data-cites="NeuralMachineTranslation2016sennrich">(<a href="#ref-NeuralMachineTranslation2016sennrich" role="doc-biblioref">Sennrich, Haddow, and Birch 2016</a>)</span> æ¥è¿›è¡Œåˆ†è¯å’Œæ„å»ºè¯è¡¨ã€‚</p>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨<code>Hugging Face</code>çš„<code>transformers</code>åº“ä¸­çš„<code>Tokenizer</code>æ¥è¿›è¡Œåˆ†è¯å’Œæ„å»ºè¯è¡¨ï¼Œè¯è¡¨å¤§å°è®¾ç½®ä¸º10,000ã€‚</p>
<div class="sourceCode" id="cb23" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">def</span> load_or_train_joint_bpe_tokenizer(</span>
<span id="cb23-2"><a href="#cb23-2"></a>    vocab_size: <span class="bu">int</span>,</span>
<span id="cb23-3"><a href="#cb23-3"></a>    save_prefix: <span class="bu">str</span>,</span>
<span id="cb23-4"><a href="#cb23-4"></a>    save_name: <span class="bu">str</span> <span class="op">=</span> <span class="st">"bpe_joint.json"</span>,</span>
<span id="cb23-5"><a href="#cb23-5"></a>    src_corpus_file: <span class="bu">str</span> <span class="op">=</span> <span class="st">"train_src.txt"</span>,</span>
<span id="cb23-6"><a href="#cb23-6"></a>    tgt_corpus_file: <span class="bu">str</span> <span class="op">=</span> <span class="st">"train_tgt.txt"</span>,</span>
<span id="cb23-7"><a href="#cb23-7"></a>):</span>
<span id="cb23-8"><a href="#cb23-8"></a>    save_path <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>save_prefix<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>save_name<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb23-9"><a href="#cb23-9"></a></span>
<span id="cb23-10"><a href="#cb23-10"></a>    <span class="cf">if</span> os.path.exists(save_path):</span>
<span id="cb23-11"><a href="#cb23-11"></a>        <span class="bu">print</span>(<span class="ss">f"Loading tokenizer from </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-12"><a href="#cb23-12"></a>        <span class="cf">return</span> Tokenizer.from_file(save_path)</span>
<span id="cb23-13"><a href="#cb23-13"></a></span>
<span id="cb23-14"><a href="#cb23-14"></a>    <span class="co"># Train ONE tokenizer on BOTH corpora (concatenated dataset)</span></span>
<span id="cb23-15"><a href="#cb23-15"></a>    tokenizer <span class="op">=</span> Tokenizer(models.BPE(unk_token<span class="op">=</span><span class="st">"&lt;unk&gt;"</span>))</span>
<span id="cb23-16"><a href="#cb23-16"></a>    tokenizer.normalizer <span class="op">=</span> NFKC()</span>
<span id="cb23-17"><a href="#cb23-17"></a>    tokenizer.pre_tokenizer <span class="op">=</span> ByteLevel(add_prefix_space<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-18"><a href="#cb23-18"></a>    tokenizer.decoder <span class="op">=</span> ByteLevelDecoder()</span>
<span id="cb23-19"><a href="#cb23-19"></a></span>
<span id="cb23-20"><a href="#cb23-20"></a>    trainer <span class="op">=</span> trainers.BpeTrainer(</span>
<span id="cb23-21"><a href="#cb23-21"></a>        vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb23-22"><a href="#cb23-22"></a>        special_tokens<span class="op">=</span>[<span class="st">"&lt;pad&gt;"</span>, <span class="st">"&lt;unk&gt;"</span>, <span class="st">"&lt;s&gt;"</span>, <span class="st">"&lt;/s&gt;"</span>],</span>
<span id="cb23-23"><a href="#cb23-23"></a>    )</span>
<span id="cb23-24"><a href="#cb23-24"></a></span>
<span id="cb23-25"><a href="#cb23-25"></a>    tokenizer.train([src_corpus_file, tgt_corpus_file], trainer)</span>
<span id="cb23-26"><a href="#cb23-26"></a>    tokenizer.save(save_path)</span>
<span id="cb23-27"><a href="#cb23-27"></a>    <span class="bu">print</span>(<span class="ss">f"Saved tokenizer to </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-28"><a href="#cb23-28"></a></span>
<span id="cb23-29"><a href="#cb23-29"></a>    <span class="cf">return</span> tokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>åœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬æå‰å¤„ç†å¥½æ•°æ®é›†ï¼Œä¿å­˜ä¸º<code>PT</code>æ ¼å¼ï¼Œæ–¹ä¾¿åç»­çš„è®­ç»ƒä½¿ç”¨ã€‚</p>
<div class="sourceCode" id="cb24" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">def</span> encode_file_to_pt(</span>
<span id="cb24-2"><a href="#cb24-2"></a>    tokenizer,</span>
<span id="cb24-3"><a href="#cb24-3"></a>    in_path: <span class="bu">str</span>,</span>
<span id="cb24-4"><a href="#cb24-4"></a>    out_path: <span class="bu">str</span>,</span>
<span id="cb24-5"><a href="#cb24-5"></a>    add_special_tokens: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb24-6"><a href="#cb24-6"></a>    bos_token: <span class="bu">str</span> <span class="op">=</span> <span class="st">"&lt;s&gt;"</span>,</span>
<span id="cb24-7"><a href="#cb24-7"></a>    eos_token: <span class="bu">str</span> <span class="op">=</span> <span class="st">"&lt;/s&gt;"</span>,</span>
<span id="cb24-8"><a href="#cb24-8"></a>    max_lines: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb24-9"><a href="#cb24-9"></a>):</span>
<span id="cb24-10"><a href="#cb24-10"></a>    <span class="cf">if</span> os.path.exists(out_path):</span>
<span id="cb24-11"><a href="#cb24-11"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>out_path<span class="sc">}</span><span class="ss"> already exists"</span>)</span>
<span id="cb24-12"><a href="#cb24-12"></a>        <span class="cf">return</span></span>
<span id="cb24-13"><a href="#cb24-13"></a></span>
<span id="cb24-14"><a href="#cb24-14"></a>    bos_id <span class="op">=</span> tokenizer.token_to_id(bos_token)</span>
<span id="cb24-15"><a href="#cb24-15"></a>    eos_id <span class="op">=</span> tokenizer.token_to_id(eos_token)</span>
<span id="cb24-16"><a href="#cb24-16"></a></span>
<span id="cb24-17"><a href="#cb24-17"></a>    all_ids <span class="op">=</span> []</span>
<span id="cb24-18"><a href="#cb24-18"></a>    <span class="cf">with</span> <span class="bu">open</span>(in_path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> f:</span>
<span id="cb24-19"><a href="#cb24-19"></a>        <span class="cf">for</span> i, line <span class="kw">in</span> <span class="bu">enumerate</span>(f):</span>
<span id="cb24-20"><a href="#cb24-20"></a>            <span class="cf">if</span> max_lines <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> i <span class="op">&gt;=</span> max_lines:</span>
<span id="cb24-21"><a href="#cb24-21"></a>                <span class="cf">break</span></span>
<span id="cb24-22"><a href="#cb24-22"></a>            text <span class="op">=</span> line.rstrip(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb24-23"><a href="#cb24-23"></a>            enc <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb24-24"><a href="#cb24-24"></a>            ids <span class="op">=</span> enc.ids</span>
<span id="cb24-25"><a href="#cb24-25"></a></span>
<span id="cb24-26"><a href="#cb24-26"></a>            <span class="cf">if</span> add_special_tokens:</span>
<span id="cb24-27"><a href="#cb24-27"></a>                ids <span class="op">=</span> [bos_id] <span class="op">+</span> ids <span class="op">+</span> [eos_id]</span>
<span id="cb24-28"><a href="#cb24-28"></a></span>
<span id="cb24-29"><a href="#cb24-29"></a>            all_ids.append(torch.tensor(ids, dtype<span class="op">=</span>torch.int32))</span>
<span id="cb24-30"><a href="#cb24-30"></a></span>
<span id="cb24-31"><a href="#cb24-31"></a>    os.makedirs(os.path.dirname(out_path) <span class="kw">or</span> <span class="st">"."</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-32"><a href="#cb24-32"></a>    torch.save(all_ids, out_path)</span>
<span id="cb24-33"><a href="#cb24-33"></a>    <span class="bu">print</span>(<span class="ss">f"Saved </span><span class="sc">{</span><span class="bu">len</span>(all_ids)<span class="sc">}</span><span class="ss"> sequences to </span><span class="sc">{</span>out_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-34"><a href="#cb24-34"></a></span>
<span id="cb24-35"><a href="#cb24-35"></a></span>
<span id="cb24-36"><a href="#cb24-36"></a>encode_file_to_pt(tokenizer, <span class="st">"train_src.txt"</span>, <span class="st">"train_src_ids.pt"</span>)</span>
<span id="cb24-37"><a href="#cb24-37"></a>encode_file_to_pt(tokenizer, <span class="st">"train_tgt.txt"</span>, <span class="st">"train_tgt_ids.pt"</span>)</span>
<span id="cb24-38"><a href="#cb24-38"></a>encode_file_to_pt(tokenizer, <span class="st">"test_src.txt"</span>, <span class="st">"test_src_ids.pt"</span>)</span>
<span id="cb24-39"><a href="#cb24-39"></a>encode_file_to_pt(tokenizer, <span class="st">"test_tgt.txt"</span>, <span class="st">"test_tgt_ids.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="padding-samples" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="padding-samples"><span class="header-section-number">3.1.2</span> Padding Samples</h3>
<p>è‡³æ­¤ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å°±å‡†å¤‡å¥½äº†ã€‚åœ¨åç»­çš„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½è¿™äº›é¢„å¤„ç†å¥½çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œéœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬åœ¨åŠ è½½æ•°æ®é›†æ—¶ï¼Œéœ€è¦å¯¹è¾“å…¥åºåˆ—è¿›è¡ŒPaddingï¼Œä»¥ç¡®ä¿æ¯ä¸ªBatchä¸­çš„åºåˆ—é•¿åº¦ä¸€è‡´ã€‚åœ¨è®ºæ–‡ä¸­æœ‰ä¸€ä¸ªæ–¹å¼ï¼Œå°±æ˜¯å°†é•¿åº¦å·®ä¸å¤šçš„åºåˆ—æ”¾åœ¨åŒä¸€ä¸ªBatchä¸­ï¼Œè¿™æ ·å¯ä»¥å‡å°‘Paddingçš„æ•°é‡ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª <code>Sampler</code> æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚</p>
<div class="sourceCode" id="cb25" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="kw">class</span> BucketBatchSampler(Sampler[<span class="bu">list</span>[<span class="bu">int</span>]]):</span>
<span id="cb25-2"><a href="#cb25-2"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="co">    Yields batches of indices where sequences have similar lengths.</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co">    length_fn: function(idx) -&gt; int</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="co">    """</span></span>
<span id="cb25-6"><a href="#cb25-6"></a></span>
<span id="cb25-7"><a href="#cb25-7"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb25-8"><a href="#cb25-8"></a>        <span class="va">self</span>,</span>
<span id="cb25-9"><a href="#cb25-9"></a>        lengths,</span>
<span id="cb25-10"><a href="#cb25-10"></a>        batch_size: <span class="bu">int</span>,</span>
<span id="cb25-11"><a href="#cb25-11"></a>        bucket_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span>,</span>
<span id="cb25-12"><a href="#cb25-12"></a>        shuffle: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb25-13"><a href="#cb25-13"></a>        drop_last: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb25-14"><a href="#cb25-14"></a>        seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb25-15"><a href="#cb25-15"></a>    ):</span>
<span id="cb25-16"><a href="#cb25-16"></a>        <span class="va">self</span>.lengths <span class="op">=</span> <span class="bu">list</span>(lengths)</span>
<span id="cb25-17"><a href="#cb25-17"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb25-18"><a href="#cb25-18"></a>        <span class="va">self</span>.bucket_size <span class="op">=</span> bucket_size</span>
<span id="cb25-19"><a href="#cb25-19"></a>        <span class="va">self</span>.shuffle <span class="op">=</span> shuffle</span>
<span id="cb25-20"><a href="#cb25-20"></a>        <span class="va">self</span>.drop_last <span class="op">=</span> drop_last</span>
<span id="cb25-21"><a href="#cb25-21"></a>        <span class="va">self</span>.seed <span class="op">=</span> seed</span>
<span id="cb25-22"><a href="#cb25-22"></a></span>
<span id="cb25-23"><a href="#cb25-23"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb25-24"><a href="#cb25-24"></a>        rng <span class="op">=</span> random.Random(<span class="va">self</span>.seed)</span>
<span id="cb25-25"><a href="#cb25-25"></a></span>
<span id="cb25-26"><a href="#cb25-26"></a>        indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.lengths)))</span>
<span id="cb25-27"><a href="#cb25-27"></a>        <span class="cf">if</span> <span class="va">self</span>.shuffle:</span>
<span id="cb25-28"><a href="#cb25-28"></a>            rng.shuffle(indices)</span>
<span id="cb25-29"><a href="#cb25-29"></a></span>
<span id="cb25-30"><a href="#cb25-30"></a>        <span class="co"># chunk into buckets</span></span>
<span id="cb25-31"><a href="#cb25-31"></a>        <span class="cf">for</span> b_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(indices), <span class="va">self</span>.bucket_size):</span>
<span id="cb25-32"><a href="#cb25-32"></a>            bucket <span class="op">=</span> indices[b_start : b_start <span class="op">+</span> <span class="va">self</span>.bucket_size]</span>
<span id="cb25-33"><a href="#cb25-33"></a>            <span class="co"># sort inside bucket by length</span></span>
<span id="cb25-34"><a href="#cb25-34"></a>            bucket.sort(key<span class="op">=</span><span class="kw">lambda</span> i: <span class="va">self</span>.lengths[i])</span>
<span id="cb25-35"><a href="#cb25-35"></a></span>
<span id="cb25-36"><a href="#cb25-36"></a>            <span class="co"># make batches</span></span>
<span id="cb25-37"><a href="#cb25-37"></a>            batches <span class="op">=</span> [bucket[i : i <span class="op">+</span> <span class="va">self</span>.batch_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(bucket), <span class="va">self</span>.batch_size)]</span>
<span id="cb25-38"><a href="#cb25-38"></a>            <span class="cf">if</span> <span class="va">self</span>.drop_last <span class="kw">and</span> <span class="bu">len</span>(batches) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(batches[<span class="op">-</span><span class="dv">1</span>]) <span class="op">&lt;</span> <span class="va">self</span>.batch_size:</span>
<span id="cb25-39"><a href="#cb25-39"></a>                batches <span class="op">=</span> batches[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-40"><a href="#cb25-40"></a></span>
<span id="cb25-41"><a href="#cb25-41"></a>            <span class="cf">if</span> <span class="va">self</span>.shuffle:</span>
<span id="cb25-42"><a href="#cb25-42"></a>                rng.shuffle(batches)</span>
<span id="cb25-43"><a href="#cb25-43"></a></span>
<span id="cb25-44"><a href="#cb25-44"></a>            <span class="cf">for</span> batch <span class="kw">in</span> batches:</span>
<span id="cb25-45"><a href="#cb25-45"></a>                <span class="cf">yield</span> batch</span>
<span id="cb25-46"><a href="#cb25-46"></a></span>
<span id="cb25-47"><a href="#cb25-47"></a>        <span class="co"># update seed so next epoch reshuffles differently</span></span>
<span id="cb25-48"><a href="#cb25-48"></a>        <span class="va">self</span>.seed <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb25-49"><a href="#cb25-49"></a></span>
<span id="cb25-50"><a href="#cb25-50"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb25-51"><a href="#cb25-51"></a>        n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.lengths)</span>
<span id="cb25-52"><a href="#cb25-52"></a>        <span class="cf">if</span> <span class="va">self</span>.drop_last:</span>
<span id="cb25-53"><a href="#cb25-53"></a>            <span class="cf">return</span> n <span class="op">//</span> <span class="va">self</span>.batch_size</span>
<span id="cb25-54"><a href="#cb25-54"></a>        <span class="cf">return</span> math.ceil(n <span class="op">/</span> <span class="va">self</span>.batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>è¿™ä¸ª <code>BucketBatchSampler</code> ä¼šæ ¹æ®åºåˆ—çš„é•¿åº¦å°†å®ƒä»¬åˆ†é…åˆ°ä¸åŒçš„Bucketä¸­ï¼Œç„¶ååœ¨æ¯ä¸ªBucketå†…æŒ‰é•¿åº¦æ’åºï¼Œæœ€åç”ŸæˆBatchã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªBatchä¸­çš„åºåˆ—é•¿åº¦ç›¸ä¼¼ï¼Œä»è€Œå‡å°‘Paddingçš„æ•°é‡ã€‚</p>
<p>æœ‰äº†ä¸€ä¸ªBatchä¹‹åï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ª <code>collate_fn</code> æ¥å¯¹Batchä¸­çš„åºåˆ—è¿›è¡ŒPadding:</p>
<div class="sourceCode" id="cb26" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="kw">def</span> translation_collate(batch, pad_id: <span class="bu">int</span>, sos_id: <span class="bu">int</span>, eos_id: <span class="bu">int</span>, max_len: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb26-3"><a href="#cb26-3"></a>    src_list <span class="op">=</span> [item[<span class="st">"src_ids"</span>] <span class="cf">for</span> item <span class="kw">in</span> batch]</span>
<span id="cb26-4"><a href="#cb26-4"></a>    tgt_list <span class="op">=</span> [item[<span class="st">"tgt_ids"</span>] <span class="cf">for</span> item <span class="kw">in</span> batch]</span>
<span id="cb26-5"><a href="#cb26-5"></a></span>
<span id="cb26-6"><a href="#cb26-6"></a>    <span class="cf">if</span> max_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb26-7"><a href="#cb26-7"></a>        src_list <span class="op">=</span> [x[:max_len] <span class="cf">for</span> x <span class="kw">in</span> src_list]</span>
<span id="cb26-8"><a href="#cb26-8"></a>        tgt_list <span class="op">=</span> [x[:max_len] <span class="cf">for</span> x <span class="kw">in</span> tgt_list]  <span class="co"># leave room for EOS</span></span>
<span id="cb26-9"><a href="#cb26-9"></a></span>
<span id="cb26-10"><a href="#cb26-10"></a>    decoder_list <span class="op">=</span> [t[:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> tgt_list]</span>
<span id="cb26-11"><a href="#cb26-11"></a>    labels_list <span class="op">=</span> [t[<span class="dv">1</span>:] <span class="cf">for</span> t <span class="kw">in</span> tgt_list]</span>
<span id="cb26-12"><a href="#cb26-12"></a></span>
<span id="cb26-13"><a href="#cb26-13"></a>    src_max <span class="op">=</span> <span class="bu">max</span>(x.numel() <span class="cf">for</span> x <span class="kw">in</span> src_list)</span>
<span id="cb26-14"><a href="#cb26-14"></a>    dec_max <span class="op">=</span> <span class="bu">max</span>(x.numel() <span class="cf">for</span> x <span class="kw">in</span> decoder_list)</span>
<span id="cb26-15"><a href="#cb26-15"></a></span>
<span id="cb26-16"><a href="#cb26-16"></a>    <span class="kw">def</span> pad_1d(x: torch.Tensor, L: <span class="bu">int</span>):</span>
<span id="cb26-17"><a href="#cb26-17"></a>        x <span class="op">=</span> x.to(torch.<span class="bu">long</span>)</span>
<span id="cb26-18"><a href="#cb26-18"></a>        <span class="cf">if</span> x.numel() <span class="op">==</span> L:</span>
<span id="cb26-19"><a href="#cb26-19"></a>            <span class="cf">return</span> x</span>
<span id="cb26-20"><a href="#cb26-20"></a>        <span class="cf">return</span> torch.cat([x, x.new_full((L <span class="op">-</span> x.numel(),), pad_id, dtype<span class="op">=</span>torch.<span class="bu">long</span>)])</span>
<span id="cb26-21"><a href="#cb26-21"></a></span>
<span id="cb26-22"><a href="#cb26-22"></a>    encoder_input_ids <span class="op">=</span> torch.stack([pad_1d(x, src_max) <span class="cf">for</span> x <span class="kw">in</span> src_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-23"><a href="#cb26-23"></a>    decoder_input_ids <span class="op">=</span> torch.stack([pad_1d(x, dec_max) <span class="cf">for</span> x <span class="kw">in</span> decoder_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-24"><a href="#cb26-24"></a>    labels <span class="op">=</span> torch.stack([pad_1d(x, dec_max) <span class="cf">for</span> x <span class="kw">in</span> labels_list], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-25"><a href="#cb26-25"></a></span>
<span id="cb26-26"><a href="#cb26-26"></a>    <span class="cf">return</span> {</span>
<span id="cb26-27"><a href="#cb26-27"></a>        <span class="st">"encoder_input_ids"</span>: encoder_input_ids,</span>
<span id="cb26-28"><a href="#cb26-28"></a>        <span class="st">"decoder_input_ids"</span>: decoder_input_ids,</span>
<span id="cb26-29"><a href="#cb26-29"></a>        <span class="st">"labels"</span>: labels,</span>
<span id="cb26-30"><a href="#cb26-30"></a>        <span class="st">"encoder_mask"</span>: create_padding_mask(encoder_input_ids, pad_id),</span>
<span id="cb26-31"><a href="#cb26-31"></a>        <span class="st">"decoder_mask"</span>: create_padding_mask(decoder_input_ids, pad_id),</span>
<span id="cb26-32"><a href="#cb26-32"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>åœ¨è¿™ä¸ª <code>collate_fn</code> ä¸­ï¼Œæˆ‘ä»¬è¿˜åŒæ—¶æ„é€ äº†Labelsï¼Œ Labelsæ˜¯Decoderè¾“å…¥åºåˆ—å³ç§»ä¸€ä½å¾—åˆ°çš„<code>[t[1:] for t in tgt_list]</code>ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶ï¼Œèƒ½å¤Ÿæ­£ç¡®åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚</p>
<p>è‡³æ­¤ï¼Œæˆ‘ä»¬çš„æ•°æ®é¢„å¤„ç†å’ŒBatchå‡†å¤‡å·¥ä½œå°±å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ã€‚</p>
</section>
</section>
<section id="weight-initialization" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="weight-initialization"><span class="header-section-number">3.2</span> Weight Initialization</h2>
<p>åœ¨è®ºæ–‡ä¸­ï¼Œæ²¡æœ‰æåˆ°å¦‚ä½•Initializeçš„ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ç”¨Xavier initialization, æ¥åˆå§‹åŒ–Transformeræ¨¡å‹çš„æƒé‡å‚æ•°ã€‚Xavieråˆå§‹åŒ–æ—¨åœ¨ä¿æŒæ¯å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºçš„æ–¹å·®ç›¸ç­‰ï¼Œä»è€Œé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚ï¼Œå…¶è¾“å…¥ç»´åº¦ä¸º <span class="math inline">\(n_{in}\)</span>ï¼Œè¾“å‡ºç»´åº¦ä¸º <span class="math inline">\(n_{out}\)</span>ï¼Œé‚£ä¹ˆXavieråˆå§‹åŒ–çš„æƒé‡çŸ©é˜µ <span class="math inline">\(W\)</span> çš„æ¯ä¸ªå…ƒç´ å¯ä»¥ä»ä»¥ä¸‹å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·:</p>
<p><span id="eq-xavier-initialization"><span class="math display">\[
W_{i,j} \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
\tag{32}\]</span></span></p>
<div class="sourceCode" id="cb27" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb27-2"><a href="#cb27-2"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb27-3"><a href="#cb27-3"></a>        nn.init.xavier_uniform_(module.weight)</span>
<span id="cb27-4"><a href="#cb27-4"></a>        <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb27-5"><a href="#cb27-5"></a>            nn.init.zeros_(module.bias)</span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb27-8"><a href="#cb27-8"></a>        nn.init.xavier_uniform_(module.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>å…·ä½“çš„åŸå› ä¸ºä»€ä¹ˆXavier initializationæœ‰æ•ˆï¼Œåœ¨è¿™é‡Œå°±ä¸å¤šèµ˜è¿°äº†ï¼Œä¹‹åå¯èƒ½ä¼šæœ‰ä¸“é—¨çš„æ–‡ç« æ¥ä»‹ç»è¿™ä¸ªå†…å®¹ã€‚</p>
</section>
<section id="optimizer" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="optimizer"><span class="header-section-number">3.3</span> Optimizer</h2>
<p>Transformerçš„è®ºæ–‡ä¸­ï¼Œç”¨çš„æ˜¯Adam Optimizer <span class="citation" data-cites="AdamMethodStochastic2017kingma">(<a href="#ref-AdamMethodStochastic2017kingma" role="doc-biblioref">Kingma and Ba 2017</a>)</span>, å®ƒæ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•ï¼Œç»“åˆäº†åŠ¨é‡æ³•å’ŒRMSPropçš„ä¼˜ç‚¹ã€‚Adamé€šè¿‡è®¡ç®—æ¢¯åº¦çš„ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„å¹³æ–¹çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼‰æ¥è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚Adamçš„æ›´æ–°è§„åˆ™å¦‚ä¸‹:</p>
<div id="algo-adam-wd" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-comment-delimiter="#" data-pseudocode-number="1" data-line-number-punc=":" data-no-end="false" data-indent-size="1.2em" data-line-number="true">
<div class="pseudocode">
\begin{algorithm} \caption{Adam with L2 Regularization(Weight Decay)} \begin{algorithmic} \Require Parameters $\theta_0$ \Require Learning rate $\alpha$, betas $(\beta_1,\beta_2)$ \Require Weight decay coefficient $\lambda$ \State Initialize $m_0 \gets 0$, $v_0 \gets 0$, $t \gets 0$ \While{not converged} \State $t \gets t + 1$ \State Compute gradient $g_t \gets \nabla_{\theta}\mathcal{L}(\theta_{t-1})$ \State Apply L2 regularization: $g_t^{\text{wd}} \gets g_t + \lambda \theta_{t-1}$ \State First moment: $m_t \gets \beta_1 m_{t-1} + (1-\beta_1) g_t^{\text{wd}}$ \State Second moment: $v_t \gets \beta_2 v_{t-1} + (1-\beta_2)\left(g_t^{\text{wd}}\right)^2$ \State Bias-corrected moments: $\hat m_t \gets \dfrac{m_t}{1-\beta_1^t}$, $\hat v_t \gets \dfrac{v_t}{1-\beta_2^t}$ \State Parameter update: $\theta_t \gets \theta_{t-1} - \alpha \dfrac{\hat m_t}{\sqrt{\hat v_t} + \epsilon}$ \EndWhile \end{algorithmic} \end{algorithm}
</div>
</div>
<p>ç”¨Pythonå®ç°Adam Optimizerå¦‚ä¸‹:</p>
<div class="sourceCode" id="cb28" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="kw">class</span> Adam:</span>
<span id="cb28-2"><a href="#cb28-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.98</span>), weight_decay<span class="op">=</span><span class="fl">0.01</span>, eps<span class="op">=</span><span class="fl">1e-9</span>):</span>
<span id="cb28-3"><a href="#cb28-3"></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb28-4"><a href="#cb28-4"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb28-5"><a href="#cb28-5"></a>        <span class="va">self</span>.betas <span class="op">=</span> betas</span>
<span id="cb28-6"><a href="#cb28-6"></a>        <span class="va">self</span>.weight_decay <span class="op">=</span> weight_decay</span>
<span id="cb28-7"><a href="#cb28-7"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb28-8"><a href="#cb28-8"></a></span>
<span id="cb28-9"><a href="#cb28-9"></a>        <span class="va">self</span>.state <span class="op">=</span> {}</span>
<span id="cb28-10"><a href="#cb28-10"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-11"><a href="#cb28-11"></a>            <span class="va">self</span>.state[p] <span class="op">=</span> {</span>
<span id="cb28-12"><a href="#cb28-12"></a>                <span class="st">"step"</span>: <span class="dv">0</span>,</span>
<span id="cb28-13"><a href="#cb28-13"></a>                <span class="st">"m"</span>: torch.zeros_like(p.data),</span>
<span id="cb28-14"><a href="#cb28-14"></a>                <span class="st">"v"</span>: torch.zeros_like(p.data),</span>
<span id="cb28-15"><a href="#cb28-15"></a>            }</span>
<span id="cb28-16"><a href="#cb28-16"></a></span>
<span id="cb28-17"><a href="#cb28-17"></a>    <span class="kw">def</span> update_lr(<span class="va">self</span>, new_lr):</span>
<span id="cb28-18"><a href="#cb28-18"></a>        <span class="va">self</span>.lr <span class="op">=</span> new_lr</span>
<span id="cb28-19"><a href="#cb28-19"></a></span>
<span id="cb28-20"><a href="#cb28-20"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb28-21"><a href="#cb28-21"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-22"><a href="#cb28-22"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb28-23"><a href="#cb28-23"></a>                <span class="cf">continue</span></span>
<span id="cb28-24"><a href="#cb28-24"></a></span>
<span id="cb28-25"><a href="#cb28-25"></a>            grad <span class="op">=</span> p.grad.data</span>
<span id="cb28-26"><a href="#cb28-26"></a>            state <span class="op">=</span> <span class="va">self</span>.state[p]</span>
<span id="cb28-27"><a href="#cb28-27"></a></span>
<span id="cb28-28"><a href="#cb28-28"></a>            state[<span class="st">"step"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb28-29"><a href="#cb28-29"></a>            beta1, beta2 <span class="op">=</span> <span class="va">self</span>.betas</span>
<span id="cb28-30"><a href="#cb28-30"></a></span>
<span id="cb28-31"><a href="#cb28-31"></a>            <span class="co"># Update biased first moment estimate</span></span>
<span id="cb28-32"><a href="#cb28-32"></a>            state[<span class="st">"m"</span>] <span class="op">=</span> beta1 <span class="op">*</span> state[<span class="st">"m"</span>] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> grad</span>
<span id="cb28-33"><a href="#cb28-33"></a>            <span class="co"># Update biased second raw moment estimate</span></span>
<span id="cb28-34"><a href="#cb28-34"></a>            state[<span class="st">"v"</span>] <span class="op">=</span> beta2 <span class="op">*</span> state[<span class="st">"v"</span>] <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> (grad <span class="op">*</span> grad)</span>
<span id="cb28-35"><a href="#cb28-35"></a></span>
<span id="cb28-36"><a href="#cb28-36"></a>            <span class="co"># Compute bias-corrected first moment estimate</span></span>
<span id="cb28-37"><a href="#cb28-37"></a>            m_hat <span class="op">=</span> state[<span class="st">"m"</span>] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta1 <span class="op">**</span> state[<span class="st">"step"</span>])</span>
<span id="cb28-38"><a href="#cb28-38"></a>            <span class="co"># Compute bias-corrected second raw moment estimate</span></span>
<span id="cb28-39"><a href="#cb28-39"></a>            v_hat <span class="op">=</span> state[<span class="st">"v"</span>] <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta2 <span class="op">**</span> state[<span class="st">"step"</span>])</span>
<span id="cb28-40"><a href="#cb28-40"></a></span>
<span id="cb28-41"><a href="#cb28-41"></a>            <span class="co"># Update parameters</span></span>
<span id="cb28-42"><a href="#cb28-42"></a>            p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> m_hat <span class="op">/</span> (torch.sqrt(v_hat) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb28-43"><a href="#cb28-43"></a></span>
<span id="cb28-44"><a href="#cb28-44"></a>            <span class="co"># Apply weight decay</span></span>
<span id="cb28-45"><a href="#cb28-45"></a>            <span class="cf">if</span> <span class="va">self</span>.weight_decay <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb28-46"><a href="#cb28-46"></a>                p.data <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> <span class="va">self</span>.weight_decay <span class="op">*</span> p.data</span>
<span id="cb28-47"><a href="#cb28-47"></a></span>
<span id="cb28-48"><a href="#cb28-48"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb28-49"><a href="#cb28-49"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb28-50"><a href="#cb28-50"></a>            <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb28-51"><a href="#cb28-51"></a>                p.grad.detach_()</span>
<span id="cb28-52"><a href="#cb28-52"></a>                p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: Adam Optimizer</p>
</div>
<div class="note-container foldable-content">
<p>å¯¹äºä¸äº†çš„Adamçš„åŒå­¦ï¼Œä¹Ÿä¸ç”¨å¤ªæ‹…å¿ƒï¼Œä¹‹åæˆ‘ä»¬ä¼šæœ‰ä¸€ç³»åˆ—çš„æ–‡ç« ï¼Œä¸“é—¨ä»‹ç»è¿™äº›ä¼˜åŒ–å™¨çš„ï¼ŒåŒ…æ‹¬Adam<span class="citation" data-cites="AdamMethodStochastic2017kingma">(<a href="#ref-AdamMethodStochastic2017kingma" role="doc-biblioref">Kingma and Ba 2017</a>)</span>ï¼ŒAdamW<span class="citation" data-cites="DecoupledWeightDecay2019loshchilov">(<a href="#ref-DecoupledWeightDecay2019loshchilov" role="doc-biblioref">Loshchilov and Hutter 2019</a>)</span>ï¼Œä»¥åŠæœ€è¿‘æ¯”è¾ƒç«çš„Muon<span class="citation" data-cites="jordan2024muon">(<a href="#ref-jordan2024muon" role="doc-biblioref">Jordan et al. 2024</a>)</span>ç­‰ã€‚</p>
</div>
</div>
<section id="learning-rate-scheduler" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="learning-rate-scheduler"><span class="header-section-number">3.3.1</span> Learning Rate Scheduler</h3>
<p>åœ¨Transformerçš„è®ºæ–‡ä¸­ï¼Œä½œè€…åˆ©ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLearning Rate Schedulerï¼‰ï¼Œå®ƒåœ¨è®­ç»ƒçš„åˆå§‹é˜¶æ®µé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œç„¶ååœ¨è¾¾åˆ°é¢„è®¾çš„æ­¥æ•°åé€æ¸å‡å°å­¦ä¹ ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œå­¦ä¹ ç‡çš„è®¡ç®—å…¬å¼å¦‚ä¸‹:</p>
<p><span id="eq-transformer-learning-rate"><span class="math display">\[
\text{lrate} = d_{model}^{-0.5} \cdot \min\left(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5}\right)
\tag{33}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(d_{model}\)</span> æ˜¯æ¨¡å‹çš„éšè—å±‚ç»´åº¦ï¼Œ<span class="math inline">\(step\_num\)</span> æ˜¯å½“å‰çš„è®­ç»ƒæ­¥æ•°ï¼Œ<span class="math inline">\(warmup\_steps\)</span> æ˜¯é¢„è®¾çš„é¢„çƒ­æ­¥æ•°ã€‚åœ¨è®­ç»ƒçš„å‰ <span class="math inline">\(warmup\_steps\)</span> æ­¥ä¸­ï¼Œå­¦ä¹ ç‡çº¿æ€§å¢åŠ ï¼›åœ¨ä¹‹åçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ä¹ ç‡æŒ‰ç…§ <span class="math inline">\(step\_num^{-0.5}\)</span> çš„æ¯”ä¾‹é€æ¸å‡å°ã€‚</p>
<div id="fig-learning-rate-scheduler" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-learning-rate-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/learning-rate-scheduler.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-learning-rate-scheduler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Visualization of Learning Rate Scheduler
</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb29" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">def</span> get_lr(cur_step, warmup_steps, d_model):</span>
<span id="cb29-2"><a href="#cb29-2"></a>    lrate <span class="op">=</span> (d_model<span class="op">**-</span><span class="fl">0.5</span>) <span class="op">*</span> <span class="bu">min</span>(cur_step<span class="op">**-</span><span class="fl">0.5</span>, cur_step <span class="op">*</span> (warmup_steps<span class="op">**-</span><span class="fl">1.5</span>))</span>
<span id="cb29-3"><a href="#cb29-3"></a>    <span class="cf">return</span> lrate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="loss-function" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">3.4</span> Loss Function</h2>
<p>åœ¨Transformeræ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCross Entropy Lossï¼‰ä½œä¸ºä¸»è¦çš„æŸå¤±å‡½æ•°ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°ç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªåŒ…å« <span class="math inline">\(N\)</span> ä¸ªæ ·æœ¬çš„è®­ç»ƒé›†ï¼Œæ¯ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ä¸º <span class="math inline">\(y_i\)</span>ï¼Œæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸º <span class="math inline">\(\hat{y}_i\)</span>ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°çš„è®¡ç®—å…¬å¼å¦‚ä¸‹:</p>
<p><span id="eq-cross-entropy-loss"><span class="math display">\[
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
\tag{34}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(C\)</span> æ˜¯ç±»åˆ«çš„æ€»æ•°ï¼Œ<span class="math inline">\(y_{i,c}\)</span> æ˜¯æ ·æœ¬ <span class="math inline">\(i\)</span> åœ¨ç±»åˆ« <span class="math inline">\(c\)</span> ä¸Šçš„çœŸå®æ ‡ç­¾ï¼ˆone-hot encodingï¼‰ï¼Œ<span class="math inline">\(\hat{y}_{i,c}\)</span> æ˜¯æ¨¡å‹å¯¹æ ·æœ¬ <span class="math inline">\(i\)</span> åœ¨ç±»åˆ« <span class="math inline">\(c\)</span> ä¸Šçš„é¢„æµ‹æ¦‚ç‡ã€‚</p>
<p>ç»“åˆLabel Smoothing <a href="#eq-label-smoothing" class="quarto-xref">Equation&nbsp;31</a>, äº¤å‰ç†µæŸå¤±å‡½æ•°çš„è®¡ç®—å…¬å¼å¯ä»¥è°ƒæ•´ä¸º:</p>
<p><span id="eq-cross-entropy-loss-smooth"><span class="math display">\[
\mathcal{L}_{smooth} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c}^{smooth} \log(\hat{y}_{i,c})
\tag{35}\]</span></span> å…¶ä¸­ï¼Œ<span class="math inline">\(y_{i,c}^{smooth}\)</span> æ˜¯ç»è¿‡Label Smoothingå¤„ç†åçš„ç›®æ ‡æ ‡ç­¾ã€‚</p>
<div class="sourceCode" id="cb30" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="kw">def</span> cross_entropy_loss(logits, labels, ignore_index<span class="op">=</span><span class="dv">0</span>, label_smoothing<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="co"># Create mask for ignore_index</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>    mask <span class="op">=</span> labels <span class="op">!=</span> ignore_index</span>
<span id="cb30-4"><a href="#cb30-4"></a>    num_classes <span class="op">=</span> logits.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb30-5"><a href="#cb30-5"></a></span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="cf">if</span> label_smoothing <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb30-7"><a href="#cb30-7"></a>        smooth_labels <span class="op">=</span> F.one_hot(labels, num_classes).<span class="bu">float</span>()</span>
<span id="cb30-8"><a href="#cb30-8"></a>        smooth_labels <span class="op">=</span> smooth_labels <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> label_smoothing) <span class="op">+</span> label_smoothing <span class="op">/</span> num_classes</span>
<span id="cb30-9"><a href="#cb30-9"></a>    <span class="cf">else</span>:</span>
<span id="cb30-10"><a href="#cb30-10"></a>        smooth_labels <span class="op">=</span> F.one_hot(labels, num_classes).<span class="bu">float</span>()</span>
<span id="cb30-11"><a href="#cb30-11"></a></span>
<span id="cb30-12"><a href="#cb30-12"></a>    log_probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb30-13"><a href="#cb30-13"></a>    loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(smooth_labels <span class="op">*</span> log_probs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb30-14"><a href="#cb30-14"></a>    loss <span class="op">=</span> loss <span class="op">*</span> mask.<span class="bu">float</span>()</span>
<span id="cb30-15"><a href="#cb30-15"></a>    loss <span class="op">=</span> loss.<span class="bu">sum</span>() <span class="op">/</span> mask.<span class="bu">sum</span>()</span>
<span id="cb30-16"><a href="#cb30-16"></a></span>
<span id="cb30-17"><a href="#cb30-17"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="evaluation-metric" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evaluation-metric"><span class="header-section-number">3.5</span> Evaluation Metric</h2>
<section id="blue" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="blue"><span class="header-section-number">3.5.1</span> BLUE</h3>
<p>åœ¨è¯„ä¼°Transformeræ¨¡å‹çš„æ€§èƒ½æ—¶ï¼Œé€šå¸¸ä½¿ç”¨BLEUï¼ˆBilingual Evaluation Understudyï¼‰åˆ†æ•°ä½œä¸ºä¸»è¦çš„è¯„ä»·æŒ‡æ ‡ã€‚BLEUåˆ†æ•°æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘è´¨é‡çš„è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼Œé€šè¿‡æ¯”è¾ƒæœºå™¨ç”Ÿæˆçš„ç¿»è¯‘ä¸ä¸€ä¸ªæˆ–å¤šä¸ªå‚è€ƒç¿»è¯‘ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥è¡¡é‡ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚BLEUåˆ†æ•°çš„è®¡ç®—è¿‡ç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤:</p>
<ol type="1">
<li><strong>N-gramåŒ¹é…</strong>: è®¡ç®—æœºå™¨ç¿»è¯‘è¾“å‡ºä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„n-gramåŒ¹é…æ•°é‡ï¼Œé€šå¸¸è€ƒè™‘1-gramåˆ°4-gramã€‚</li>
<li><strong>ç²¾ç¡®ç‡è®¡ç®—</strong>: å¯¹æ¯ä¸ªn-gramï¼Œè®¡ç®—åŒ¹é…çš„n-gramæ•°é‡ä¸æœºå™¨ç¿»è¯‘è¾“å‡ºä¸­n-gramæ€»æ•°çš„æ¯”å€¼ï¼Œå¾—åˆ°ç²¾ç¡®ç‡ã€‚</li>
<li><strong>å‡ ä½•å¹³å‡</strong>: å°†å„ä¸ªn-gramçš„ç²¾ç¡®ç‡è¿›è¡Œå‡ ä½•å¹³å‡ï¼Œä»¥ç»¼åˆè€ƒè™‘ä¸åŒé•¿åº¦çš„n-gramåŒ¹é…æƒ…å†µã€‚</li>
<li><strong>é•¿åº¦æƒ©ç½š</strong>: ä¸ºäº†é˜²æ­¢æœºå™¨ç¿»è¯‘è¾“å‡ºè¿‡çŸ­ï¼ŒBLEUåˆ†æ•°å¼•å…¥äº†é•¿åº¦æƒ©ç½šé¡¹ï¼Œæ ¹æ®æœºå™¨ç¿»è¯‘è¾“å‡ºçš„é•¿åº¦ä¸å‚è€ƒç¿»è¯‘çš„é•¿åº¦è¿›è¡Œè°ƒæ•´ã€‚</li>
<li><strong>æœ€ç»ˆè®¡ç®—</strong>: å°†å‡ ä½•å¹³å‡çš„ç²¾ç¡®ç‡ä¸é•¿åº¦æƒ©ç½šç›¸ä¹˜ï¼Œå¾—åˆ°æœ€ç»ˆçš„BLEUåˆ†æ•°ï¼ŒèŒƒå›´åœ¨0åˆ°1ä¹‹é—´ï¼Œé€šå¸¸è¡¨ç¤ºä¸ºç™¾åˆ†æ¯”å½¢å¼ã€‚</li>
</ol>
<p>è®¡ç®—BLEUåˆ†æ•°çš„å…¬å¼å¦‚ä¸‹: <span id="eq-bleu-score"><span class="math display">\[
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\tag{36}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(\text{BP}\)</span> æ˜¯é•¿åº¦æƒ©ç½šé¡¹ï¼Œ<span class="math inline">\(p_n\)</span> æ˜¯n-gramçš„ç²¾ç¡®ç‡ï¼Œ<span class="math inline">\(w_n\)</span> æ˜¯n-gramçš„æƒé‡ï¼Œé€šå¸¸å‡åŒ€åˆ†é…ã€‚</p>
<div class="sourceCode" id="cb31" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="im">import</span> nltk</span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="im">from</span> nltk.translate.bleu_score <span class="im">import</span> sentence_bleu, SmoothingFunction</span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="kw">def</span> compute_bleu(reference, candidate):</span>
<span id="cb31-4"><a href="#cb31-4"></a>    reference_tokens <span class="op">=</span> [reference.split()]</span>
<span id="cb31-5"><a href="#cb31-5"></a>    candidate_tokens <span class="op">=</span> candidate.split()</span>
<span id="cb31-6"><a href="#cb31-6"></a>    smoothing_function <span class="op">=</span> SmoothingFunction().method1</span>
<span id="cb31-7"><a href="#cb31-7"></a>    bleu_score <span class="op">=</span> sentence_bleu(reference_tokens, candidate_tokens,</span>
<span id="cb31-8"><a href="#cb31-8"></a>                               weights<span class="op">=</span>(<span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>),</span>
<span id="cb31-9"><a href="#cb31-9"></a>                               smoothing_function<span class="op">=</span>smoothing_function)</span>
<span id="cb31-10"><a href="#cb31-10"></a>    <span class="cf">return</span> bleu_score <span class="op">*</span> <span class="dv">100</span>  <span class="co"># Convert to percentage</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="perplexity" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="perplexity"><span class="header-section-number">3.5.2</span> Perplexity</h3>
<p>å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¸¸ç”¨æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æ¨¡å‹å¯¹ç»™å®šæ–‡æœ¬åºåˆ—çš„é¢„æµ‹èƒ½åŠ›ã€‚å›°æƒ‘åº¦çš„å®šä¹‰æ˜¯è¯­è¨€æ¨¡å‹å¯¹æµ‹è¯•é›†ä¸Šæ¯ä¸ªè¯çš„å¹³å‡ä¸ç¡®å®šæ€§ï¼Œæ•°å€¼è¶Šä½è¡¨ç¤ºæ¨¡å‹å¯¹æ–‡æœ¬çš„é¢„æµ‹è¶Šå‡†ç¡®ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæµ‹è¯•é›† <span class="math inline">\(W = w_1, w_2, \ldots, w_N\)</span>ï¼Œè¯­è¨€æ¨¡å‹è®¡ç®—è¯¥åºåˆ—çš„æ¦‚ç‡ <span class="math inline">\(P(W)\)</span>ï¼Œå›°æƒ‘åº¦çš„è®¡ç®—å…¬å¼å¦‚ä¸‹:</p>
<p><span id="eq-perplexity"><span class="math display">\[
\text{Perplexity}(W) = P(W)^{-\frac{1}{N}} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, w_2, \ldots, w_{i-1})\right)
\tag{37}\]</span></span></p>
<p>å…¶ä¸­ï¼Œ<span class="math inline">\(N\)</span> æ˜¯æµ‹è¯•é›†ä¸­çš„è¯æ•°ï¼Œ<span class="math inline">\(P(w_i | w_1, w_2, \ldots, w_{i-1})\)</span> æ˜¯è¯­è¨€æ¨¡å‹é¢„æµ‹ç¬¬ <span class="math inline">\(i\)</span> ä¸ªè¯çš„æ¡ä»¶æ¦‚ç‡ã€‚å›°æƒ‘åº¦å¯ä»¥ç†è§£ä¸ºæ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶é¢ä¸´çš„é€‰æ‹©æ•°é‡çš„æŒ‡æ•°çº§å¢é•¿ã€‚</p>
<p>åœ¨å®é™…è®¡ç®—ä¸­ï¼Œå›°æƒ‘åº¦é€šå¸¸é€šè¿‡äº¤å‰ç†µæŸå¤±æ¥é—´æ¥è®¡ç®—: <span id="eq-perplexity-cross-entropy"><span class="math display">\[
\text{Perplexity}(W) = \exp(\text{CrossEntropyLoss})
\tag{38}\]</span></span></p>
<p>åœ¨ä¹‹å‰ï¼Œæˆ‘ä»¬æœ‰æåˆ°Label Smoothing, å®ƒä¼šå½±å“å›°æƒ‘åº¦çš„è®¡ç®—ï¼Œå› ä¸ºLabel Smoothingä¼šæ”¹å˜ç›®æ ‡åˆ†å¸ƒï¼Œä»è€Œå½±å“äº¤å‰ç†µæŸå¤±çš„è®¡ç®—ï¼Œè¿›è€Œå½±å“å›°æƒ‘åº¦çš„æ•°å€¼ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨Label Smoothingæ—¶ï¼Œå›°æƒ‘åº¦çš„æ•°å€¼å¯èƒ½ä¼šæœ‰æ‰€åå·®ï¼Œéœ€è¦è°¨æ…è§£é‡Šã€‚</p>
<div id="fig-perplexity-label-smoothing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-perplexity-label-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/perplexity-label-smoothing.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-perplexity-label-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œéšç€ <span class="math inline">\(\epsilon\)</span> çš„å¢åŠ ï¼ŒPerplexity ä¹Ÿåœ¨å¢åŠ ã€‚è¿™æ˜¯å› ä¸º Label Smoothing å¼•å…¥äº†å™ªå£°ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´éš¾ä»¥å‡†ç¡®é¢„æµ‹ç›®æ ‡è¯ï¼Œä»è€Œå¯¼è‡´äº¤å‰ç†µæŸå¤±å¢åŠ ã€‚
</figcaption>
</figure>
</div>
</section>
</section>
<section id="training" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="training"><span class="header-section-number">3.6</span> Training</h2>
<p>åœ¨è®­ç»ƒTransformeræ¨¡å‹ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä»¥ä¸‹è¶…å‚æ•°:</p>
<div class="sourceCode" id="cb32" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="at">@dataclass</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="kw">class</span> ModelConfig:</span>
<span id="cb32-3"><a href="#cb32-3"></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> VOCAB_SIZE</span>
<span id="cb32-4"><a href="#cb32-4"></a>    max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb32-5"><a href="#cb32-5"></a></span>
<span id="cb32-6"><a href="#cb32-6"></a>    d_model: <span class="bu">int</span> <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb32-7"><a href="#cb32-7"></a>    d_ff: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb32-8"><a href="#cb32-8"></a>    num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-9"><a href="#cb32-9"></a>    num_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb32-10"><a href="#cb32-10"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-11"><a href="#cb32-11"></a></span>
<span id="cb32-12"><a href="#cb32-12"></a><span class="at">@dataclass</span></span>
<span id="cb32-13"><a href="#cb32-13"></a><span class="kw">class</span> TrainConfig:</span>
<span id="cb32-14"><a href="#cb32-14"></a>    batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb32-15"><a href="#cb32-15"></a>    gradient_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-16"><a href="#cb32-16"></a>    total_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10_000</span>  <span class="co"># set to 0 for automatic calculation</span></span>
<span id="cb32-17"><a href="#cb32-17"></a>    warmup_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># will be set by total_steps // 10</span></span>
<span id="cb32-18"><a href="#cb32-18"></a></span>
<span id="cb32-19"><a href="#cb32-19"></a>    lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">5e-3</span></span>
<span id="cb32-20"><a href="#cb32-20"></a>    min_lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb32-21"><a href="#cb32-21"></a>    betas: <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>] <span class="op">=</span> field(default_factory<span class="op">=</span><span class="kw">lambda</span>: (<span class="fl">0.9</span>, <span class="fl">0.98</span>))</span>
<span id="cb32-22"><a href="#cb32-22"></a>    weight_decay: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb32-23"><a href="#cb32-23"></a>    optim_eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb32-24"><a href="#cb32-24"></a></span>
<span id="cb32-25"><a href="#cb32-25"></a>    label_smoothing: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb32-26"><a href="#cb32-26"></a></span>
<span id="cb32-27"><a href="#cb32-27"></a>    debug: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span></span>
<span id="cb32-28"><a href="#cb32-28"></a>    device <span class="op">=</span> get_device()</span>
<span id="cb32-29"><a href="#cb32-29"></a>    mixed_precision: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb32-30"><a href="#cb32-30"></a>    eval_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Mixed Precision Trainingæ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚æ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨16ä½æµ®ç‚¹æ•°ï¼ˆFP16ï¼‰å’Œ32ä½æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰çš„ç»„åˆï¼Œæ—¢ä¿æŒäº†æ¨¡å‹çš„ç²¾åº¦ï¼Œåˆæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸»è¦ä½¿ç”¨FP16è¿›è¡Œè®¡ç®—ï¼Œè€Œå…³é”®çš„å‚æ•°æ›´æ–°å’Œæ¢¯åº¦ç´¯ç§¯åˆ™ä½¿ç”¨FP32ï¼Œä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ã€‚</p>
<p>åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†Gradient AccumulationæŠ€æœ¯ï¼Œä»¥ä¾¿åœ¨æ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹ä½¿ç”¨è¾ƒå¤§çš„æœ‰æ•ˆæ‰¹é‡å¤§å°è¿›è¡Œè®­ç»ƒã€‚æ¢¯åº¦ç´¯ç§¯çš„åŸºæœ¬æ€æƒ³æ˜¯å°†å¤šä¸ªå°æ‰¹é‡çš„æ¢¯åº¦ç´¯ç§¯èµ·æ¥ï¼Œç„¶åå†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ä¸€ä¸ªè¾ƒå¤§çš„æ‰¹é‡å¤§å° <span class="math inline">\(B\)</span> è¿›è¡Œè®­ç»ƒï¼Œä½†ç”±äºæ˜¾å­˜é™åˆ¶ï¼Œæˆ‘ä»¬åªèƒ½ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ‰¹é‡å¤§å° <span class="math inline">\(b\)</span>ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å°† <span class="math inline">\(B/b\)</span> ä¸ªå°æ‰¹é‡çš„æ¢¯åº¦ç´¯ç§¯èµ·æ¥ï¼Œç„¶åå†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚</p>
<p>ç®€å•æ¥çœ‹ï¼Œæˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯å¦‚ä¸‹:</p>
<div class="sourceCode" id="cb33" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb33-2"><a href="#cb33-2"></a>    optimizer.zero_grad()</span>
<span id="cb33-3"><a href="#cb33-3"></a>    <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(gradient_steps):</span>
<span id="cb33-4"><a href="#cb33-4"></a>        <span class="cf">with</span> torch.autocast(</span>
<span id="cb33-5"><a href="#cb33-5"></a>            device_type<span class="op">=</span>train_config.device.<span class="bu">type</span>, enabled<span class="op">=</span>train_config.mixed_precision, dtype<span class="op">=</span>torch.bfloat16</span>
<span id="cb33-6"><a href="#cb33-6"></a>        ):</span>
<span id="cb33-7"><a href="#cb33-7"></a>            logits <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb33-8"><a href="#cb33-8"></a>            loss <span class="op">=</span> cross_entropy_loss(</span>
<span id="cb33-9"><a href="#cb33-9"></a>                logits,</span>
<span id="cb33-10"><a href="#cb33-10"></a>                labels,</span>
<span id="cb33-11"><a href="#cb33-11"></a>                ignore_index<span class="op">=</span>translation_dataset.pad_id,</span>
<span id="cb33-12"><a href="#cb33-12"></a>                label_smoothing<span class="op">=</span>train_config.label_smoothing,</span>
<span id="cb33-13"><a href="#cb33-13"></a>            )</span>
<span id="cb33-14"><a href="#cb33-14"></a>            loss <span class="op">/=</span> train_config.gradient_steps</span>
<span id="cb33-15"><a href="#cb33-15"></a>        loss.backward()</span>
<span id="cb33-16"><a href="#cb33-16"></a>    </span>
<span id="cb33-17"><a href="#cb33-17"></a>    optimizer.update_lr()</span>
<span id="cb33-18"><a href="#cb33-18"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE: OOM Error</p>
</div>
<div class="note-container foldable-content">
<p>å¦‚æœå¤§å®¶åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œé‡åˆ°äº†OOM Errorï¼Œæˆ‘ä»¬å¯ä»¥è°ƒå°æˆ‘ä»¬çš„Batch Sizeï¼ŒåŒæ—¶å¢å¤§æˆ‘ä»¬çš„Gradient Stepsï¼Œè¿™æ ·å¯ä»¥ä¿æŒæœ€ç»ˆçš„Batch Sizeä¸å˜ã€‚</p>
</div>
</div>
</section>
<section id="results" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="results"><span class="header-section-number">3.7</span> Results</h2>
<div id="fig-loss-curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/Transformer-loss.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: The Loss Curve of Transformer training for 10,000 steps
</figcaption>
</figure>
</div>
<p>æœ‰ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡å°±æ˜¯ï¼ŒLossçš„ä¸‹é™å‘ˆç° zig-zag patternï¼Œè¿™ä¸ªç°è±¡åœ¨å¾ˆå¤šNLPæ¨¡å‹çš„è®­ç»ƒä¸­éƒ½ä¼šå‡ºç°ï¼Œä½†æ˜¯æˆ‘è¿˜æ²¡æœ‰æ‰¾åˆ°ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Šï¼Œå¯èƒ½æ˜¯Learning Rate è°ƒèŠ‚çš„åŸå› ï¼Œä¹Ÿå¯èƒ½æ˜¯Dataloaderçš„é—®é¢˜ï¼Œæœ‰æ—¶é—´æˆ‘å°±æ¢ç´¢è¿™ä¸ªé—®é¢˜çš„ã€‚æ¬¢è¿å¤§å®¶åœ¨è¯„è®ºåŒºç•™è¨€è®¨è®ºï¼</p>
<p>ä¸‹é¢æ˜¯è®­ç»ƒå®Œ10,000æ­¥åä¸€ä¸ªç¿»è¯‘çš„ä¾‹å­:</p>
<div class="sourceCode" id="cb34" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb34-1"><a href="#cb34-1"></a>English Input: Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow</span>
<span id="cb34-2"><a href="#cb34-2"></a>challenge.</span>
<span id="cb34-3"><a href="#cb34-3"></a>Model Output: &lt;s&gt; å‡ å¹´å‰,åœ¨TED, Peter Skillmanä»‹ç»äº†ä¸€ä¸ªè®¾è®¡æŒ‘æˆ˜  å«åšæ¨é¥¼å¹²çš„æŒ‘æˆ˜-- &lt;/s&gt;</span>
<span id="cb34-4"><a href="#cb34-4"></a>Reference: å‡ å¹´å‰ï¼Œåœ¨è¿™é‡Œçš„ TED ä¸Šï¼ŒPeter Skillman æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæ£‰èŠ±ç³–æŒ‘æˆ˜â€çš„è®¾è®¡æŒ‘æˆ˜ã€‚</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ä¸€äº›ç®€å•çš„ä¾‹å­</p>
<div class="sourceCode" id="cb35" data-code-line-numbers=""><pre class="sourceCode numberSource text number-lines code-with-copy"><code class="sourceCode"><span id="cb35-1"><a href="#cb35-1"></a>English Input: Who are you?</span>
<span id="cb35-2"><a href="#cb35-2"></a>Model Output: &lt;s&gt; ä½ æ˜¯è°?&lt;/s&gt;</span>
<span id="cb35-3"><a href="#cb35-3"></a>English Input: What is your name?</span>
<span id="cb35-4"><a href="#cb35-4"></a>Model Output: &lt;s&gt; ä½ å«ä»€ä¹ˆ?&lt;/s&gt;</span>
<span id="cb35-5"><a href="#cb35-5"></a>English Input: I love Artificial Intelligence.</span>
<span id="cb35-6"><a href="#cb35-6"></a>Model Output: &lt;s&gt; æˆ‘å–œæ¬¢é­…åŠ›ã€‚&lt;/s&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å·²ç»èƒ½å¤Ÿè¿›è¡Œç®€å•çš„è‹±æ–‡åˆ°ä¸­æ–‡çš„ç¿»è¯‘äº†ï¼Œå½“ç„¶è·ç¦»å®é™…åº”ç”¨è¿˜æœ‰å¾ˆå¤§çš„å·®è·ï¼Œæ¯”å¦‚ç¿»è¯‘çš„æµç•…åº¦å’Œå‡†ç¡®åº¦è¿˜éœ€è¦æå‡ï¼Œæ¨¡å‹çš„è§„æ¨¡ä¹Ÿéœ€è¦æ›´å¤§ï¼Œè®­ç»ƒçš„æ•°æ®ä¹Ÿéœ€è¦æ›´å¤šç­‰ç­‰ã€‚</p>
</section>
</section>
<section id="summary" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Summary</h1>
<p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†Transformeræ¨¡å‹ï¼šTransformer æ˜¯ä¸€ç§ä»¥æ³¨æ„åŠ›æœºåˆ¶ä¸ºæ ¸å¿ƒçš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå®ƒç”¨ Attention åœ¨å…¨å±€èŒƒå›´å†…ç›´æ¥å»ºæ¨¡ token ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œæ‘†è„±äº† RNN/CNN çš„é¡ºåºè®¡ç®—ç“¶é¢ˆå¹¶å®ç°é«˜æ•ˆå¹¶è¡Œã€‚åœ¨è§£ç ç«¯é€šè¿‡ Masked Self-Attention å±è”½æœªæ¥ä¿¡æ¯ï¼Œä¿è¯è‡ªå›å½’ç”Ÿæˆçš„å› æœæ€§ï¼›åŒæ—¶ç”¨ Positional Encoding å°†é¡ºåºä¿¡æ¯æ³¨å…¥è¡¨ç¤ºï¼Œä½¿æ¨¡å‹åœ¨æ— å¾ªç¯ç»“æ„ä¸‹ä»èƒ½ç†è§£ä½ç½®ä¸ç›¸å¯¹æ¬¡åºã€‚æ•´ä½“é‡‡ç”¨ç»å…¸çš„ Encoderâ€“Decoder ç»“æ„ï¼šç¼–ç å™¨é€šè¿‡è‡ªæ³¨æ„åŠ›æå–æºåºåˆ—ä¸Šä¸‹æ–‡ï¼Œè§£ç å™¨ç»“åˆè‡ªæ³¨æ„åŠ›ä¸ç¼–ç å™¨è¾“å‡ºè¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚ä¸ºç¨³å®šæ·±å±‚è®­ç»ƒä¸æå‡è¡¨è¾¾èƒ½åŠ›ï¼ŒTransformer åœ¨æ¯ä¸ªå­å±‚å¼•å…¥ Residual Connections ä¸ Layer Normalization æ¥æ”¹å–„æ¢¯åº¦ä¼ æ’­ä¸æ•°å€¼ç¨³å®šæ€§ï¼Œå¹¶ä½¿ç”¨ä½ç½®ç‹¬ç«‹çš„ Feed Forward Network æä¾›éçº¿æ€§ç‰¹å¾å˜æ¢ä¸è¡¨ç¤ºå¢å¼ºã€‚</p>
<p>åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†Transformeræ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„å…·ä½“å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬</p>
<ul>
<li>æ•°æ®é¢„å¤„ç†ã€</li>
<li>æ¨¡å‹è®­ç»ƒ</li>
<li>Adam çš„å®ç°</li>
<li>ä»¥åŠè¯„ä¼°æŒ‡æ ‡ç­‰å†…å®¹ã€‚</li>
</ul>
<p>é€šè¿‡è¿™äº›ç»†èŠ‚çš„ä»‹ç»ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£Transformeræ¨¡å‹çš„å·¥ä½œåŸç†ä»¥åŠå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚</p>
<p>æ€»ä¹‹ï¼Œå…³äºTransformerä»¥åŠåç»­çš„æ”¹è¿›ï¼Œæœ‰å¤ªå¤šå¤ªå¤šå¯ä»¥è®²çš„äº†ï¼Œæ¯”å¦‚ï¼š</p>
<ul>
<li>Position Encodingçš„å˜ä½“</li>
<li>Normalizationçš„å˜ä½“ï¼Œä»¥åŠä½ç½®</li>
<li>Attentionçš„ä¼˜åŒ–</li>
<li>Feed Forward Networkçš„æ”¹è¿›</li>
<li>å¹¶è¡Œçš„è®­ç»ƒä¸æ¨ç†æŠ€æœ¯</li>
<li>å¦‚ä½•å¾®è°ƒé¢„è®­ç»ƒçš„Transformeræ¨¡å‹</li>
</ul>
<p>åœ¨ç¬¬ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬åªèƒ½ä»‹ç»Transformerçš„åŸºç¡€å†…å®¹ï¼Œåç»­æˆ‘ä»¬ä¼šæœ‰æ›´å¤šçš„æ–‡ç« ï¼Œæ¥ä»‹ç»è¿™äº›å†…å®¹ï¼Œæ•¬è¯·æœŸå¾…ï¼</p>
</section>
<section id="key-concepts" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Key Concepts</h1>
<div id="tbl-transformer-key-concepts" class="hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-transformer-key-concepts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table">
<colgroup>
<col style="width: 41%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Self-Attention</strong></td>
<td style="text-align: left;">Queryã€Keyã€Value éƒ½æ¥è‡ªåŒä¸€åºåˆ—ï¼Œé€šè¿‡è®¡ç®— <span class="math inline">\(QK^\top\)</span> å¾—åˆ°ä»»æ„ä¸¤ä¸ª token ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå†å¯¹ Value åšåŠ æƒæ±‚å’Œï¼ˆå¼ <a href="#eq-attention" class="quarto-xref">Equation&nbsp;11</a>ï¼‰ã€‚å®ƒä¸ä¾èµ–è·ç¦»ï¼Œä¸€æ¬¡çŸ©é˜µä¹˜æ³•å°±èƒ½è®©æ¯ä¸ª token ç›´æ¥â€œçœ‹åˆ°â€åºåˆ—ä¸­æ‰€æœ‰ä½ç½®ï¼Œæ˜¯ Transformer èƒ½å»ºæ¨¡é•¿è·ç¦»ä¾èµ–çš„æ ¸å¿ƒåŸå› ã€‚</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Scaled Dot-Product Attention</strong></td>
<td style="text-align: left;">åœ¨ç‚¹ç§¯æ³¨æ„åŠ›ä¸­å¼•å…¥ <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span> ç¼©æ”¾ï¼ˆ<a href="#eq-attention" class="quarto-xref">Equation&nbsp;11</a>ï¼‰ï¼Œæ˜¯å› ä¸ºå½“ <span class="math inline">\(d_k\)</span> è¾ƒå¤§æ—¶ï¼Œ<span class="math inline">\(QK^\top\)</span> çš„æ–¹å·®éšç»´åº¦çº¿æ€§å¢å¤§ï¼ˆ<a href="#eq-attention-dot-product-variance" class="quarto-xref">Equation&nbsp;13</a>ï¼‰ï¼Œä¼šå¯¼è‡´ softmax è¿›å…¥é¥±å’ŒåŒºã€æ¢¯åº¦æ¥è¿‘ 0ã€‚ç¼©æ”¾ç›¸å½“äºå¯¹ logits åšæ–¹å·®å½’ä¸€åŒ–ï¼Œä¿è¯æ¢¯åº¦å¤„åœ¨å¯å­¦ä¹ åŒºé—´ï¼ˆè§ scaling-d_k å›¾ï¼‰ã€‚</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Multi-Head Attention</strong></td>
<td style="text-align: left;">å°† <span class="math inline">\(d_{model}\)</span> æ‹†æˆå¤šä¸ª headï¼Œæ¯ä¸ª head éƒ½æœ‰ç‹¬ç«‹çš„ <span class="math inline">\(W^Q,W^K,W^V\)</span>ï¼ˆ<a href="#eq-multi-head-attention-head" class="quarto-xref">Equation&nbsp;17</a>ï¼‰ï¼Œåœ¨ä¸åŒè¡¨ç¤ºå­ç©ºé—´å¹¶è¡Œåšæ³¨æ„åŠ›ï¼›å•å¤´åªèƒ½å­¦ä¸€ç§åŠ æƒæ¨¡å¼ï¼Œå¤šå¤´èƒ½åŒæ—¶å­¦å±€éƒ¨å¯¹é½ã€é•¿è·ä¾èµ–ã€è¯­ä¹‰èšåˆç­‰å¤šç§å…³ç³»ï¼Œæœ€å concat å†çº¿æ€§æ˜ å°„ï¼ˆ<a href="#eq-multi-head-attention" class="quarto-xref">Equation&nbsp;16</a>ï¼‰ã€‚</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Positional Encoding</strong></td>
<td style="text-align: left;">Transformer æœ¬èº«æ˜¯ permutation-invariantï¼Œéœ€è¦æ˜¾å¼æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚æ–‡ç« ä½¿ç”¨æ­£å¼¦/ä½™å¼¦ç»å¯¹ä½ç½®ç¼–ç ï¼ˆ<a href="#eq-position-embedding" class="quarto-xref">Equation&nbsp;7</a>ï¼‰ï¼Œä¸åŒç»´åº¦å¯¹åº”ä¸åŒé¢‘ç‡ï¼šä½ç»´é«˜é¢‘åˆ»ç”»å±€éƒ¨ä½ç½®ï¼Œé«˜ç»´ä½é¢‘åˆ»ç”»å…¨å±€ä½ç½®ï¼›ä¸è¯å‘é‡ç›¸åŠ è€Œéæ‹¼æ¥ï¼Œä¿æŒç»´åº¦ä¸å˜ã€è®¡ç®—å¤æ‚åº¦ä¸å¢åŠ ï¼Œå¹¶å…è®¸å¤–æ¨åˆ°æ›´é•¿åºåˆ—ã€‚</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Encoderâ€“Decoder Attention</strong></td>
<td style="text-align: left;">Cross-Attention ä¸­ Query æ¥è‡ªè§£ç å™¨ï¼ŒKey/Value æ¥è‡ªç¼–ç å™¨è¾“å‡ºï¼ˆ<a href="#eq-cross-attention" class="quarto-xref">Equation&nbsp;21</a>ï¼‰ã€‚è¿™ä¸€æ­¥æœ¬è´¨æ˜¯åœ¨ç”Ÿæˆæ¯ä¸ªç›®æ ‡è¯æ—¶ï¼Œå¯¹æºå¥åšä¸€æ¬¡ä¿¡æ¯æ£€ç´¢ä¸å¯¹é½ï¼Œæ˜¯æœºå™¨ç¿»è¯‘ä¸­â€œçœ‹æºå¥å“ªé‡Œæœ€ç›¸å…³â€çš„æ•°å­¦å®ç°ã€‚</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Masked (Causal) Self-Attention</strong></td>
<td style="text-align: left;">åœ¨è§£ç å™¨è‡ªæ³¨æ„åŠ›ä¸­åŠ å…¥ä¸Šä¸‰è§’ maskï¼ˆ<a href="#eq-mask-matrix" class="quarto-xref">Equation&nbsp;20</a>ï¼‰ï¼ŒæŠŠæœªæ¥ä½ç½®çš„ logits è®¾ä¸º <span class="math inline">\(-\infty\)</span>ï¼Œsoftmax åæ¦‚ç‡ä¸º 0ï¼ˆ<a href="#eq-causal-attention" class="quarto-xref">Equation&nbsp;19</a>ï¼‰ã€‚è¿™æ ·ä¿è¯æ¨¡å‹åœ¨ä½ç½® <span class="math inline">\(i\)</span> åªèƒ½è®¿é—® <span class="math inline">\(j\le i\)</span> çš„ä¿¡æ¯ï¼Œä¸¥æ ¼æ»¡è¶³è‡ªå›å½’ç”Ÿæˆï¼Œä¸å‘ç”Ÿä¿¡æ¯æ³„éœ²ï¼ˆè§ causal mask å›¾ï¼‰ã€‚</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Position-wise Feed Forward Network</strong></td>
<td style="text-align: left;">FFN æ˜¯å¯¹æ¯ä¸ª token ç‹¬ç«‹åº”ç”¨çš„ä¸¤å±‚ MLPï¼ˆ<a href="#eq-feed-forward-network" class="quarto-xref">Equation&nbsp;24</a>ï¼‰ï¼Œä¸­é—´ç»´åº¦ <span class="math inline">\(d_{ff}\approx 4d_{model}\)</span>ã€‚Attention è´Ÿè´£â€œè·¨ token æ··ä¿¡æ¯â€ï¼ŒFFN è´Ÿè´£â€œåœ¨å• token ç»´åº¦ä¸Šåšéçº¿æ€§ç‰¹å¾å˜æ¢â€ï¼Œä¸¤è€…åˆ†å·¥æ˜ç¡®ã€äº’è¡¥ã€‚</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Residual Connection + LayerNorm</strong></td>
<td style="text-align: left;">æ¯ä¸ªå­å±‚é‡‡ç”¨ <span class="math inline">\(x + \text{sublayer}(x)\)</span> çš„æ®‹å·®ç»“æ„ï¼Œå†åš LayerNormï¼ˆ<a href="#eq-residual-connection" class="quarto-xref">Equation&nbsp;25</a>ï¼‰ã€‚åå‘ä¼ æ’­æ—¶æ¢¯åº¦åŒ…å«ä¸€æ¡â€œç›´é€šè·¯å¾„â€ï¼ˆ<a href="#eq-backprop-residual" class="quarto-xref">Equation&nbsp;26</a>ï¼‰ï¼Œæœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼›LayerNorm åœ¨ç‰¹å¾ç»´å½’ä¸€åŒ–ï¼Œç¨³å®šæ¿€æ´»åˆ†å¸ƒï¼Œä½¿æ·±å±‚ Transformer å¯è®­ç»ƒã€‚</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Attention Time Complexity</strong></td>
<td style="text-align: left;">Self-Attention çš„ä¸»æˆæœ¬æ¥è‡ª <span class="math inline">\(QK^\top\)</span> å’Œ <span class="math inline">\(AV\)</span>ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2 d)\)</span>ï¼ˆ<a href="#eq-self-attention-time-complexity" class="quarto-xref">Equation&nbsp;22</a>ï¼‰ã€‚ç›¸æ¯” RNN çš„ <span class="math inline">\(\mathcal{O}(nd^2)\)</span>ï¼ŒTransformer çš„ä¼˜åŠ¿ä¸åœ¨ç†è®ºé˜¶æ•°ï¼Œè€Œåœ¨çŸ©é˜µåŒ–å¹¶è¡Œè®¡ç®—ï¼Œè¿™ä¹Ÿæ˜¯åç»­ FlashAttentionã€Sparse/Linear Attention ç ”ç©¶çš„æ ¹æºã€‚</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-transformer-key-concepts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Key Concepts in Transformer
</figcaption>
</figure>
</div>
</section>
<section id="q-a" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Q &amp; A</h1>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 1: ä¸ºä»€ä¹ˆæ³¨æ„åŠ›è¦é™¤ä»¥ <span class="math inline">\(\sqrt{d_k}\)</span> ï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: å› ä¸º <span class="math inline">\(d_k\)</span> å˜å¤§æ—¶ç‚¹ç§¯çš„æ–¹å·®ä¼šå˜å¤§ï¼Œsoftmax æ›´å®¹æ˜“é¥±å’Œ <a href="#fig-scaling-d-k" class="quarto-xref">Figure&nbsp;3</a>ï¼ˆæŸäº›ä½ç½®æƒé‡æ¥è¿‘ 1ã€å…¶ä½™æ¥è¿‘ 0ï¼‰ï¼Œå¯¼è‡´æ¢¯åº¦å° <a href="#fig-scaling-d-k-gradient" class="quarto-xref">Figure&nbsp;4</a>ã€è®­ç»ƒä¸ç¨³å®šï¼›ç¼©æ”¾èƒ½æŠŠæ•°å€¼æ‹‰å›åˆé€‚èŒƒå›´ã€‚</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 2: å¤šå¤´æ³¨æ„åŠ›ç›¸æ¯”å•å¤´æ³¨æ„åŠ›â€œæœ¬è´¨ä¸Šâ€å¤šäº†ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: å•å¤´æ³¨æ„åŠ›åœ¨ä¸€ä¸ªè¡¨ç¤ºå­ç©ºé—´é‡Œåšä¸€æ¬¡å…¨å±€åŠ æƒï¼›å¤šå¤´æŠŠè¡¨ç¤ºæ‹†æˆå¤šä¸ªå­ç©ºé—´å¹¶è¡Œåš attentionï¼Œç­‰ä»·äºåŒæ—¶å­¦ä¹ å¤šç§å…³ç³»æ¨¡å¼ï¼ˆä¾‹å¦‚å±€éƒ¨ä¾å­˜ã€é•¿è·ç¦»æŒ‡ä»£ã€è¯­ä¹‰èšåˆç­‰ï¼‰ï¼Œå†èåˆï¼Œè¡¨è¾¾åŠ›æ›´å¼ºä¸”æ›´ç¨³å¥ã€‚</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 3: æ²¡æœ‰ RNN/CNNï¼ŒTransformer æ€ä¹ˆçŸ¥é“â€œé¡ºåºâ€ï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: é€šè¿‡æŠŠä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ <a href="#sec-postion-embedding" class="quarto-xref">Section&nbsp;2.2</a> åŠ åˆ°è¾“å…¥ embedding ä¸Šï¼Œè®©æ¨¡å‹åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­å¯åˆ©ç”¨ä½ç½®ä¿¡æ¯ï¼›è®ºæ–‡ç”¨æ­£å¼¦/ä½™å¼¦çš„å›ºå®šç¼–ç å®ç°ã€‚</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 4: Decoder ä¸ºä»€ä¹ˆå¿…é¡» Maskï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: è®­ç»ƒæ—¶ç›®æ ‡åºåˆ—æ˜¯å·²çŸ¥çš„ï¼Œä½†ç”Ÿæˆæ—¶å¿…é¡»é€æ­¥é¢„æµ‹ï¼›å¦‚æœä¸ maskï¼Œæ¨¡å‹è®­ç»ƒæ—¶èƒ½çœ‹åˆ°æœªæ¥ tokenï¼Œä¼šé€ æˆâ€œè®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´â€ï¼Œå¹¶ç ´åè‡ªå›å½’å»ºæ¨¡ã€‚</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 5: Transformer çš„è®¡ç®—å¤æ‚åº¦ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: è‡ªæ³¨æ„åŠ›éœ€è¦æ„é€  <span class="math inline">\(n \times n\)</span> çš„æ³¨æ„åŠ›çŸ©é˜µ <a href="#sec-attention-complexity" class="quarto-xref">Section&nbsp;2.3.5</a>ï¼Œæ—¶é—´ä¸å†…å­˜å¯¹åºåˆ—é•¿åº¦ <span class="math inline">\(n\)</span> æ˜¯äºŒæ¬¡çš„ï¼›è¿™ä¹Ÿæ˜¯åæ¥ Longformer/Performer/FlashAttention/çº¿æ€§æ³¨æ„åŠ›ç­‰å·¥ä½œçš„åŠ¨æœºä¹‹ä¸€ã€‚</p>
</div>
</div>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question 6: FFN ä¸ºä»€ä¹ˆæ˜¯â€œå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹â€ï¼Ÿ</p>
</div>
<div class="question-container foldable-content">
<p>Answer: æ³¨æ„åŠ›è´Ÿè´£â€œè·¨ä½ç½®çš„ä¿¡æ¯æ··åˆâ€ï¼ŒFFN è´Ÿè´£â€œæ¯ä¸ªä½ç½®çš„éçº¿æ€§ç‰¹å¾å˜æ¢â€ï¼›äºŒè€…åˆ†å·¥æ¸…æ™°ï¼Œä¸”ä½ç½®ç‹¬ç«‹è®¡ç®—æ›´æ˜“å¹¶è¡Œä¸å®ç°ã€‚</p>
</div>
</div>
</section>
<section id="related-resource-further-reading" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Related resource &amp; Further Reading</h1>
<ul>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a>: A step-by-step explanation of the Transformer model with code snippets.</li>
<li><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a>: A comprehensive overview of the Transformer architecture and its variants.</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>: A visual and intuitive explanation of the Transformer model.</li>
</ul>
<hr>
<p>æˆ‘ä»¬çŸ¥é“ï¼ŒTransformerçš„æ ¸å¿ƒæ˜¯Attentionæœºåˆ¶ï¼Œåç»­æœ‰è®¸å¤šå·¥ä½œï¼Œéƒ½æ˜¯å›´ç»•ç€ä¼˜åŒ–Attentionæœºåˆ¶å±•å¼€çš„ï¼Œä¸‹é¢æ˜¯ä¸€äº›æ¯”è¾ƒé‡è¦çš„Transformerçš„å˜ç§:</p>
<ul>
<li>Flash-Attention<span class="citation" data-cites="FlashAttentionFastMemoryEfficient2022dao">(<a href="#ref-FlashAttentionFastMemoryEfficient2022dao" role="doc-biblioref">Dao et al. 2022</a>)</span>: ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å—å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>Multi-Query Attention / Grouped Query Attention: ä¸€ç§æ”¹è¿›çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å…±äº«æŸ¥è¯¢å‘é‡æ¥å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Linear Attention: ä¸€ç§çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—ä»äºŒæ¬¡å¤æ‚åº¦é™ä½åˆ°çº¿æ€§å¤æ‚åº¦ï¼Œæé«˜äº†é•¿åºåˆ—å¤„ç†çš„æ•ˆç‡ã€‚</li>
<li>Native Sparse Attention <span class="citation" data-cites="NativeSparseAttention2025yuan">(<a href="#ref-NativeSparseAttention2025yuan" role="doc-biblioref">Yuan et al. 2025</a>)</span>: ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡åªè®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›æƒé‡æ¥æé«˜æ•ˆç‡ï¼Œé€‚ç”¨äºé•¿åºåˆ—å¤„ç†ã€‚</li>
<li>KV Cache: ä¸€ç§ç¼“å­˜æœºåˆ¶ï¼Œé€šè¿‡å­˜å‚¨è¿‡å»çš„é”®å€¼å¯¹æ¥åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ï¼Œå‡å°‘é‡å¤è®¡ç®—ã€‚</li>
</ul>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€äº›Transformerçš„æ”¹è¿›æ–¹å‘:</p>
<ul>
<li>Feed Forward Network
<ul>
<li>Mixture of Expert <span class="citation" data-cites="SwitchTransformersScaling2022fedus">(<a href="#ref-SwitchTransformersScaling2022fedus" role="doc-biblioref">Fedus, Zoph, and Shazeer 2022</a>)</span>: ä¸€ç§åŸºäºä¸“å®¶æ¨¡å‹çš„å‰é¦ˆç½‘ç»œï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ä¸“å®¶å­ç½‘ç»œæ¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ul></li>
<li>Normalization:
<ul>
<li>RMS-Normalization <span class="citation" data-cites="RootMeanSquare2019zhang">(<a href="#ref-RootMeanSquare2019zhang" role="doc-biblioref">Zhang and Sennrich 2019</a>)</span>: ä¸€ç§å½’ä¸€åŒ–æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—è¾“å…¥çš„å‡æ–¹æ ¹å€¼æ¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œå…·æœ‰æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§å’Œè®­ç»ƒæ•ˆæœã€‚</li>
</ul></li>
<li>Position Embedding:
<ul>
<li>RoPE<span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span>: ä¸€ç§æ—‹è½¬ä½ç½®ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡æ—‹è½¬åµŒå…¥å‘é‡æ¥æ•æ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹å¯¹é•¿åºåˆ—çš„å¤„ç†èƒ½åŠ›ã€‚</li>
</ul></li>
</ul>
<p>æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†Transformerè¿ç”¨åˆ°ä¸åŒçš„ Modalityä¸­ï¼Œæ¯”å¦‚Visionï¼Œ è¿™å°±æ˜¯æˆ‘ä»¬ä¸‹ä¸€ç¯‡è¦å­¦ä¹ çš„Vision Transformer<span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>ã€‚</p>
<p>æ€»ä¹‹ï¼ŒTransformeræœ€ä¸ºä¸€ä¸ªå¼ºå¤§çš„ç½‘ç»œæ¶æ„ï¼Œå·²ç»è¢«å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸï¼Œå¹¶ä¸”ä»åœ¨ä¸æ–­å‘å±•å’Œæ¼”è¿›ä¸­ã€‚å¸Œæœ›é€šè¿‡è¿™ç¯‡æ–‡ç« ï¼Œå¤§å®¶èƒ½å¤Ÿå¯¹Transformeræœ‰ä¸€ä¸ªå…¨é¢çš„äº†è§£ï¼Œå¹¶èƒ½å¤Ÿåœ¨å®é™…åº”ç”¨ä¸­çµæ´»è¿ç”¨è¿™ä¸€å¼ºå¤§çš„å·¥å…·ã€‚åœ¨2026å¹´çš„AIä¸­ï¼ŒTransformeræ— ç–‘å°†ç»§ç»­å‘æŒ¥å…¶é‡è¦ä½œç”¨ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</section>
<section id="in-the-end" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> In the end</h1>
<p>åˆ›ä½œä¸æ˜“ï¼Œå¦‚æœä½ è§‰å¾—å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿è¯·æˆ‘ <span class="hilite-teal">å–æ¯å’–å•¡/æ”¯ä»˜å®çº¢åŒ…</span>ï¼Œæ”¯æŒæˆ‘ç»§ç»­åˆ›ä½œï¼ä½ ä»¬çš„æ”¯æŒæ˜¯æˆ‘æœ€å¤§çš„åŠ¨åŠ›ï¼ :) <br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../style/AliPay.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="400"></p>
</figure>
</div>



</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-LayerNormalization2016ba" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>â€œLayer <span>Normalization</span>.â€</span> July 21, 2016. <a href="https://doi.org/10.48550/arXiv.1607.06450">https://doi.org/10.48550/arXiv.1607.06450</a>.
</div>
<div id="ref-FlashAttentionFastMemoryEfficient2022dao" class="csl-entry" role="listitem">
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. <span>â€œ<span>FlashAttention</span>: <span>Fast</span> and <span>Memory-Efficient Exact Attention</span> with <span>IO-Awareness</span>.â€</span> June 23, 2022. <a href="https://doi.org/10.48550/arXiv.2205.14135">https://doi.org/10.48550/arXiv.2205.14135</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>â€œAn <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.â€</span> June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-SwitchTransformersScaling2022fedus" class="csl-entry" role="listitem">
Fedus, William, Barret Zoph, and Noam Shazeer. 2022. <span>â€œSwitch <span>Transformers</span>: <span>Scaling</span> to <span>Trillion Parameter Models</span> with <span>Simple</span> and <span>Efficient Sparsity</span>.â€</span> June 16, 2022. <a href="https://doi.org/10.48550/arXiv.2101.03961">https://doi.org/10.48550/arXiv.2101.03961</a>.
</div>
<div id="ref-DeepResidualLearning2015he" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>â€œDeep <span>Residual Learning</span> for <span>Image Recognition</span>.â€</span> December 10, 2015. <a href="https://doi.org/10.48550/arXiv.1512.03385">https://doi.org/10.48550/arXiv.1512.03385</a>.
</div>
<div id="ref-jordan2024muon" class="csl-entry" role="listitem">
Jordan, Keller, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. 2024. <span>â€œMuon: An Optimizer for Hidden Layers in Neural Networks.â€</span> <a href="https://kellerjordan.github.io/posts/muon/">https://kellerjordan.github.io/posts/muon/</a>.
</div>
<div id="ref-AdamMethodStochastic2017kingma" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>â€œAdam: <span>A Method</span> for <span>Stochastic Optimization</span>.â€</span> January 30, 2017. <a href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</a>.
</div>
<div id="ref-DecoupledWeightDecay2019loshchilov" class="csl-entry" role="listitem">
Loshchilov, Ilya, and Frank Hutter. 2019. <span>â€œDecoupled <span>Weight Decay Regularization</span>.â€</span> January 4, 2019. <a href="https://doi.org/10.48550/arXiv.1711.05101">https://doi.org/10.48550/arXiv.1711.05101</a>.
</div>
<div id="ref-NeuralMachineTranslation2016sennrich" class="csl-entry" role="listitem">
Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. <span>â€œNeural <span>Machine Translation</span> of <span>Rare Words</span> with <span>Subword Units</span>.â€</span> June 10, 2016. <a href="https://doi.org/10.48550/arXiv.1508.07909">https://doi.org/10.48550/arXiv.1508.07909</a>.
</div>
<div id="ref-RoFormerEnhancedTransformer2023su" class="csl-entry" role="listitem">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. <span>â€œ<span>RoFormer</span>: <span>Enhanced Transformer</span> with <span>Rotary Position Embedding</span>.â€</span> November 8, 2023. <a href="https://doi.org/10.48550/arXiv.2104.09864">https://doi.org/10.48550/arXiv.2104.09864</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>â€œAttention <span>Is All You Need</span>.â€</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-NativeSparseAttention2025yuan" class="csl-entry" role="listitem">
Yuan, Jingyang, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, et al. 2025. <span>â€œNative <span>Sparse Attention</span>: <span>Hardware-Aligned</span> and <span>Natively Trainable Sparse Attention</span>.â€</span> February 27, 2025. <a href="https://doi.org/10.48550/arXiv.2502.11089">https://doi.org/10.48550/arXiv.2502.11089</a>.
</div>
<div id="ref-RootMeanSquare2019zhang" class="csl-entry" role="listitem">
Zhang, Biao, and Rico Sennrich. 2019. <span>â€œRoot <span>Mean Square Layer Normalization</span>.â€</span> October 16, 2019. <a href="https://doi.org/10.48550/arXiv.1910.07467">https://doi.org/10.48550/arXiv.1910.07467</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yyzhang2025\.github\.io\/LearningNotes");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/100-AI-Papers/100_Papers_index.html" class="pagination-link" aria-label="About this series">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">About this series</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="pagination-link" aria-label="002: Vision Transformer">
        <span class="nav-page-text">002: Vision Transformer</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with â¤ï¸ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>