<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Vision Transformer (ViT) 通过将图像切分为 Patch 并直接应用标准 Transformer 架构，实现了图像分类任务。本文介绍了 ViT 的核心组件，包括 Patch Embedding、Position Embedding、[CLS] Token 以及 Transformer 编码器块，探讨了 ViT 相较于传统 CNN 的归纳偏置差异(Inductive Bias)，并展示了 ViT 在大规模数据集上的优异表现。">

<title>02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer) – Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-c0eab5a31fbea23c8affb95fb4fbb9c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // 可访问性（键盘）
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html">Vision Transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../../logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/sta210-s22" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">100 AI Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a></li>
  <li><a href="#vision-transformer" id="toc-vision-transformer" class="nav-link" data-scroll-target="#vision-transformer"><span class="header-section-number">2</span> Vision-Transformer</a>
  <ul>
  <li><a href="#patch-embedding" id="toc-patch-embedding" class="nav-link" data-scroll-target="#patch-embedding"><span class="header-section-number">2.1</span> Patch Embedding</a></li>
  <li><a href="#position-encoding" id="toc-position-encoding" class="nav-link" data-scroll-target="#position-encoding"><span class="header-section-number">2.2</span> Position Encoding</a>
  <ul>
  <li><a href="#position-interpolation" id="toc-position-interpolation" class="nav-link" data-scroll-target="#position-interpolation"><span class="header-section-number">2.2.1</span> Position Interpolation</a></li>
  </ul></li>
  <li><a href="#cls-tokens-mlp-head" id="toc-cls-tokens-mlp-head" class="nav-link" data-scroll-target="#cls-tokens-mlp-head"><span class="header-section-number">2.3</span> <code>[CLS]</code> Tokens &amp; MLP Head</a></li>
  <li><a href="#transformer-encoder-block" id="toc-transformer-encoder-block" class="nav-link" data-scroll-target="#transformer-encoder-block"><span class="header-section-number">2.4</span> Transformer Encoder Block</a>
  <ul>
  <li><a href="#multi-heads-attention" id="toc-multi-heads-attention" class="nav-link" data-scroll-target="#multi-heads-attention"><span class="header-section-number">2.4.1</span> Multi-Heads Attention</a></li>
  </ul></li>
  <li><a href="#cnn-vs.-vit-inductive-bias" id="toc-cnn-vs.-vit-inductive-bias" class="nav-link" data-scroll-target="#cnn-vs.-vit-inductive-bias"><span class="header-section-number">2.5</span> CNN vs.&nbsp;ViT: Inductive bias</a></li>
  <li><a href="#vit-model-variants" id="toc-vit-model-variants" class="nav-link" data-scroll-target="#vit-model-variants"><span class="header-section-number">2.6</span> ViT Model Variants</a></li>
  <li><a href="#experiment" id="toc-experiment" class="nav-link" data-scroll-target="#experiment"><span class="header-section-number">2.7</span> Experiment</a>
  <ul>
  <li><a href="#optimizer-loss-function" id="toc-optimizer-loss-function" class="nav-link" data-scroll-target="#optimizer-loss-function"><span class="header-section-number">2.7.1</span> Optimizer &amp; Loss Function</a></li>
  <li><a href="#result" id="toc-result" class="nav-link" data-scroll-target="#result"><span class="header-section-number">2.7.2</span> Result</a></li>
  <li><a href="#training-recipe" id="toc-training-recipe" class="nav-link" data-scroll-target="#training-recipe"><span class="header-section-number">2.7.3</span> Training Recipe</a></li>
  <li><a href="#training-summary" id="toc-training-summary" class="nav-link" data-scroll-target="#training-summary"><span class="header-section-number">2.7.4</span> Training Summary</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#others" id="toc-others" class="nav-link" data-scroll-target="#others"><span class="header-section-number">3</span> Others</a>
  <ul>
  <li><a href="#self-supervised-pre-training" id="toc-self-supervised-pre-training" class="nav-link" data-scroll-target="#self-supervised-pre-training"><span class="header-section-number">3.1</span> Self-Supervised Pre-Training</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">4</span> Key Concepts</a></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa"><span class="header-section-number">5</span> Q&amp;A</a>
  <ul>
  <li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1"><span class="header-section-number">5.1</span> Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2"><span class="header-section-number">5.2</span> Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3"><span class="header-section-number">5.3</span> Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4"><span class="header-section-number">5.4</span> Question 4</a></li>
  <li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5"><span class="header-section-number">5.5</span> Question 5</a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6"><span class="header-section-number">5.6</span> Question 6</a></li>
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7"><span class="header-section-number">5.7</span> Question 7</a></li>
  <li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8"><span class="header-section-number">5.8</span> Question 8</a></li>
  </ul></li>
  <li><a href="#related-resource-further-reading" id="toc-related-resource-further-reading" class="nav-link" data-scroll-target="#related-resource-further-reading"><span class="header-section-number">6</span> Related resource &amp; Further Reading</a>
  <ul>
  <li><a href="#减少tokens的技巧" id="toc-减少tokens的技巧" class="nav-link" data-scroll-target="#减少tokens的技巧"><span class="header-section-number">6.1</span> 减少Tokens的技巧</a>
  <ul>
  <li><a href="#patch-merge" id="toc-patch-merge" class="nav-link" data-scroll-target="#patch-merge"><span class="header-section-number">6.1.1</span> Patch Merge</a></li>
  <li><a href="#pixel-shuffle" id="toc-pixel-shuffle" class="nav-link" data-scroll-target="#pixel-shuffle"><span class="header-section-number">6.1.2</span> Pixel Shuffle</a></li>
  </ul></li>
  <li><a href="#vision-language-model" id="toc-vision-language-model" class="nav-link" data-scroll-target="#vision-language-model"><span class="header-section-number">6.2</span> Vision Language Model</a></li>
  <li><a href="#video-transformer" id="toc-video-transformer" class="nav-link" data-scroll-target="#video-transformer"><span class="header-section-number">6.3</span> Video Transformer</a></li>
  <li><a href="#native-resolution" id="toc-native-resolution" class="nav-link" data-scroll-target="#native-resolution"><span class="header-section-number">6.4</span> Native Resolution</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">7</span> Appendix</a>
  <ul>
  <li><a href="#axial-attention轴向注意力" id="toc-axial-attention轴向注意力" class="nav-link" data-scroll-target="#axial-attention轴向注意力"><span class="header-section-number">7.1</span> Axial Attention（轴向注意力）</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html">Vision Transformer</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (<tag style="color:blue">Vision-Transformer</tag>)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Computer Vision</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    Vision Transformer (ViT) 通过将图像切分为 Patch 并直接应用标准 Transformer 架构，实现了图像分类任务。本文介绍了 ViT 的核心组件，包括 <code>Patch Embedding</code>、<code>Position Embedding</code>、<code>[CLS] Token</code> 以及 Transformer 编码器块，探讨了 ViT 相较于传统 CNN 的归纳偏置差异(Inductive Bias)，并展示了 ViT 在大规模数据集上的优异表现。
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="preliminary" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preliminary</h1>
<p>在阅读本章之前，我们需要先了解一下什么是Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>。 如果有不熟悉的同学，欢迎阅读我们 100 Paper with Code 系列的第一篇:<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">01: Attention is all you need (Transformer)</a></p>
</section>
<section id="vision-transformer" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Vision-Transformer</h1>
<p>在了解了什么是<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>之后，我们来看看如何将Transformer应用于Computer Vision。Vision Transformer（ViT）<span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span> 是一个将Transformer架构应用于图像分类的模型。它的核心思想是<u>将图像划分为小块（patches），然后将这些小块视为序列数据，类似于处理文本数据</u>。</p>
<p>它的整体架构以及工作流程，如下图所示:</p>
<div id="fig-vit-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vit-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/vit.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vit-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Vision Transformer Architecture (Image Source: <a href="https://github.com/lucidrains/vit-pytorch">lucidrains</a>)
</figcaption>
</figure>
</div>
<p>我们可以看到，ViT的整体流程可以分为以下几个步骤:</p>
<ol type="1">
<li>图像切分（Patchify）: 将输入图像划分为若干大小相同的小块（Patch），并展开为一维序列。</li>
<li>线性映射（Linear Projection）: 使用Linear Layer将每个 Patch 映射为固定维度的隐藏向量（Hidden Embedding）。</li>
<li>加入 <code>[CLS]</code> 标记: 在序列开头添加一个特殊的 <code>[CLS]</code> token，用于表示整张图像的<strong>全局语义</strong>。</li>
<li>位置编码: 为每个向量添加可学习的位置嵌入（Position Embedding），保留空间位置信息。</li>
<li>Transformer Encoder: 将上述序列输入 Transformer Encoder，以捕捉全局依赖关系。</li>
<li>分类头（MLP Head）: 最终通过一个多层感知机（MLP）分类头输出图像所属的类别。</li>
</ol>
<p>相对于 <a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a> ，ViT 的主要区别在于:</p>
<ol type="1">
<li>增加了一个 Patchify 步骤，将图像划分为若干小块并转化为序列输入；</li>
<li>将原本的正余弦位置编码（Sinusoidal position embedding）替换为可学习的位置嵌入（Learned position embedding）；</li>
<li>在最后额外添加了一个分类头（Classification head），用于完成图像分类任务。</li>
</ol>
<p>我们之前已经学习过了什么是<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>建议忘记了的同学再去回顾一下，我们就不多重复了。 我们首先来看如何进行Patch Embedding</p>
<section id="patch-embedding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="patch-embedding"><span class="header-section-number">2.1</span> Patch Embedding</h2>
<p>在<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>这一篇，我们了解到，它是作用于<strong>Sequence Modeling</strong>的，很显然，Image不是 Sequence的, 它有长<span class="math inline">\(H\)</span>和宽<span class="math inline">\(W\)</span>。很直观的第一种想法就是，将图片直接展开，从二维 <span class="math inline">\((3, H, W)\)</span> 展开成一维的 <span class="math inline">\((3 \times H \times W)\)</span>. 这样我们就得到的图片的 <strong>Sequence Model</strong>。如下图 <a href="#fig-flat-image" class="quarto-xref">Figure&nbsp;2</a> 所示</p>
<div id="fig-flat-image" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flat-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/iGPT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flat-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of flattening an image into a sequence (Image Source: <a href="https://openai.com/research/image-gpt">iGPT</a>)
</figcaption>
</figure>
</div>
<p>这种方法有一种明显的问题就是:<span style="color:rgb(255, 0, 0)">Sequence的长度太长</span>。 举个例子，对于 <span class="math inline">\(3\times 256 \times 256\)</span> 的图片，我们有 <span class="math inline">\(256 \times 256 = 65,336\)</span> 个tokens，通过这种方法，所需要的训练时长很长 (在<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> 这一节，我们了解过，Attention的时间复杂度是 <span class="math inline">\(\mathcal{O}(n^{2}d)\)</span>)。 除了这个问题，另一个问题是:<tag style="color:red">它没有用到图片的特性</tag>: <em>相邻的pixel 之间，是有很高的correlation</em>。</p>
<p>所以我们很自然的想到:如果把相邻的pixels和在一组，组成一个patch，这样不就既减少了tokens的数量，又用到了pixel之间的correlation。这就是Vision Transformer的Patch Embedding组件。</p>
<blockquote class="blockquote">
<p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image <span class="math inline">\(x \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened 2D patches<span class="math inline">\(x \in \mathbb{R}^{N \times (P^{2} \times C)}\)</span>, where (<span class="math inline">\(H, W\)</span>) is the resolution of the original image, <span class="math inline">\(C\)</span> is the number of channels, (<span class="math inline">\(P, P\)</span>) is the resolution of each image patch, and <span class="math inline">\(N = HW/P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. <u>The Transformer uses constant latent vector size <span class="math inline">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math inline">\(D\)</span> dimensions with a trainable linear projection</u>. We refer to the output of this projection as the <strong>patch embeddings</strong>. <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 </cite></p>
</blockquote>
<p>有了Patch之后，我们就将图片从 <span class="math inline">\((H, W, C)\)</span> 变成了 <span class="math inline">\((N, P, P, C)\)</span>， 其中 <span class="math inline">\(N = \frac{H \times W}{P^{2}}\)</span>。 我们来看看代码怎么实现:</p>
<div class="sourceCode" id="cb1" data-code-line-numbers="8,9,10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load Image and resize it to certain size</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>image_path <span class="op">=</span> IMAGE_PATH</span>
<span id="cb1-3"><a href="#cb1-3"></a>img_bgr <span class="op">=</span> cv2.imread(image_path)</span>
<span id="cb1-4"><a href="#cb1-4"></a>img_resized <span class="op">=</span> cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation<span class="op">=</span>cv2.INTER_AREA)</span>
<span id="cb1-5"><a href="#cb1-5"></a>img <span class="op">=</span> cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) </span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Patchify </span></span>
<span id="cb1-8"><a href="#cb1-8"></a>patches <span class="op">=</span> einops.rearrange( </span>
<span id="cb1-9"><a href="#cb1-9"></a>     img, <span class="st">"(h ph) (w pw) c -&gt; (h w) ph pw c"</span>, ph<span class="op">=</span>PATCH_SIZE, pw<span class="op">=</span>PATCH_SIZE </span>
<span id="cb1-10"><a href="#cb1-10"></a>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>通过这个Patchify之后，我们将图片分成了 <span class="math inline">\(\left( \frac{H}{P} \times \frac{W}{P}, P, P, C \right)\)</span> 个Patches</p>
<div id="fig-illustrate-patchify" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-illustrate-patchify-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="assets/before-patch.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="assets/after-patch.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-illustrate-patchify-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Illustration of Patchify
</figcaption>
</figure>
</div>
<p>接下来，我们将每个Patch 展开成一个向量，变成 <span class="math inline">\((N, P^{2} \times C)\)</span>，然后传入一个Linear Layer，将其映射到一个隐藏空间，变成 <span class="math inline">\((N, d_{model})\)</span>。 这样，我们就得到了Transformer可以接受的输入。</p>
<div class="sourceCode" id="cb2" data-code-line-numbers="1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>flat_patch <span class="op">=</span> einops.rearrange( patches, <span class="st">"n ph pw c -&gt; n (ph pw c)"</span>) </span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>mlp <span class="op">=</span> nn.Linear(PATCH_SIZE <span class="op">*</span> PATCH_SIZE <span class="op">*</span> <span class="dv">3</span>, d_model)</span>
<span id="cb2-4"><a href="#cb2-4"></a>patch_embedding <span class="op">=</span> mlp(flat_patch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>通过这种方法，我们就得到了Transformer可以接受的任意长度的输入。不过在实际操作中，我们并不会用以上的方式，而是用一个卷积层来实现这个Patch Embedding的过程。原因有二:</p>
<ol type="1">
<li><strong>效率更高</strong>: 将Patchify + Flatten + Linear 合成一个卷积层，可以减少中间的内存读写，提高计算效率。</li>
<li><strong>代码更简洁</strong>: 用一个卷积层就可以实现所有的功能，代码量更少，更易读懂。</li>
</ol>
<p>如果我们用一个 卷积层，参数设置为:</p>
<ul>
<li>kernel_size = PATCH_SIZE （卷积核覆盖一个 patch）</li>
<li>stride = PATCH_SIZE （不重叠地移动，相当于切 patch）</li>
<li>in_channels = 3（RGB）</li>
<li>out_channels = d_model</li>
</ul>
<p>那么卷积会:</p>
<ol type="1">
<li>把输入图片分成 <code>PATCH_SIZE x PATCH_SIZE</code> 的不重叠块（因为 stride = kernel_size）。</li>
<li>对每个 patch 做一次线性映射（因为<strong>卷积本质上就是对局部区域做加权求和</strong>，相当于 Linear）。</li>
<li>输出的 shape 自动就是 (batch, num_patches, d_model)。</li>
</ol>
<p>这正好等价于 切 patch + flatten + Linear 的组合.</p>
<p>代码如下:</p>
<div class="sourceCode" id="cb3" data-code-line-numbers="8,9,10,11,12,13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> PatchEmbedder(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.num_patches_per_side <span class="op">=</span> config.image_size <span class="op">//</span> config.patch_size</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> <span class="va">self</span>.num_patches_per_side<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Conv2d( </span>
<span id="cb3-9"><a href="#cb3-9"></a>            in_channels<span class="op">=</span>config.num_channels, </span>
<span id="cb3-10"><a href="#cb3-10"></a>            out_channels<span class="op">=</span>config.d_model, </span>
<span id="cb3-11"><a href="#cb3-11"></a>            kernel_size<span class="op">=</span>config.patch_size, </span>
<span id="cb3-12"><a href="#cb3-12"></a>            stride<span class="op">=</span>config.patch_size, </span>
<span id="cb3-13"><a href="#cb3-13"></a>        ) </span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb3-16"><a href="#cb3-16"></a>        <span class="co"># x: (B, C, H, W)</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)  <span class="co"># (B, D, H/P, W/P)</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)  <span class="co"># (B, D, N)</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, N, D)</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>用卷积的好处，除了可以更高效的实现Patch Embedding，代码更加简洁之外，我们还可以通过改变 <code>stride</code> 来使一些Patch Overlapping，获得一个多尺度的结构。（尽管这个在ViT中没有提到，但是我觉得我们可以利用这一点）</p>
<div class="callout-tldr">
<p><tag style="color:blue">TAKEAWAY:</tag> <br> ViT 的核心改动只有两步:<strong>patchify（切块）+ embedding（线性投影）</strong>，其余几乎就是原封不动的 Transformer Encoder</p>
</div>
<p>可以补一句点明:<strong>“ViT 原版用 non-overlap patch（更纯），后续很多工作会用 overlap/conv stem 来补 inductive bias”</strong>，把读者往后续论文自然引过去。</p>
<p>我们计算一下，通过这种方法，可以减少多少Tokens的数量:</p>
<ul>
<li>图像大小 <span class="math inline">\(224 \times 224\)</span>，<span class="math inline">\(P=16\)</span> → <span class="math inline">\(N= \frac{224\times 224}{16 ^{ 2}}=196\)</span></li>
<li><span class="math inline">\(P=8\)</span> → <span class="math inline">\(N= \frac{224\times 224}{8 ^{ 2}}=784\)</span>（注意力矩阵变成 16 倍）</li>
<li>所以 <strong>patch size / token 数直接决定训练的效率</strong></li>
</ul>
</section>
<section id="position-encoding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="position-encoding"><span class="header-section-number">2.2</span> Position Encoding</h2>
<p>将图片转化为 Transformer 的输入之后，接下来Transformer中的另一个组件就是传入 Position Information。 我们知道在<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> 中，他们用的是 <strong>Sine-cosine position embedding</strong>，在那篇文章中，我们也提到了，还存在其他不同的Position Encoding的办法，ViT用的就是另一种办法，<strong>Learned Position Embedding</strong>。Learned Position Embedding的方法很简单，也很好理解，<u>对于每一个位置，我们给他一个index，将这个index传入一个 Embedding Matrix， 我们就得到一个Position Embedding</u>。不过与Token Embedding不同的是，我们会用到所有的Position，也整个matrix， 所以我们不用定index，直接定义整个Embedding，然后将它传入Transformer中。</p>
<div class="sourceCode" id="cb4" data-code-line-numbers="5,6,7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> PosEmbedder(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> nn.Parameter( </span>
<span id="cb4-6"><a href="#cb4-6"></a>            torch.randn(<span class="dv">1</span>, (config.image_size <span class="op">//</span> config.patch_size) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, config.d_model) </span>
<span id="cb4-7"><a href="#cb4-7"></a>        ) <span class="co"># +1 for cls token </span></span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, config.d_model))</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb4-12"><a href="#cb4-12"></a>        B <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb4-13"><a href="#cb4-13"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(B, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.position_embeddings</span>
<span id="cb4-17"><a href="#cb4-17"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout-question" title="为什么ViT要用Learned Position Embedding？">
<p>为什么ViT要用Learned Position Embedding呢？在ViT这篇文章中，他们尝试过不同的Position Embedding，比如:</p>
<ul>
<li>No Positional Information</li>
<li>1-dimensional Positional Embedding</li>
<li>2-dimensional Positional Embedding</li>
<li>Relative Positional Embedding</li>
</ul>
<p>发现，除了No Positional Information之外，其余3种在Image Classification中的表现，都是差不多的。</p>
<div id="fig-positional-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/position-encoding-exp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Compare different Position Encoding methods in Image Classification Task
</figcaption>
</figure>
</div>
<p>论文中表示，可能是因为所需要的Position的信息较小，对于不同种类的Position Embedding的方法，学习这个Position Information的能力，都是差不多的。</p>
<blockquote class="blockquote">
<p>We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, <em>in patch-level inputs, the spatial dimensions are much smaller</em> than the original pixel-level inputs, e.g., <span class="math inline">\(14 \times 14\)</span> as opposed to <span class="math inline">\(224 \times 224\)</span>, and <em>learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.</em> <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 </cite></p>
</blockquote>
<p>不过，尽管Position的方法不重要，但是不同的训练参数，还是会影响到学习到的Position Information, 下图所示:</p>
<div id="fig-position-diff-hyper" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-diff-hyper-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/pos-info-hyper.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-diff-hyper-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of different Position Information learned under different hyper-parameters.
</figcaption>
</figure>
</div>
</div>
<section id="position-interpolation" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="position-interpolation"><span class="header-section-number">2.2.1</span> Position Interpolation</h3>
<p>当我们有了一个Pre-Training的模型，我们想用它Fine-Tuning到一个不同图片大小的数据库，我们改怎么做呢?</p>
<p>第一个方法当然是，<em>Resize 我们的图片</em>，到ViT Pre-training的图片大小，但是，这个能导致较大的图片，失去很多细节。如果我们想保持图片的大小不变，同时让模型训练，我们就需要Extend Position Encoding，因为当Patch Size不变，图片大小变了的话，产生的Number of Patches 也是会改变的，我们需要做的是: 找到一种方法，增大或者减小Position的数量。 这就是所谓的<strong>Position Interpolation</strong>。</p>
<blockquote class="blockquote">
<p>The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform <strong>2D interpolation</strong> of the pre-trained position embeddings, according to their location in the original image <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 </cite></p>
</blockquote>
<p>我们来看看代码是怎么实现Position Interpolation的:</p>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> interpolate_pos_encoding(<span class="va">self</span>, x, w, h):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    npatch <span class="op">=</span> x.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    N <span class="op">=</span> <span class="va">self</span>.pos_embed.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="cf">if</span> npatch <span class="op">==</span> N <span class="kw">and</span> w <span class="op">==</span> h:</span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_embed</span>
<span id="cb5-6"><a href="#cb5-6"></a>    class_pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed[:, <span class="dv">0</span>]</span>
<span id="cb5-7"><a href="#cb5-7"></a>    patch_pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed[:, <span class="dv">1</span>:]</span>
<span id="cb5-8"><a href="#cb5-8"></a>    dim <span class="op">=</span> x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-9"><a href="#cb5-9"></a>    w0 <span class="op">=</span> w <span class="op">//</span> <span class="va">self</span>.patch_embed.patch_size</span>
<span id="cb5-10"><a href="#cb5-10"></a>    h0 <span class="op">=</span> h <span class="op">//</span> <span class="va">self</span>.patch_embed.patch_size</span>
<span id="cb5-11"><a href="#cb5-11"></a>    </span>
<span id="cb5-12"><a href="#cb5-12"></a>    patch_pos_embed <span class="op">=</span> F.interpolate(</span>
<span id="cb5-13"><a href="#cb5-13"></a>        patch_pos_embed.reshape(</span>
<span id="cb5-14"><a href="#cb5-14"></a>            <span class="dv">1</span>, </span>
<span id="cb5-15"><a href="#cb5-15"></a>            <span class="bu">int</span>(math.sqrt(N)), </span>
<span id="cb5-16"><a href="#cb5-16"></a>            <span class="bu">int</span>(math.sqrt(N)), </span>
<span id="cb5-17"><a href="#cb5-17"></a>            dim</span>
<span id="cb5-18"><a href="#cb5-18"></a>        ).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb5-19"><a href="#cb5-19"></a>        scale_factor<span class="op">=</span>(w0 <span class="op">/</span> math.sqrt(N), h0 <span class="op">/</span> math.sqrt(N)),</span>
<span id="cb5-20"><a href="#cb5-20"></a>        mode<span class="op">=</span><span class="st">'bicubic'</span>,</span>
<span id="cb5-21"><a href="#cb5-21"></a>    )</span>
<span id="cb5-22"><a href="#cb5-22"></a>    patch_pos_embed <span class="op">=</span> patch_pos_embed.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, dim)</span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="cf">return</span> torch.cat((class_pos_embed.unsqueeze(<span class="dv">0</span>), patch_pos_embed), dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout-tldr">
<p><tag style="color:blue">TAKEAWAY:</tag> <br> 2D interpolation of the pre-trained position embeddings</p>
<ul>
<li>ViT 在预训练时，通常用固定输入分辨率（比如 224×224） → 生成固定数量的 patch（比如 16×16 patch → 196 个 patch）。</li>
<li>但在 fine-tuning 时，输入图片可能大小不一样，比如 384×384，这时 patch 数量就变了。</li>
<li>这会导致原本的 位置编码 (position embeddings) 和新的 patch 数量对不上。</li>
<li>解决办法:对预训练好的位置编码做 二维插值 (2D interpolation)，根据 patch 在原图中的空间位置，把位置编码拉伸/缩放到新的分辨率。</li>
</ul>
</div>
</section>
</section>
<section id="cls-tokens-mlp-head" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="cls-tokens-mlp-head"><span class="header-section-number">2.3</span> <code>[CLS]</code> Tokens &amp; MLP Head</h2>
<p>在 <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> 这一节，我们了解到:每输入一个token，Transformer会输出对应的token。这就是说，对于每个patch，Transformer会输出对应的Tokens，那么，我们应该选择哪一个token作为我们图片的表示呢。 BERT <span class="citation" data-cites="BERTPretrainingDeep2019devlin">(<a href="#ref-BERTPretrainingDeep2019devlin" role="doc-biblioref">Devlin et al. 2019</a>)</span>， 用了一个 <code>[CLS]</code>, 来表示一个句子。同理，我们也可以添加一个 <code>[CLS]</code> token, 来表示一张图片。同时，对于 <code>[CLS]</code> token, 我们也要在给他一个表示位置的信息。这就是为什么在Position Encoding上，我们有 <code>(config.image_size // config.patch_size) ** 2 + 1,</code> 位置信息，其中 <code>+1</code> 就是 <code>[CLS]</code> 的位置信息。</p>
<p>总结一下 <code>[CLS]</code> token 的作用就是用来聚合所有的Patch的消息，然后用来Image 的Representation。</p>
<p>我们想一下，除了加一个 <code>[CLS]</code> token，之外，我们还有其他办法来表示图片吗。有一种很自然的方法就是，将所有的patch的消息收集起来，然后去一个平均值来表示这个图片。类似于传统的ConvNet(e.g.&nbsp;ResNet) 我们可以通过 <code>AvgPooling</code> 来实现。 不过论文中提到， 对于两种不同的Image Representation，需要有不同的Learning Rate 来训练这个网络。 通过下图，我们看到，不用的收集信息的方法，需要不同的learning rate <img src="assets/gap-vs-cls-learning-rate.png" id="fig-gap-vs-cls-lr" class="img-fluid"></p>
<div class="callout-tldr">
<p>结构上二者都可行，但需要不同 LR / recipe。 实践里很多实现默认用 <code>CLS</code>（与 BERT 对齐、下游更统一），也有用 GAP 的变体比如我们接下来要学的 Swin Transformer <span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span></p>
</div>
<p>有了Image Represent之后，我们只需要将这个传入一个简单的MLP，我们就可以得到一个Classifier。MLP的输入是hidden dim，输出则是我们Number of Classes。不同的Index 表示不同的Classses。</p>
<blockquote class="blockquote">
<p>An initial attempt at using only image-patch embeddings, <strong>globally average-pooling (GAP)</strong> them, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate, <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 </cite></p>
</blockquote>
<blockquote class="blockquote">
<p>Both during pre-training and fine-tuning, a classification head is attached to <span class="math inline">\(\mathrm{z}_{L}^{0}\)</span>. The classification head is implemented by a MLP with <u>one hidden layer at pre-training time and by a single linear layer at fine-tuning time</u>. <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 </cite></p>
</blockquote>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">class</span> MLPHead(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(config.d_model, config.num_classes)</span>
<span id="cb6-6"><a href="#cb6-6"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_rate)</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb6-9"><a href="#cb6-9"></a>        cls <span class="op">=</span> x[:, <span class="dv">0</span>, :]</span>
<span id="cb6-10"><a href="#cb6-10"></a>        cls <span class="op">=</span> <span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc1(cls)))</span>
<span id="cb6-11"><a href="#cb6-11"></a>        cls <span class="op">=</span> <span class="va">self</span>.fc2(cls)</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>        <span class="cf">return</span> cls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="transformer-encoder-block" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="transformer-encoder-block"><span class="header-section-number">2.4</span> Transformer Encoder Block</h2>
<p>至此，我们已经讲完了ViT, 与<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a>的主要不同之处。接下来，就是Transformer的Encoder。 <img src="assets/ViT-Encoder.png" id="fig-vit-encoder" class="img-fluid"></p>
<p>这部分，和Transformer原本的Encoder很类似，只不过有几处不同:</p>
<ul>
<li><strong>Pre-Norm</strong>: 在ViT同，输入先进行一个LayerNorm，然后在传入MHA或者MLP中，反观在Transformer原本的Encoder中，我们是先将MHA或者MLP的输出与输入加在一起，之后再进行一个Normalization。这叫做<strong>Post-Norm</strong></li>
<li>MLP的实现:在Transformer Encoder中，用的是 <code>ReLU</code>, 而在ViT中，用的是 <code>GELU</code></li>
</ul>
<p>除此之外，其他部分都是一样的。一下是ViT Encoder的实现:</p>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a>        <span class="va">self</span>.mha <span class="op">=</span> MHA(config)</span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb7-10"><a href="#cb7-10"></a>        attn, _ <span class="op">=</span> <span class="va">self</span>.mha(<span class="va">self</span>.norm1(x))</span>
<span id="cb7-11"><a href="#cb7-11"></a>        x <span class="op">=</span> x <span class="op">+</span> attn</span>
<span id="cb7-12"><a href="#cb7-12"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.norm2(x))</span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="multi-heads-attention" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="multi-heads-attention"><span class="header-section-number">2.4.1</span> Multi-Heads Attention</h3>
<p>还有一个就是Attention模块，Attention模块与<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a>中的是一模一样，在这里就不过多的赘述了。</p>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> MHA(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb8-3"><a href="#cb8-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_heads</span>
<span id="cb8-5"><a href="#cb8-5"></a>        <span class="va">self</span>.d_model <span class="op">=</span> config.d_model</span>
<span id="cb8-6"><a href="#cb8-6"></a>        <span class="cf">assert</span> config.d_model <span class="op">%</span> config.num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a>        <span class="va">self</span>.qkv_linear <span class="op">=</span> nn.Linear(config.d_model, config.d_model <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb8-10"><a href="#cb8-10"></a>        <span class="va">self</span>.out_linear <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>        <span class="va">self</span>.attention_dropout <span class="op">=</span> nn.Dropout(config.attention_dropout_rate)</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb8-15"><a href="#cb8-15"></a>        B, N, C <span class="op">=</span> x.shape  <span class="co"># Batch size, Number of tokens, Embedding dimension</span></span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a>        q, k, v <span class="op">=</span> (</span>
<span id="cb8-18"><a href="#cb8-18"></a>            <span class="va">self</span>.qkv_linear(x).reshape(B, N, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>).unbind(<span class="dv">0</span>)</span>
<span id="cb8-19"><a href="#cb8-19"></a>        )</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a>        scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> torch.sqrt(</span>
<span id="cb8-22"><a href="#cb8-22"></a>            torch.tensor(<span class="va">self</span>.d_k, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-23"><a href="#cb8-23"></a>        )  <span class="co"># (B, num_heads, N, N)</span></span>
<span id="cb8-24"><a href="#cb8-24"></a>        attn_weight <span class="op">=</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, num_heads, N, N)</span></span>
<span id="cb8-25"><a href="#cb8-25"></a>        attn <span class="op">=</span> <span class="va">self</span>.attention_dropout(attn_weight)</span>
<span id="cb8-26"><a href="#cb8-26"></a></span>
<span id="cb8-27"><a href="#cb8-27"></a>        context <span class="op">=</span> torch.matmul(attn, v)  <span class="co"># (B, num_heads, N, d_k)</span></span>
<span id="cb8-28"><a href="#cb8-28"></a>        context <span class="op">=</span> context.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N, C)  <span class="co"># (B, N, d_model)</span></span>
<span id="cb8-29"><a href="#cb8-29"></a></span>
<span id="cb8-30"><a href="#cb8-30"></a>        out <span class="op">=</span> <span class="va">self</span>.out_linear(context)  <span class="co"># (B, N, d_model)</span></span>
<span id="cb8-31"><a href="#cb8-31"></a></span>
<span id="cb8-32"><a href="#cb8-32"></a>        <span class="cf">return</span> out, attn_weight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="cnn-vs.-vit-inductive-bias" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="cnn-vs.-vit-inductive-bias"><span class="header-section-number">2.5</span> CNN vs.&nbsp;ViT: Inductive bias</h2>
<p>至此，我们已经介绍完了Vision Transformer，我们来从Inductive Bias的方面，看看 CNN 和 ViT 有什么不同</p>
<div class="callout callout-style-default callout-note callout-titled" title="什么是Inductive Bias">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
什么是Inductive Bias
</div>
</div>
<div class="callout-body-container callout-body">
<p>在深度学习里，Inductive Bias（归纳偏置）是指模型在学习之前，因<strong>结构或设计而自带的假设或先验</strong> ，比如 Convolution Layer, 它就是假设相邻的pixel之间，是有一定联系的，因此可以用一个Kernel来将学习这些关系。</p>
</div>
</div>
<p>对于图像来说，常见的先验就是:</p>
<ul>
<li>局部像素是相关的（locality）</li>
<li>相邻区域的模式有规律（2D neighborhood）</li>
<li>物体无论出现在图像哪里，识别方式应该一样（Translation Equivariance）</li>
</ul>
<p>那么，CNN 的结构怎么体现这些偏置？ 1. 局部性 (Locality): - 卷积核（例如 3×3）只和局部像素打交道，而不是全图。 - 这意味着模型“相信”图像的重要特征来自局部邻域，而不是遥远区域。 2. 二维邻域结构 (2D structure): - 卷积操作是沿着 图像的二维网格进行的，天然利用了图像的行列结构。 - 这和文本（序列 1D）不一样，CNN 明确知道输入是 2D 排列的。 3. 平移等变性 (Translation equivariance): - 卷积核的参数在整张图共享。 - 所以猫在左上角还是右下角，卷积核都能检测到“猫耳朵”。 - 这让 CNN 自动具有“识别位置无关”的能力。</p>
<p>这些性质不是模型通过训练学出来的，而是因为卷积操作本身的数学结构就带来的（也是我们人为设计的）:</p>
<ul>
<li>kernel 的局部连接 → 局部性</li>
<li>kernel 滑动覆盖全图 → 平移等变性</li>
<li>操作在二维空间定义 → 邻域结构</li>
</ul>
<p>所以，哪怕我们不给 CNN 喂太多数据，它也会利用这些偏置去学习特征。</p>
<p>而对于 ViT 来说，其归纳偏置非常弱，几乎完全依赖数据和训练来学习，不过它也有利用了一些图片的Inductive Bias:</p>
<ol type="1">
<li>Patch 切分 (Patchification) • ViT 唯一的“图像先验”之一就是把输入图片切成 patch。 • 这一操作隐含了:图像是一个二维结构，可以被分块处理。</li>
<li>位置编码 (Positional Embeddings) • Transformer 本身只处理序列，没有空间结构的概念。 • ViT 通过加位置编码告诉模型 patch 在图像中的相对位置。 • 在输入分辨率变化时，会做 二维插值 (2D interpolation) 来适配，这也是一种人工引入的 2D 先验。</li>
<li>其他部分 • 除了以上两点，ViT 的注意力机制是 全局的 (global)，没有局部性约束。 • 没有像 CNN 那样内置的平移等变性或局部邻域结构。</li>
</ol>
<p>这样就是为什么ViT需要更多数据和计算才能学到同样的空间归纳规律。</p>
</section>
<section id="vit-model-variants" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="vit-model-variants"><span class="header-section-number">2.6</span> ViT Model Variants</h2>
<p>ViT 有3种不同的基本变形， 如下图所示</p>
<p><img src="assets/Vision-Transformer-ViT-variance.png" class="img-fluid"></p>
<p>ViT的名字通常表示为: ViT-L/16: 意思是，ViT-Large，然后用的16 Patch Size。 需要注意的是，Patch Size越大，我们得到的tokens就越少，也就是需要更少的训练时实现, 但通常需要更大的图片来训练。</p>
</section>
<section id="experiment" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="experiment"><span class="header-section-number">2.7</span> Experiment</h2>
<p>我们先来看看原论文是如何训练的: - <strong>预训练（所有模型，包括 ResNet）</strong>:作者统一使用 Adam（<span class="math inline">\(\beta_{1} = 0.9, \beta_{2}=0.999\)</span>），batch size = 4096，并使用较大的 weight decay = 0.1。作者指出这对所有模型的迁移表现都有帮助 - <strong>学习率策略</strong>:采用线性 warmup + 线性 decay（细节见 Appendix B.1）。 - <strong>微调（fine-tuning）</strong>:对所有模型统一改用带 momentum 的 SGD，batch size 512. - <strong>权重平均</strong>:同时使用 Polyak averaging（指数滑动平均，系数 0.9999）以进一步提升效果。</p>
<p>接下来，我们来训练我们定义的ViT。具体的代码可以在 <a href="https://github.com/YYZhang2025/100-AI-Code/blob/main/02_ViT.ipynb">这里</a> 查看。 ### Dataset 我们用CIFAT-10的训练集来训练ViT。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>我们之前提到了ViT要在大规模的数据集上才可以发挥它的能力，由于资源有限，我们只在这展示ViT的训练流程，在了解了这个训练流程之后，很容易拓展到其他的大数据集。</p>
</div>
</div>
<div class="sourceCode" id="cb9" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>IMG_MEAN <span class="op">=</span> [<span class="fl">0.4914</span>, <span class="fl">0.4822</span>, <span class="fl">0.4465</span>]</span>
<span id="cb9-2"><a href="#cb9-2"></a>IMG_STD <span class="op">=</span> [<span class="fl">0.2470</span>, <span class="fl">0.2435</span>, <span class="fl">0.2616</span>]</span>
<span id="cb9-3"><a href="#cb9-3"></a>IMG_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>data_transform <span class="op">=</span> transforms.Compose(</span>
<span id="cb9-7"><a href="#cb9-7"></a>    [</span>
<span id="cb9-8"><a href="#cb9-8"></a>        transforms.Resize(</span>
<span id="cb9-9"><a href="#cb9-9"></a>            (IMG_SIZE, IMG_SIZE),</span>
<span id="cb9-10"><a href="#cb9-10"></a>            interpolation<span class="op">=</span>transforms.InterpolationMode.BILINEAR,</span>
<span id="cb9-11"><a href="#cb9-11"></a>        ),</span>
<span id="cb9-12"><a href="#cb9-12"></a>        transforms.RandomHorizontalFlip(),</span>
<span id="cb9-13"><a href="#cb9-13"></a>        transforms.ToTensor(),</span>
<span id="cb9-14"><a href="#cb9-14"></a>        transforms.Normalize(IMG_MEAN, IMG_STD),</span>
<span id="cb9-15"><a href="#cb9-15"></a>    ]</span>
<span id="cb9-16"><a href="#cb9-16"></a>)</span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb9-19"><a href="#cb9-19"></a>    root<span class="op">=</span>train_config.data_dir, download<span class="op">=</span><span class="va">True</span>, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>data_transform</span>
<span id="cb9-20"><a href="#cb9-20"></a>)</span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="cf">if</span> train_config.debug:</span>
<span id="cb9-22"><a href="#cb9-22"></a>    train_dataset <span class="op">=</span> torch.utils.data.Subset(train_dataset, <span class="bu">range</span>(<span class="dv">1000</span>))</span>
<span id="cb9-23"><a href="#cb9-23"></a></span>
<span id="cb9-24"><a href="#cb9-24"></a></span>
<span id="cb9-25"><a href="#cb9-25"></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb9-26"><a href="#cb9-26"></a>    train_dataset,</span>
<span id="cb9-27"><a href="#cb9-27"></a>    batch_size<span class="op">=</span>train_config.batch_size,</span>
<span id="cb9-28"><a href="#cb9-28"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-29"><a href="#cb9-29"></a>    num_workers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb9-30"><a href="#cb9-30"></a>    pin_memory<span class="op">=</span><span class="va">True</span> <span class="cf">if</span> train_config.device.<span class="bu">type</span> <span class="op">==</span> <span class="st">"cuda"</span> <span class="cf">else</span> <span class="va">False</span>,</span>
<span id="cb9-31"><a href="#cb9-31"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="optimizer-loss-function" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="optimizer-loss-function"><span class="header-section-number">2.7.1</span> Optimizer &amp; Loss Function</h3>
<p>论文中应用了 Adam Optimizer，我们在此也用Adam。因为我们训练的是Image Classification Task，所以损失函数是是Cross Entropy Loss:</p>
<div class="sourceCode" id="cb10" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb10-2"><a href="#cb10-2"></a>optimizer <span class="op">=</span> torch.optim.Adam(</span>
<span id="cb10-3"><a href="#cb10-3"></a>    model.parameters(),</span>
<span id="cb10-4"><a href="#cb10-4"></a>    lr<span class="op">=</span>train_config.lr,</span>
<span id="cb10-5"><a href="#cb10-5"></a>    betas<span class="op">=</span>train_config.betas,</span>
<span id="cb10-6"><a href="#cb10-6"></a>    weight_decay<span class="op">=</span>train_config.weight_decay,</span>
<span id="cb10-7"><a href="#cb10-7"></a>)</span>
<span id="cb10-8"><a href="#cb10-8"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(</span>
<span id="cb10-9"><a href="#cb10-9"></a>    optimizer, train_config.num_epochs, eta_min<span class="op">=</span>train_config.min_lr</span>
<span id="cb10-10"><a href="#cb10-10"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="result" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="result"><span class="header-section-number">2.7.2</span> Result</h3>
<p><img src="assets/loss-curve.png" class="img-fluid"></p>
<p><img src="assets/acc-curve.png" class="img-fluid"></p>
<p>我们来看看，这个Toy ViT 究竟训练的怎么样:</p>
<div class="sourceCode" id="cb11" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>N_ROWS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>N_COLS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>N_IMGS <span class="op">=</span> N_ROWS <span class="op">*</span> N_COLS</span>
<span id="cb11-4"><a href="#cb11-4"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb11-5"><a href="#cb11-5"></a>    root<span class="op">=</span>train_config.data_dir,</span>
<span id="cb11-6"><a href="#cb11-6"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-7"><a href="#cb11-7"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb11-8"><a href="#cb11-8"></a>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>IDX_TO_CLASS <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> test_dataset.class_to_idx.items()}</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a>org_imgs, true_labels <span class="op">=</span> test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]</span>
<span id="cb11-12"><a href="#cb11-12"></a>transformed_imgs <span class="op">=</span> torch.stack([data_transform(Image.fromarray(img)) <span class="cf">for</span> img <span class="kw">in</span> org_imgs])</span>
<span id="cb11-13"><a href="#cb11-13"></a>pred_labels <span class="op">=</span> model(tensor_to_device(transformed_imgs, device<span class="op">=</span>train_config.device)).argmax(dim<span class="op">=</span><span class="dv">1</span>).cpu()</span>
<span id="cb11-14"><a href="#cb11-14"></a></span>
<span id="cb11-15"><a href="#cb11-15"></a></span>
<span id="cb11-16"><a href="#cb11-16"></a>fig, axes <span class="op">=</span> plt.subplots(N_ROWS, N_COLS, figsize<span class="op">=</span>(N_COLS <span class="op">*</span> <span class="dv">2</span>, N_ROWS <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_ROWS):</span>
<span id="cb11-18"><a href="#cb11-18"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N_COLS):</span>
<span id="cb11-19"><a href="#cb11-19"></a>        idx <span class="op">=</span> i <span class="op">*</span> N_COLS <span class="op">+</span> j</span>
<span id="cb11-20"><a href="#cb11-20"></a>        axes[i, j].imshow(org_imgs[idx])</span>
<span id="cb11-21"><a href="#cb11-21"></a>        axes[i, j].axis(<span class="st">"off"</span>)</span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="cf">if</span> true_labels[idx] <span class="op">==</span> pred_labels[idx].item():</span>
<span id="cb11-23"><a href="#cb11-23"></a>            axes[i, j].set_title(</span>
<span id="cb11-24"><a href="#cb11-24"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb11-25"><a href="#cb11-25"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-26"><a href="#cb11-26"></a>                color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb11-27"><a href="#cb11-27"></a>            )</span>
<span id="cb11-28"><a href="#cb11-28"></a>        <span class="cf">else</span>:</span>
<span id="cb11-29"><a href="#cb11-29"></a>            axes[i, j].set_title(</span>
<span id="cb11-30"><a href="#cb11-30"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb11-31"><a href="#cb11-31"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-32"><a href="#cb11-32"></a>                color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb11-33"><a href="#cb11-33"></a>            )</span>
<span id="cb11-34"><a href="#cb11-34"></a>plt.tight_layout()</span>
<span id="cb11-35"><a href="#cb11-35"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="assets/vis-pred.png" class="img-fluid"></p>
<p>我们也可以看看Attention Map的</p>
<div class="sourceCode" id="cb12" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>N_ROWS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>N_COLS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>N_IMGS <span class="op">=</span> N_ROWS <span class="op">*</span> N_COLS</span>
<span id="cb12-4"><a href="#cb12-4"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb12-5"><a href="#cb12-5"></a>    root<span class="op">=</span>train_config.data_dir,</span>
<span id="cb12-6"><a href="#cb12-6"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-7"><a href="#cb12-7"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-8"><a href="#cb12-8"></a>)</span>
<span id="cb12-9"><a href="#cb12-9"></a>IDX_TO_CLASS <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> test_dataset.class_to_idx.items()}</span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a>org_imgs, true_labels <span class="op">=</span> test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]</span>
<span id="cb12-12"><a href="#cb12-12"></a>transformed_imgs <span class="op">=</span> torch.stack([data_transform(Image.fromarray(img)) <span class="cf">for</span> img <span class="kw">in</span> org_imgs]).to(</span>
<span id="cb12-13"><a href="#cb12-13"></a>    train_config.device</span>
<span id="cb12-14"><a href="#cb12-14"></a>)</span>
<span id="cb12-15"><a href="#cb12-15"></a></span>
<span id="cb12-16"><a href="#cb12-16"></a><span class="co"># pred_labels = model(tensor_to_device(transformed_imgs, device=train_config.device)).argmax(dim=1).cpu()</span></span>
<span id="cb12-17"><a href="#cb12-17"></a>TARGET_LAYER <span class="op">=</span> <span class="dv">5</span>  <span class="co"># 0-based index</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>HEAD <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-19"><a href="#cb12-19"></a>GAMMA <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>FLOOR <span class="op">=</span> <span class="fl">0.15</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a><span class="kw">def</span> to_vit_attention_vis(img_uint8, attn_map, gamma<span class="op">=</span><span class="fl">0.7</span>, floor<span class="op">=</span><span class="fl">0.15</span>):</span>
<span id="cb12-24"><a href="#cb12-24"></a>    <span class="co">"""</span></span>
<span id="cb12-25"><a href="#cb12-25"></a><span class="co">    gamma:     &gt;0, smaller -&gt; sharper spotlight</span></span>
<span id="cb12-26"><a href="#cb12-26"></a><span class="co">    floor:     how visible the dark region is (0 = pure black background)</span></span>
<span id="cb12-27"><a href="#cb12-27"></a><span class="co">    """</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>    img <span class="op">=</span> img_uint8.astype(np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb12-29"><a href="#cb12-29"></a></span>
<span id="cb12-30"><a href="#cb12-30"></a>    <span class="co"># normalize attention to [0,1]</span></span>
<span id="cb12-31"><a href="#cb12-31"></a>    a <span class="op">=</span> attn_map.astype(np.float32)</span>
<span id="cb12-32"><a href="#cb12-32"></a>    a <span class="op">=</span> a <span class="op">-</span> a.<span class="bu">min</span>()</span>
<span id="cb12-33"><a href="#cb12-33"></a>    a <span class="op">=</span> a <span class="op">/</span> (a.<span class="bu">max</span>() <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-35"><a href="#cb12-35"></a>    <span class="co"># make it more "spotlight-like"</span></span>
<span id="cb12-36"><a href="#cb12-36"></a>    a <span class="op">=</span> a<span class="op">**</span>gamma  <span class="co"># sharpen</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>    a <span class="op">=</span> floor <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> floor) <span class="op">*</span> a  <span class="co"># keep some visibility outside</span></span>
<span id="cb12-38"><a href="#cb12-38"></a>    a <span class="op">=</span> np.clip(a, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb12-39"><a href="#cb12-39"></a></span>
<span id="cb12-40"><a href="#cb12-40"></a>    <span class="cf">return</span> img <span class="op">*</span> a[..., <span class="va">None</span>]  <span class="co"># darken outside attention</span></span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a></span>
<span id="cb12-43"><a href="#cb12-43"></a>model.<span class="bu">eval</span>()</span>
<span id="cb12-44"><a href="#cb12-44"></a>x <span class="op">=</span> model.backbone.patch_embedder(transformed_imgs)</span>
<span id="cb12-45"><a href="#cb12-45"></a>x <span class="op">=</span> model.backbone.pos_embedder(x)</span>
<span id="cb12-46"><a href="#cb12-46"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(TARGET_LAYER):</span>
<span id="cb12-47"><a href="#cb12-47"></a>    x <span class="op">=</span> model.backbone.encoder_layers[i](x)</span>
<span id="cb12-48"><a href="#cb12-48"></a></span>
<span id="cb12-49"><a href="#cb12-49"></a>_, attn_weights <span class="op">=</span> model.backbone.encoder_layers[TARGET_LAYER].mha(</span>
<span id="cb12-50"><a href="#cb12-50"></a>    model.backbone.encoder_layers[TARGET_LAYER].norm1(x)</span>
<span id="cb12-51"><a href="#cb12-51"></a>)</span>
<span id="cb12-52"><a href="#cb12-52"></a><span class="cf">if</span> HEAD <span class="op">&gt;=</span> <span class="dv">0</span>:</span>
<span id="cb12-53"><a href="#cb12-53"></a>    attn_weights <span class="op">=</span> attn_weights[:, HEAD, <span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># (B, N)</span></span>
<span id="cb12-54"><a href="#cb12-54"></a><span class="cf">else</span>:</span>
<span id="cb12-55"><a href="#cb12-55"></a>    attn_weights <span class="op">=</span> attn_weights.mean(dim<span class="op">=</span><span class="dv">1</span>)[:, <span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># (B, N)</span></span>
<span id="cb12-56"><a href="#cb12-56"></a></span>
<span id="cb12-57"><a href="#cb12-57"></a><span class="co"># Reshape attention weights to (B, H, W)</span></span>
<span id="cb12-58"><a href="#cb12-58"></a>num_patches_per_side <span class="op">=</span> model_config.image_size <span class="op">//</span> model_config.patch_size</span>
<span id="cb12-59"><a href="#cb12-59"></a>attn_maps <span class="op">=</span> attn_weights.reshape(<span class="op">-</span><span class="dv">1</span>, num_patches_per_side, num_patches_per_side)  <span class="co"># (B, H, W)</span></span>
<span id="cb12-60"><a href="#cb12-60"></a></span>
<span id="cb12-61"><a href="#cb12-61"></a><span class="co"># Upsample attention maps to image size</span></span>
<span id="cb12-62"><a href="#cb12-62"></a>attn_maps_upsampled <span class="op">=</span> F.interpolate(</span>
<span id="cb12-63"><a href="#cb12-63"></a>    attn_maps.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-64"><a href="#cb12-64"></a>    size<span class="op">=</span>(model_config.image_size, model_config.image_size),</span>
<span id="cb12-65"><a href="#cb12-65"></a>    mode<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb12-66"><a href="#cb12-66"></a>    align_corners<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-67"><a href="#cb12-67"></a>).squeeze(<span class="dv">1</span>)  <span class="co"># (B, H, W)</span></span>
<span id="cb12-68"><a href="#cb12-68"></a></span>
<span id="cb12-69"><a href="#cb12-69"></a><span class="co"># Move attention map to CPU for visualization</span></span>
<span id="cb12-70"><a href="#cb12-70"></a>attn_maps_upsampled <span class="op">=</span> attn_maps_upsampled.cpu().detach().numpy()</span>
<span id="cb12-71"><a href="#cb12-71"></a></span>
<span id="cb12-72"><a href="#cb12-72"></a><span class="co"># Visualize Overlaid Attention Maps</span></span>
<span id="cb12-73"><a href="#cb12-73"></a>fig, axes <span class="op">=</span> plt.subplots(N_ROWS, N_COLS, figsize<span class="op">=</span>(N_COLS <span class="op">*</span> <span class="dv">2</span>, N_ROWS <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb12-74"><a href="#cb12-74"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_ROWS):</span>
<span id="cb12-75"><a href="#cb12-75"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N_COLS):</span>
<span id="cb12-76"><a href="#cb12-76"></a>        idx <span class="op">=</span> i <span class="op">*</span> N_COLS <span class="op">+</span> j</span>
<span id="cb12-77"><a href="#cb12-77"></a>        vis <span class="op">=</span> to_vit_attention_vis(</span>
<span id="cb12-78"><a href="#cb12-78"></a>            org_imgs[idx],</span>
<span id="cb12-79"><a href="#cb12-79"></a>            attn_maps_upsampled[idx],</span>
<span id="cb12-80"><a href="#cb12-80"></a>            gamma<span class="op">=</span>GAMMA,</span>
<span id="cb12-81"><a href="#cb12-81"></a>            floor<span class="op">=</span>FLOOR,</span>
<span id="cb12-82"><a href="#cb12-82"></a>        )</span>
<span id="cb12-83"><a href="#cb12-83"></a></span>
<span id="cb12-84"><a href="#cb12-84"></a>        axes[i, j].imshow(vis)</span>
<span id="cb12-85"><a href="#cb12-85"></a>        axes[i, j].axis(<span class="st">"off"</span>)</span>
<span id="cb12-86"><a href="#cb12-86"></a>        <span class="cf">if</span> true_labels[idx] <span class="op">==</span> pred_labels[idx].item():</span>
<span id="cb12-87"><a href="#cb12-87"></a>            axes[i, j].set_title(</span>
<span id="cb12-88"><a href="#cb12-88"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb12-89"><a href="#cb12-89"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-90"><a href="#cb12-90"></a>                color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb12-91"><a href="#cb12-91"></a>            )</span>
<span id="cb12-92"><a href="#cb12-92"></a>        <span class="cf">else</span>:</span>
<span id="cb12-93"><a href="#cb12-93"></a>            axes[i, j].set_title(</span>
<span id="cb12-94"><a href="#cb12-94"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb12-95"><a href="#cb12-95"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-96"><a href="#cb12-96"></a>                color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb12-97"><a href="#cb12-97"></a>            )</span>
<span id="cb12-98"><a href="#cb12-98"></a>plt.tight_layout()</span>
<span id="cb12-99"><a href="#cb12-99"></a>plt.suptitle(</span>
<span id="cb12-100"><a href="#cb12-100"></a>    <span class="ss">f"Attention Maps Over Images On Layer </span><span class="sc">{</span>TARGET_LAYER<span class="sc">}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'Head '</span> <span class="op">+</span> <span class="bu">str</span>(HEAD) <span class="cf">if</span> HEAD <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'Avg Head'</span><span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb12-101"><a href="#cb12-101"></a>    y<span class="op">=</span><span class="fl">1.02</span>,</span>
<span id="cb12-102"><a href="#cb12-102"></a>)</span>
<span id="cb12-103"><a href="#cb12-103"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="assets/attn.png" class="img-fluid"></p>
</section>
<section id="training-recipe" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="training-recipe"><span class="header-section-number">2.7.3</span> Training Recipe</h3>
<p>接下来我提供几个可能的提升Accuracy的方法（由于时间现实，暂时没有能实现，有兴趣的读者欢迎自行尝试）:</p>
<ul>
<li><strong>优化器</strong>:AdamW（而非 Adam）+ weight decay</li>
<li><strong>学习率策略</strong>:warmup + cosine，batch size 对 LR 的线性缩放规则</li>
<li><strong>增强</strong>:RandAugment / Mixup / CutMix</li>
<li><strong>正则</strong>:DropPath（stochastic depth）、Label smoothing</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="ViT on small data（ImageNet-1k级别）常用 recipe">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ViT on small data（ImageNet-1k级别）常用 recipe
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>AdamW + cosine + warmup + strong aug（RA/Mixup/CutMix）+ DropPath + label smoothing</p>
</section>
<section id="training-summary" class="level3" data-number="2.7.4">
<h3 data-number="2.7.4" class="anchored" data-anchor-id="training-summary"><span class="header-section-number">2.7.4</span> Training Summary</h3>
<p>从结果来看，这个ViT表现的并不是很好，甚至不如简单的Convolution Layer。不过这种结果是在我们预料之中的，因为我们的数据量太少了，ViT还不能从数据中学到有效的信息。 不过出乎我意料的是，Attention Map几乎是平均的， 我们期待的是，类似于论文中的Attention Map。</p>
<p><img src="assets/Vision-Transformer-attention-map.png" class="img-fluid"></p>
<p>其中一个解释就是，我们训练的图片太小了，<span class="math inline">\(32 \times 32\)</span>，导致每个Token收集到的信息很平均，这就导致了Attention Map看起来在每个地方都是一样。</p>
<p>解决办法就是:</p>
<ol type="1">
<li>用Grad-CAM<span class="citation" data-cites="GradCAMVisualExplanations2020selvaraju">(<a href="#ref-GradCAMVisualExplanations2020selvaraju" role="doc-biblioref">Selvaraju et al. 2020</a>)</span> 或者 是 Attention Rollout<a href="@QuantifyingAttentionFlow2020abnar.qmd"><span class="citation" data-cites="QuantifyingAttentionFlow2020abnar">Abnar and Zuidema (<span>2020</span>)</span></a></li>
<li>提高图片的Resolution，比如用ImageNet来训练</li>
</ol>
<p>在这里就不具体展开了，有兴趣的同学自行查看。</p>
<p>从实验结果来看，这个 toy ViT 的表现确实不算理想，甚至不如一个简单的卷积网络。这其实在预期之内:<u>ViT 的归纳偏置更弱（缺少卷积的局部性与平移等变性），在数据量较小、训练 recipe 不够强的情况下更容易欠拟合或泛化不足</u>，因此很难在 CIFAR-10 这类小规模数据上占到便宜。</p>
<p>比较“反直觉”的是，我们可视化得到的 <strong>Attention Map 几乎接近均匀分布</strong>，而不是像原论文那样呈现出更清晰的语义聚焦（例如对物体区域的注意力更强）:</p>
<p><img src="assets/Vision-Transformer-attention-map.png" class="img-fluid"></p>
<p>一种合理的解释是:我们的输入分辨率只有 (<span class="math inline">\(32\times32\)</span>)，在常见 patch 设置下 token 数量非常有限（例如 (<span class="math inline">\(P=4\)</span>) 时也只有 (<span class="math inline">\(8\times8=64\)</span>) 个 patch）。在这种低分辨率、低 token 数的设定里，每个 token 覆盖的区域相对“粗”，并且早期训练阶段模型往往更倾向于学习全局平均的相关性来最小化损失，导致注意力权重看起来更平均。另一个注意的点就是:<strong>单层注意力权重本身未必等价于“可解释性”</strong>，即使模型在做出正确决策，也可能出现注意力图不够尖锐的现象。</p>
<p>如果希望得到更有信息量的可解释结果，通常有两条更稳妥的路径:</p>
<ol type="1">
<li>使用更可靠的可解释方法，例如 <strong>Grad-CAM</strong> <span class="citation" data-cites="GradCAMVisualExplanations2020selvaraju">(<a href="#ref-GradCAMVisualExplanations2020selvaraju" role="doc-biblioref">Selvaraju et al. 2020</a>)</span>，或者结合多层注意力的 <strong>Attention Rollout</strong> <span class="citation" data-cites="QuantifyingAttentionFlow2020abnar">(<a href="#ref-QuantifyingAttentionFlow2020abnar" role="doc-biblioref">Abnar and Zuidema 2020</a>)</span>，而不是只观察某一层/某一头的 attention。</li>
<li>提高输入分辨率与训练规模（例如在 ImageNet 或更大数据上训练/预训练后再迁移），让模型有机会学习到更细粒度的空间结构与更稳定的语义对齐。</li>
</ol>
<p>这里就不展开实现细节了，有兴趣的同学可以根据上述论文进一步尝试与对比。</p>
</section>
</section>
</section>
<section id="others" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Others</h1>
<section id="self-supervised-pre-training" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="self-supervised-pre-training"><span class="header-section-number">3.1</span> Self-Supervised Pre-Training</h2>
<p>除了做 Image Classification，ViT 团队也尝试了自监督预训练（Self-Supervised Pre-Training）。他们采用了一种非常“类似于BERT<span class="citation" data-cites="BERTPretrainingDeep2019devlin">(<a href="#ref-BERTPretrainingDeep2019devlin" role="doc-biblioref">Devlin et al. 2019</a>)</span>”的思路:<strong>Masked Patch Prediction</strong>——先把图像切成 patch tokens，然后随机“腐蚀（corrupt）”一部分 token，让模型去预测被腐蚀部分的内容。</p>
<div class="callout-tldr">
<p>核心是把 ViT 当成“视觉版 BERT”:随机遮住（或替换）一部分 patch token，再预测被遮住 patch 的目标（这里用离散颜色作为预测标签）。</p>
</div>
<p><img src="assets/Vision-Transformer-Self-Supervised.png" class="img-fluid"></p>
<p>他们对 <strong>50% 的 patch embedding</strong> 做 corruption，并采用与 BERT 类似的 80/10/10 策略: - 80%:用一个可学习的 <code>[mask]</code> embedding 替换 - 10%:替换成另一块随机 patch 的 embedding - 10%:保持不变（但仍然作为预测目标）</p>
<p>这种设计的直觉是:既要让模型学会“根据上下文补全缺失信息”，又要避免模型过度依赖某一种固定的 mask 模式。</p>
<p>他们最终选择了一个非常轻量但有效的预测目标:<br>
对每个被腐蚀 patch，预测其 <strong>3-bit mean color</strong>（一共 <span class="math inline">\(2^9 = 512\)</span> 种颜色，也就是一个 <strong>512-way 分类问题</strong>）。</p>
<p>同时他们也对比过几种目标设定: 1) 只预测一个 mean 3-bit color（512 分类，<strong>1 个预测</strong>）<br>
2) 把 16×16 patch 下采样成 4×4，再对每个小格预测 3-bit color（512 分类，<strong>16 个并行预测</strong>）<br>
3) 直接对完整 patch 做像素级 L2 回归（RGB 通道上的密集回归）</p>
<p>结果比较有意思:三种方式都能带来不错效果，但 <strong>像素 L2 回归略差</strong>.</p>
<blockquote class="blockquote">
<p>We employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable <code>[mask]</code> embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%). This setup is very similar to the one used for language by BERT. Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.14 </cite></p>
</blockquote>
<p>后续的工作也说明了这种方法的可行性，比如SiMIM <span class="citation" data-cites="SimMIMSimpleFramework2022xie">(<a href="#ref-SimMIMSimpleFramework2022xie" role="doc-biblioref">Xie et al. 2022</a>)</span></p>
</section>
</section>
<section id="key-concepts" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Key Concepts</h1>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Content</th>
<th>Explain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Patchify</td>
<td>把 <span class="math inline">\((H\times W)\)</span> 图像切成 <span class="math inline">\((P\times P)\)</span> 小块并展平，得到 token 序列，是 ViT 的核心“视觉→序列”接口。</td>
</tr>
<tr class="even">
<td>Patch Embedding</td>
<td>用线性投影 (<span class="math inline">\(E\)</span>) 把每个 patch 从 <span class="math inline">\((P^2C)\)</span> 映射到 Transformer 维度 (<span class="math inline">\(D\)</span>)。</td>
</tr>
<tr class="odd">
<td>[CLS] Token</td>
<td>在序列前加入可学习 token，用其输出表示整图并接分类头。</td>
</tr>
<tr class="even">
<td>Positional Embedding</td>
<td>ViT 用可学习 1D 位置编码；分辨率变化时对位置编码做 2D 插值以适配新 token 网格。</td>
</tr>
<tr class="odd">
<td>Pre-LN Transformer Encoder</td>
<td>LayerNorm 放在子层前，配合残差:MSA 和 MLP 交替堆叠。</td>
</tr>
<tr class="even">
<td>Inductive Bias</td>
<td>CNN 的局部性/平移等变性；ViT 归纳偏置更弱，更依赖数据规模与训练配方。</td>
</tr>
<tr class="odd">
<td>Model Scaling (B/L/H)</td>
<td>Base/Large/Huge 三档参数与深度，patch 越小序列越长、计算更贵。</td>
</tr>
</tbody>
</table>
</section>
<section id="qa" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Q&amp;A</h1>
<section id="question-1" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="question-1"><span class="header-section-number">5.1</span> Question 1</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 1:</span> <strong>为什么 Vision Transformer 需要大规模预训练数据？</strong></p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>因为 ViT <strong>缺乏卷积神经网络中的归纳偏置（inductive bias）</strong>，例如局部性（locality）和平移不变性（translation equivariance），这些能力需要通过大量数据来学习。</p>
</div>
</div>
</div>
</section>
<section id="question-2" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="question-2"><span class="header-section-number">5.2</span> Question 2</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 2</span>:Eq.(1) 里为什么要加 [CLS] token，它和全局平均池化有什么区别？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>[CLS] token 给模型一个“专门聚合信息的槽位”，通过注意力主动从所有 patch 拉取信息；而 GAP 是被动平均。论文在附录对比过两者表现接近，但学习率等配方可能需要不同调整。<a href="https://arxiv.org/pdf/2010.11929">arXiv+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-3" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="question-3"><span class="header-section-number">5.3</span> Question 3</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 3</span>:ViT 用 1D position embedding 不会丢掉 2D 结构吗？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>丢掉了“显式 2D 归纳偏置”，但作者发现更复杂的 2D-aware 位置编码并没有带来显著收益；ViT 依靠数据与训练从头学习空间关系。真正需要 2D 的地方主要在<strong>分辨率迁移时的位置编码插值</strong>。<a href="https://arxiv.org/pdf/2010.11929">arXiv+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-4" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="question-4"><span class="header-section-number">5.4</span> Question 4</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 4</span>:为什么论文强调“规模训练胜过归纳偏置”？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>论文实验显示:小数据预训练时强 CNN（如 BiT ResNet）更稳；随着预训练数据从 ImageNet → ImageNet-21k → JFT-300M 增大，ViT 大模型的迁移性能显著提升并反超 CNN，说明对 ViT 而言数据规模是关键瓶颈。<a href="https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/attention/transformers/transformers_for_image_recognition%2C_dosovitskiy%2C_arxiv_2020.pdf">UofT Computer Science+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-5" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="question-5"><span class="header-section-number">5.5</span> Question 5</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 5</span>:patch size 选 16 还是 32 的主要权衡是什么？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>patch 越小（如 16）→ token 数 NNN 越大 → 注意力更贵但细粒度更强；patch 越大（如 32）→ 更省算力但可能损失细节。论文也明确指出序列长度与 P2P^2P2 成反比，因此小 patch 更昂贵。</p>
</div>
</div>
</div>
</section>
<section id="question-6" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="question-6"><span class="header-section-number">5.6</span> Question 6</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 6</span>:为什么 ViT 在小数据集上通常不如 CNN？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>因为 CNN 通过卷积和权重共享内置了强先验，而 ViT 需要从数据中学习这些先验，在小数据条件下不够高效。</p>
</div>
</div>
</div>
</section>
<section id="question-7" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="question-7"><span class="header-section-number">5.7</span> Question 7</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 7</span>:分辨率微调时为什么要对位置编码做 2D 插值？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>因为 token 网格大小变了（NNN 变了），预训练的 EposE_{pos}Epos​ 不能直接对齐新位置；2D 插值让位置编码在空间上“平滑伸缩”，从而复用预训练知识。</p>
</div>
</div>
</div>
</section>
<section id="question-8" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="question-8"><span class="header-section-number">5.8</span> Question 8</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 8</span>:为什么 ViT 需要把图像切成 patch，而不是直接把每个像素当 token？</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>像素级 token 会让序列长度变成 <span class="math inline">\(H \times W\)</span>，自注意力复杂度 <span class="math inline">\(\mathcal{O}(n^{2})\)</span> 直接爆炸；patchify 把 NNN 降到 <span class="math inline">\(\frac{HW}{P^2}\)</span>​，让标准全局注意力在可接受的算力下运行，同时保留端到端学习空间结构的能力。</p>
</div>
</div>
</div>
</section>
</section>
<section id="related-resource-further-reading" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Related resource &amp; Further Reading</h1>
<p>在了解了什么是Vision Transformer之后，我们可以看看这些还有那些提升，比如这篇文章，介绍了一些训练ViT的技巧</p>
<p><a href="https://arxiv.org/pdf/2106.10270">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</a> 论文中总节了三类改进策略:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>类别</th>
<th>代表方法</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>数据增强 (Data Augmentation)</strong></td>
<td>RandAugment, Mixup, CutMix, Random Erasing</td>
<td>提高泛化，抵消数据量不足</td>
</tr>
<tr class="even">
<td><strong>正则化 (Regularization)</strong></td>
<td>DropPath（Stochastic Depth）, Label Smoothing, Gradient Clipping</td>
<td>稳定训练、防止过拟合</td>
</tr>
<tr class="odd">
<td><strong>优化器与学习率调度</strong></td>
<td>AdamW + Cosine LR + Warmup</td>
<td>对 ViT 特别关键</td>
</tr>
</tbody>
</table>
<section id="减少tokens的技巧" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="减少tokens的技巧"><span class="header-section-number">6.1</span> 减少Tokens的技巧</h2>
<section id="patch-merge" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="patch-merge"><span class="header-section-number">6.1.1</span> Patch Merge</h3>
<p>类似于 Swin Transformer<span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span> 的做法:在不同层将相邻 patch 合并（例如 2×2 → 1），减少 token 数，使模型层级化。</p>
</section>
<section id="pixel-shuffle" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="pixel-shuffle"><span class="header-section-number">6.1.2</span> Pixel Shuffle</h3>
<p><img src="assets/Vision-Transformer-Pixel-Shuffle.png" class="img-fluid"></p>
</section>
</section>
<section id="vision-language-model" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="vision-language-model"><span class="header-section-number">6.2</span> Vision Language Model</h2>
<p>我们以及学习了ViT for computer Vision， <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> for NLP， 接下来有什么办法让这两种模型结合起来呢？ CLIP <span class="citation" data-cites="LearningTransferableVisual2021radford">(<a href="#ref-LearningTransferableVisual2021radford" role="doc-biblioref">Radford et al. 2021</a>)</span>: 将 ViT 融合到 vision-language 预训练中。我们之后会学习这篇文章。</p>
</section>
<section id="video-transformer" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="video-transformer"><span class="header-section-number">6.3</span> Video Transformer</h2>
<p>在学习了如何将Transformer应用到Image中，我们可以更近一步，看看如何将Transformer应用到Video 模态中。ViViT<span class="citation" data-cites="ViViTVideoVision2021arnab">(<a href="#ref-ViViTVideoVision2021arnab" role="doc-biblioref">Arnab et al. 2021</a>)</span> 的提出，就是将Transformer应用到Video。中，我们之后会学习到这一篇。</p>
</section>
<section id="native-resolution" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="native-resolution"><span class="header-section-number">6.4</span> Native Resolution</h2>
<p>在ViT 中，我们需要将图片Resize到相同大小的图片，那我们就想不Resize，能不能直接训“不同大小/不同长宽比”的图？ NaViT <span class="citation" data-cites="PatchPackNaViT2023dehghani">(<a href="#ref-PatchPackNaViT2023dehghani" role="doc-biblioref">Dehghani et al. 2023</a>)</span>提出每张图在“原始分辨率/原始长宽比”下切成 patch token 序列，然后把多张图的 token 序列“打包（packing）”到同一条固定长度序列里训练，用 <strong>attention mask</strong> 保证不同图片之间互不“串门”。</p>
<p><img src="assets/Vision-Transformer-NaViT.png" class="img-fluid"></p>
<ul>
<li><strong>Patch n’ Pack</strong>:把多张不同分辨率图片的 patch token 序列拼成一个定长 packed sequence，以减少 padding/resize 浪费<br>
</li>
<li><strong>Attention Mask</strong>:使用 block-diagonal mask，让同一张图的 token 才能互相 attention，不同图之间完全隔离</li>
<li><strong>位置编码</strong>:采用 factorized（x/y 分解）位置编码，并可用 fractional 坐标提升对未见分辨率的泛化</li>
</ul>
<table class="table">
<colgroup>
<col style="width: 5%">
</colgroup>
<tbody>
<tr class="odd">
<td># Summary 恭喜你看到了这里，ViT的论文架构思想很简单，但就是这种大道至简的方式，才让它这个工作变得出彩。尽管如此，细看ViT的论文，还是有很多可学习的地方。接下来，我们来回顾一下。</td>
</tr>
<tr class="even">
<td>在本章中，从已熟悉 Transformer 的前提出发，围绕 <strong>“如何将 Transformer 应用于计算机视觉”</strong> 这一核心问题，系统梳理了 Vision Transformer（ViT）的整体设计与实现思路。文章首先介绍了 ViT 的基本架构，说明其本质是将二维图像通过 <strong>Patchify</strong> 转化为一维序列，并结合 <strong>Patch Embedding、Position Embedding 与 [CLS] token</strong>，使图像能够被标准的 Transformer Encoder 处理。随后介绍了 Patch Embedding 的动机与实现:<em>从像素级直接展开所带来的序列过长与计算复杂度问题出发，引出将相邻像素组合成 patch 的必要性</em>，并给出了基于 <code>einops</code> 的直观实现以及使用 <strong>Conv2d 等价实现</strong> 的工程化写法。</td>
</tr>
<tr class="odd">
<td>在位置编码部分，对比了 ViT 中采用的 <strong>Learned Position Embedding</strong> 与传统 Transformer 的正余弦位置编码，并结合论文实验说明，在 patch-level 输入下，不同位置编码策略在图像分类任务中的性能差异并不显著；同时介绍了在 fine-tuning 到不同输入分辨率时，如何通过 <strong>二维插值（2D interpolation）</strong> 扩展预训练位置编码以适配新的 patch 数量。随后从表示学习的角度讨论了 <strong>[CLS] token</strong> 在 ViT 中的作用，并与全局平均池化（GAP）进行对比，指出两种方式的性能差异主要来源于训练时学习率等超参数设置，而非结构本身。</td>
</tr>
<tr class="even">
<td>在模型结构层面，强调了 ViT Encoder 与原始 Transformer Encoder 的关键差异，包括 <strong>Pre-Norm 设计</strong> 以及使用 <strong>GELU</strong> 激活函数。接着从 <strong>Inductive Bias</strong> 的视角对比了 CNN 与 ViT:CNN 通过卷积天然引入局部性、二维结构和平移等变性等先验，而 ViT 的归纳偏置较弱，更多依赖数据和训练过程来学习这些空间规律，这也解释了 ViT 在小数据集上表现受限、但在大规模预训练条件下潜力显著的原因。最后，通过在 CIFAR-10 上训练一个 toy ViT 的实验，展示了模型的分类效果与注意力可视化，并分析了 attention map 接近均匀的现象可能与 <strong>低分辨率和数据规模有限</strong> 有关，同时给出了进一步改进与扩展的方向，为后续更高效注意力机制以及多模态、视频模型的学习奠定基础。</td>
</tr>
</tbody>
</table>
<p>创作不易，如果你觉得内容对你有帮助，欢迎请我喝杯咖啡/支付宝红包，支持我继续创作！你们的支持是我最大的动力！ :) <br> <img src="../../../style/AliPay.jpg" class="img-fluid" width="300"></p>
</section>
</section>
<section id="appendix" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Appendix</h1>
<section id="axial-attention轴向注意力" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="axial-attention轴向注意力"><span class="header-section-number">7.1</span> Axial Attention（轴向注意力）</h2>
<p>在处理 图像或视频 这类高维输入时，如果直接对所有像素做 全局 self-attention，复杂度是 <span class="math inline">\(\mathcal{O}(H^2 W^2)\)</span>当图像很大时，这个代价太高。 核心想法:把二维 attention 拆成两次一维 attention（沿着图像的两个“轴”分别做）。 1. Row-wise Attention（行注意力） • 沿着水平方向（宽度轴 W）做注意力，每一行的像素互相关注。 • 复杂度:<span class="math inline">\(\mathcal{O}(H \cdot W^2)\)</span>。 2. Column-wise Attention（列注意力） • 沿着垂直方向（高度轴 H）做注意力，每一列的像素互相关注。 • 复杂度: <span class="math inline">\(\mathcal{O}(W \cdot H^2)\)</span>。</p>
<p>组合起来，相当于在 H 和 W 两个轴上都做了全局依赖建模。 <img src="assets/axial-attention.png" id="fig-axial-attention" class="img-fluid"></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-QuantifyingAttentionFlow2020abnar" class="csl-entry" role="listitem">
Abnar, Samira, and Willem Zuidema. 2020. <span>“Quantifying <span>Attention Flow</span> in <span>Transformers</span>.”</span> May 31, 2020. <a href="https://doi.org/10.48550/arXiv.2005.00928">https://doi.org/10.48550/arXiv.2005.00928</a>.
</div>
<div id="ref-ViViTVideoVision2021arnab" class="csl-entry" role="listitem">
Arnab, Anurag, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. 2021. <span>“<span>ViViT</span>: <span>A Video Vision Transformer</span>.”</span> November 1, 2021. <a href="https://doi.org/10.48550/arXiv.2103.15691">https://doi.org/10.48550/arXiv.2103.15691</a>.
</div>
<div id="ref-PatchPackNaViT2023dehghani" class="csl-entry" role="listitem">
Dehghani, Mostafa, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, et al. 2023. <span>“Patch n’ <span>Pack</span>: <span>NaViT</span>, a <span>Vision Transformer</span> for Any <span>Aspect Ratio</span> and <span>Resolution</span>.”</span> July 12, 2023. <a href="https://doi.org/10.48550/arXiv.2307.06304">https://doi.org/10.48550/arXiv.2307.06304</a>.
</div>
<div id="ref-BERTPretrainingDeep2019devlin" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> May 24, 2019. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.”</span> June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-SwinTransformerHierarchical2021liu" class="csl-entry" role="listitem">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin <span>Transformer</span>: <span>Hierarchical Vision Transformer</span> Using <span>Shifted Windows</span>.”</span> August 17, 2021. <a href="https://doi.org/10.48550/arXiv.2103.14030">https://doi.org/10.48550/arXiv.2103.14030</a>.
</div>
<div id="ref-LearningTransferableVisual2021radford" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning <span>Transferable Visual Models From Natural Language Supervision</span>.”</span> February 26, 2021. <a href="https://doi.org/10.48550/arXiv.2103.00020">https://doi.org/10.48550/arXiv.2103.00020</a>.
</div>
<div id="ref-GradCAMVisualExplanations2020selvaraju" class="csl-entry" role="listitem">
Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2020. <span>“Grad-<span>CAM</span>: <span>Visual Explanations</span> from <span>Deep Networks</span> via <span class="nocase">Gradient-based Localization</span>.”</span> <em>International Journal of Computer Vision</em> 128 (2): 336–59. <a href="https://doi.org/10.1007/s11263-019-01228-7">https://doi.org/10.1007/s11263-019-01228-7</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-SimMIMSimpleFramework2022xie" class="csl-entry" role="listitem">
Xie, Zhenda, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 2022. <span>“<span>SimMIM</span>: <span>A Simple Framework</span> for <span>Masked Image Modeling</span>.”</span> April 17, 2022. <a href="https://doi.org/10.48550/arXiv.2111.09886">https://doi.org/10.48550/arXiv.2111.09886</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/sta210-s22\.github\.io\/website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="pagination-link" aria-label="Transformer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Transformer</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>