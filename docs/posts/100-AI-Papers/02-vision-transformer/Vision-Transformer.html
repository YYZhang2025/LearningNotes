<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Vision Transformer (ViT) é€šè¿‡å°†å›¾åƒåˆ‡åˆ†ä¸º Patch å¹¶ç›´æ¥åº”ç”¨æ ‡å‡† Transformer æ¶æ„ï¼Œå®ç°äº†å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº† ViT çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ Patch Embeddingã€Position Embeddingã€[CLS] Token ä»¥åŠ Transformer ç¼–ç å™¨å—ï¼Œæ¢è®¨äº† ViT ç›¸è¾ƒäºä¼ ç»Ÿ CNN çš„å½’çº³åç½®å·®å¼‚(Inductive Bias)ï¼Œå¹¶å±•ç¤ºäº† ViT åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚">

<title>02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer) â€“ Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/DLFaC/index.html" rel="next">
<link href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-2408ba68f110ab238f8e696a3557ddbe.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-e8e3d5716073066d903a496f042ededd.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-2408ba68f110ab238f8e696a3557ddbe.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // å¯è®¿é—®æ€§ï¼ˆé”®ç›˜ï¼‰
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">ğŸ“š 100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html">Vision Transformer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../.././style/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sta210-s22" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="https://sta210-s22.github.io/website/" title="Course website" class="quarto-navigation-tool px-1" aria-label="Course website"><i class="bi bi-website"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">ğŸ“ Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">ğŸ“š 100 AI Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../posts/DLFaC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ğŸ“š DLFaC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../posts/DLFaC/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning Foundation and Concepts(DLFaC) Learning Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Chapter01</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/DLFaC/Chapter01/Chapter01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 01: Introduction to the Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Chapter02</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/DLFaC/Chapter02/Chapter02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 02: Probabilities &amp; Information Theory</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a></li>
  <li><a href="#vision-transformer" id="toc-vision-transformer" class="nav-link" data-scroll-target="#vision-transformer"><span class="header-section-number">2</span> Vision-Transformer</a>
  <ul>
  <li><a href="#patch-embedding" id="toc-patch-embedding" class="nav-link" data-scroll-target="#patch-embedding"><span class="header-section-number">2.1</span> Patch Embedding</a></li>
  <li><a href="#position-encoding" id="toc-position-encoding" class="nav-link" data-scroll-target="#position-encoding"><span class="header-section-number">2.2</span> Position Encoding</a>
  <ul>
  <li><a href="#position-interpolation" id="toc-position-interpolation" class="nav-link" data-scroll-target="#position-interpolation"><span class="header-section-number">2.2.1</span> Position Interpolation</a></li>
  </ul></li>
  <li><a href="#cls-tokens-mlp-head" id="toc-cls-tokens-mlp-head" class="nav-link" data-scroll-target="#cls-tokens-mlp-head"><span class="header-section-number">2.3</span> <code>[CLS]</code> Tokens &amp; MLP Head</a></li>
  <li><a href="#transformer-encoder-block" id="toc-transformer-encoder-block" class="nav-link" data-scroll-target="#transformer-encoder-block"><span class="header-section-number">2.4</span> Transformer Encoder Block</a>
  <ul>
  <li><a href="#multi-heads-attention" id="toc-multi-heads-attention" class="nav-link" data-scroll-target="#multi-heads-attention"><span class="header-section-number">2.4.1</span> Multi-Heads Attention</a></li>
  </ul></li>
  <li><a href="#cnn-vs.-vit-inductive-bias" id="toc-cnn-vs.-vit-inductive-bias" class="nav-link" data-scroll-target="#cnn-vs.-vit-inductive-bias"><span class="header-section-number">2.5</span> CNN vs.&nbsp;ViT: Inductive bias</a></li>
  <li><a href="#vit-model-variants" id="toc-vit-model-variants" class="nav-link" data-scroll-target="#vit-model-variants"><span class="header-section-number">2.6</span> ViT Model Variants</a></li>
  <li><a href="#experiment" id="toc-experiment" class="nav-link" data-scroll-target="#experiment"><span class="header-section-number">2.7</span> Experiment</a>
  <ul>
  <li><a href="#optimizer-loss-function" id="toc-optimizer-loss-function" class="nav-link" data-scroll-target="#optimizer-loss-function"><span class="header-section-number">2.7.1</span> Optimizer &amp; Loss Function</a></li>
  <li><a href="#result" id="toc-result" class="nav-link" data-scroll-target="#result"><span class="header-section-number">2.7.2</span> Result</a></li>
  <li><a href="#training-recipe" id="toc-training-recipe" class="nav-link" data-scroll-target="#training-recipe"><span class="header-section-number">2.7.3</span> Training Recipe</a></li>
  <li><a href="#training-summary" id="toc-training-summary" class="nav-link" data-scroll-target="#training-summary"><span class="header-section-number">2.7.4</span> Training Summary</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#others" id="toc-others" class="nav-link" data-scroll-target="#others"><span class="header-section-number">3</span> Others</a>
  <ul>
  <li><a href="#self-supervised-pre-training" id="toc-self-supervised-pre-training" class="nav-link" data-scroll-target="#self-supervised-pre-training"><span class="header-section-number">3.1</span> Self-Supervised Pre-Training</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">4</span> Key Concepts</a></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa"><span class="header-section-number">5</span> Q&amp;A</a>
  <ul>
  <li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1"><span class="header-section-number">5.1</span> Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2"><span class="header-section-number">5.2</span> Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3"><span class="header-section-number">5.3</span> Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4"><span class="header-section-number">5.4</span> Question 4</a></li>
  <li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5"><span class="header-section-number">5.5</span> Question 5</a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6"><span class="header-section-number">5.6</span> Question 6</a></li>
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7"><span class="header-section-number">5.7</span> Question 7</a></li>
  <li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8"><span class="header-section-number">5.8</span> Question 8</a></li>
  </ul></li>
  <li><a href="#related-resource-further-reading" id="toc-related-resource-further-reading" class="nav-link" data-scroll-target="#related-resource-further-reading"><span class="header-section-number">6</span> Related resource &amp; Further Reading</a>
  <ul>
  <li><a href="#å‡å°‘tokensçš„æŠ€å·§" id="toc-å‡å°‘tokensçš„æŠ€å·§" class="nav-link" data-scroll-target="#å‡å°‘tokensçš„æŠ€å·§"><span class="header-section-number">6.1</span> å‡å°‘Tokensçš„æŠ€å·§</a>
  <ul>
  <li><a href="#patch-merge" id="toc-patch-merge" class="nav-link" data-scroll-target="#patch-merge"><span class="header-section-number">6.1.1</span> Patch Merge</a></li>
  <li><a href="#pixel-shuffle" id="toc-pixel-shuffle" class="nav-link" data-scroll-target="#pixel-shuffle"><span class="header-section-number">6.1.2</span> Pixel Shuffle</a></li>
  </ul></li>
  <li><a href="#vision-language-model" id="toc-vision-language-model" class="nav-link" data-scroll-target="#vision-language-model"><span class="header-section-number">6.2</span> Vision Language Model</a></li>
  <li><a href="#video-transformer" id="toc-video-transformer" class="nav-link" data-scroll-target="#video-transformer"><span class="header-section-number">6.3</span> Video Transformer</a></li>
  <li><a href="#native-resolution" id="toc-native-resolution" class="nav-link" data-scroll-target="#native-resolution"><span class="header-section-number">6.4</span> Native Resolution</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">7</span> Appendix</a>
  <ul>
  <li><a href="#axial-attentionè½´å‘æ³¨æ„åŠ›" id="toc-axial-attentionè½´å‘æ³¨æ„åŠ›" class="nav-link" data-scroll-target="#axial-attentionè½´å‘æ³¨æ„åŠ›"><span class="header-section-number">7.1</span> Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">ğŸ“š 100 AI Papers</a></li><li class="breadcrumb-item"><a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html">Vision Transformer</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (<tag style="color:blue">Vision-Transformer</tag>)</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Computer Vision</div>
    <div class="quarto-category">Transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    Vision Transformer (ViT) é€šè¿‡å°†å›¾åƒåˆ‡åˆ†ä¸º Patch å¹¶ç›´æ¥åº”ç”¨æ ‡å‡† Transformer æ¶æ„ï¼Œå®ç°äº†å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº† ViT çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬ <code>Patch Embedding</code>ã€<code>Position Embedding</code>ã€<code>[CLS] Token</code> ä»¥åŠ Transformer ç¼–ç å™¨å—ï¼Œæ¢è®¨äº† ViT ç›¸è¾ƒäºä¼ ç»Ÿ CNN çš„å½’çº³åç½®å·®å¼‚(Inductive Bias)ï¼Œå¹¶å±•ç¤ºäº† ViT åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="preliminary" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Preliminary</h1>
<p>åœ¨é˜…è¯»æœ¬ç« ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€ä¸‹ä»€ä¹ˆæ˜¯Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>ã€‚ å¦‚æœæœ‰ä¸ç†Ÿæ‚‰çš„åŒå­¦ï¼Œæ¬¢è¿é˜…è¯»æˆ‘ä»¬ 100 Paper with Code ç³»åˆ—çš„ç¬¬ä¸€ç¯‡:<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">01: Attention is all you need (Transformer)</a></p>
</section>
<section id="vision-transformer" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Vision-Transformer</h1>
<p>åœ¨äº†è§£äº†ä»€ä¹ˆæ˜¯<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>ä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å°†Transformeråº”ç”¨äºComputer Visionã€‚Vision Transformerï¼ˆViTï¼‰<span class="citation" data-cites="ImageWorth16x162021dosovitskiy">(<a href="#ref-ImageWorth16x162021dosovitskiy" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span> æ˜¯ä¸€ä¸ªå°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯<u>å°†å›¾åƒåˆ’åˆ†ä¸ºå°å—ï¼ˆpatchesï¼‰ï¼Œç„¶åå°†è¿™äº›å°å—è§†ä¸ºåºåˆ—æ•°æ®ï¼Œç±»ä¼¼äºå¤„ç†æ–‡æœ¬æ•°æ®</u>ã€‚</p>
<p>å®ƒçš„æ•´ä½“æ¶æ„ä»¥åŠå·¥ä½œæµç¨‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:</p>
<div id="fig-vit-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vit-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/vit.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vit-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Vision Transformer Architecture (Image Source: <a href="https://github.com/lucidrains/vit-pytorch">lucidrains</a>)
</figcaption>
</figure>
</div>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒViTçš„æ•´ä½“æµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤:</p>
<ol type="1">
<li>å›¾åƒåˆ‡åˆ†ï¼ˆPatchifyï¼‰: å°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºè‹¥å¹²å¤§å°ç›¸åŒçš„å°å—ï¼ˆPatchï¼‰ï¼Œå¹¶å±•å¼€ä¸ºä¸€ç»´åºåˆ—ã€‚</li>
<li>çº¿æ€§æ˜ å°„ï¼ˆLinear Projectionï¼‰: ä½¿ç”¨Linear Layerå°†æ¯ä¸ª Patch æ˜ å°„ä¸ºå›ºå®šç»´åº¦çš„éšè—å‘é‡ï¼ˆHidden Embeddingï¼‰ã€‚</li>
<li>åŠ å…¥ <code>[CLS]</code> æ ‡è®°: åœ¨åºåˆ—å¼€å¤´æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„ <code>[CLS]</code> tokenï¼Œç”¨äºè¡¨ç¤ºæ•´å¼ å›¾åƒçš„<strong>å…¨å±€è¯­ä¹‰</strong>ã€‚</li>
<li>ä½ç½®ç¼–ç : ä¸ºæ¯ä¸ªå‘é‡æ·»åŠ å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼ˆPosition Embeddingï¼‰ï¼Œä¿ç•™ç©ºé—´ä½ç½®ä¿¡æ¯ã€‚</li>
<li>Transformer Encoder: å°†ä¸Šè¿°åºåˆ—è¾“å…¥ Transformer Encoderï¼Œä»¥æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚</li>
<li>åˆ†ç±»å¤´ï¼ˆMLP Headï¼‰: æœ€ç»ˆé€šè¿‡ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åˆ†ç±»å¤´è¾“å‡ºå›¾åƒæ‰€å±çš„ç±»åˆ«ã€‚</li>
</ol>
<p>ç›¸å¯¹äº <a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a> ï¼ŒViT çš„ä¸»è¦åŒºåˆ«åœ¨äº:</p>
<ol type="1">
<li>å¢åŠ äº†ä¸€ä¸ª Patchify æ­¥éª¤ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºè‹¥å¹²å°å—å¹¶è½¬åŒ–ä¸ºåºåˆ—è¾“å…¥ï¼›</li>
<li>å°†åŸæœ¬çš„æ­£ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal position embeddingï¼‰æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼ˆLearned position embeddingï¼‰ï¼›</li>
<li>åœ¨æœ€åé¢å¤–æ·»åŠ äº†ä¸€ä¸ªåˆ†ç±»å¤´ï¼ˆClassification headï¼‰ï¼Œç”¨äºå®Œæˆå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
</ol>
<p>æˆ‘ä»¬ä¹‹å‰å·²ç»å­¦ä¹ è¿‡äº†ä»€ä¹ˆæ˜¯<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>å»ºè®®å¿˜è®°äº†çš„åŒå­¦å†å»å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°±ä¸å¤šé‡å¤äº†ã€‚ æˆ‘ä»¬é¦–å…ˆæ¥çœ‹å¦‚ä½•è¿›è¡ŒPatch Embedding</p>
<section id="patch-embedding" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="patch-embedding"><span class="header-section-number">2.1</span> Patch Embedding</h2>
<p>åœ¨<a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">Transformer</a>è¿™ä¸€ç¯‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°ï¼Œå®ƒæ˜¯ä½œç”¨äº<strong>Sequence Modeling</strong>çš„ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒImageä¸æ˜¯ Sequenceçš„, å®ƒæœ‰é•¿<span class="math inline">\(H\)</span>å’Œå®½<span class="math inline">\(W\)</span>ã€‚å¾ˆç›´è§‚çš„ç¬¬ä¸€ç§æƒ³æ³•å°±æ˜¯ï¼Œå°†å›¾ç‰‡ç›´æ¥å±•å¼€ï¼Œä»äºŒç»´ <span class="math inline">\((3, H, W)\)</span> å±•å¼€æˆä¸€ç»´çš„ <span class="math inline">\((3 \times H \times W)\)</span>. è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°çš„å›¾ç‰‡çš„ <strong>Sequence Model</strong>ã€‚å¦‚ä¸‹å›¾ <a href="#fig-flat-image" class="quarto-xref">Figure&nbsp;2</a> æ‰€ç¤º</p>
<div id="fig-flat-image" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flat-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/iGPT.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flat-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of flattening an image into a sequence (Image Source: <a href="https://openai.com/research/image-gpt">iGPT</a>)
</figcaption>
</figure>
</div>
<p>è¿™ç§æ–¹æ³•æœ‰ä¸€ç§æ˜æ˜¾çš„é—®é¢˜å°±æ˜¯:<span style="color:rgb(255, 0, 0)">Sequenceçš„é•¿åº¦å¤ªé•¿</span>ã€‚ ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äº <span class="math inline">\(3\times 256 \times 256\)</span> çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ <span class="math inline">\(256 \times 256 = 65,336\)</span> ä¸ªtokensï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‰€éœ€è¦çš„è®­ç»ƒæ—¶é•¿å¾ˆé•¿ (åœ¨<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£è¿‡ï¼ŒAttentionçš„æ—¶é—´å¤æ‚åº¦æ˜¯ <span class="math inline">\(\mathcal{O}(n^{2}d)\)</span>)ã€‚ é™¤äº†è¿™ä¸ªé—®é¢˜ï¼Œå¦ä¸€ä¸ªé—®é¢˜æ˜¯:<tag style="color:red">å®ƒæ²¡æœ‰ç”¨åˆ°å›¾ç‰‡çš„ç‰¹æ€§</tag>: <em>ç›¸é‚»çš„pixel ä¹‹é—´ï¼Œæ˜¯æœ‰å¾ˆé«˜çš„correlation</em>ã€‚</p>
<p>æ‰€ä»¥æˆ‘ä»¬å¾ˆè‡ªç„¶çš„æƒ³åˆ°:å¦‚æœæŠŠç›¸é‚»çš„pixelså’Œåœ¨ä¸€ç»„ï¼Œç»„æˆä¸€ä¸ªpatchï¼Œè¿™æ ·ä¸å°±æ—¢å‡å°‘äº†tokensçš„æ•°é‡ï¼Œåˆç”¨åˆ°äº†pixelä¹‹é—´çš„correlationã€‚è¿™å°±æ˜¯Vision Transformerçš„Patch Embeddingç»„ä»¶ã€‚</p>
<blockquote class="blockquote">
<p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image <span class="math inline">\(x \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened 2D patches<span class="math inline">\(x \in \mathbb{R}^{N \times (P^{2} \times C)}\)</span>, where (<span class="math inline">\(H, W\)</span>) is the resolution of the original image, <span class="math inline">\(C\)</span> is the number of channels, (<span class="math inline">\(P, P\)</span>) is the resolution of each image patch, and <span class="math inline">\(N = HW/P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. <u>The Transformer uses constant latent vector size <span class="math inline">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math inline">\(D\)</span> dimensions with a trainable linear projection</u>. We refer to the output of this projection as the <strong>patch embeddings</strong>. <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 </cite></p>
</blockquote>
<p>æœ‰äº†Patchä¹‹åï¼Œæˆ‘ä»¬å°±å°†å›¾ç‰‡ä» <span class="math inline">\((H, W, C)\)</span> å˜æˆäº† <span class="math inline">\((N, P, P, C)\)</span>ï¼Œ å…¶ä¸­ <span class="math inline">\(N = \frac{H \times W}{P^{2}}\)</span>ã€‚ æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ€ä¹ˆå®ç°:</p>
<div class="sourceCode" id="cb1" data-code-line-numbers="8,9,10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Load Image and resize it to certain size</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>image_path <span class="op">=</span> IMAGE_PATH</span>
<span id="cb1-3"><a href="#cb1-3"></a>img_bgr <span class="op">=</span> cv2.imread(image_path)</span>
<span id="cb1-4"><a href="#cb1-4"></a>img_resized <span class="op">=</span> cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation<span class="op">=</span>cv2.INTER_AREA)</span>
<span id="cb1-5"><a href="#cb1-5"></a>img <span class="op">=</span> cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) </span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Patchify </span></span>
<span id="cb1-8"><a href="#cb1-8"></a>patches <span class="op">=</span> einops.rearrange( </span>
<span id="cb1-9"><a href="#cb1-9"></a>     img, <span class="st">"(h ph) (w pw) c -&gt; (h w) ph pw c"</span>, ph<span class="op">=</span>PATCH_SIZE, pw<span class="op">=</span>PATCH_SIZE </span>
<span id="cb1-10"><a href="#cb1-10"></a>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>é€šè¿‡è¿™ä¸ªPatchifyä¹‹åï¼Œæˆ‘ä»¬å°†å›¾ç‰‡åˆ†æˆäº† <span class="math inline">\(\left( \frac{H}{P} \times \frac{W}{P}, P, P, C \right)\)</span> ä¸ªPatches</p>
<div id="fig-illustrate-patchify" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-illustrate-patchify-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="assets/before-patch.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="assets/after-patch.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-illustrate-patchify-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Illustration of Patchify
</figcaption>
</figure>
</div>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªPatch å±•å¼€æˆä¸€ä¸ªå‘é‡ï¼Œå˜æˆ <span class="math inline">\((N, P^{2} \times C)\)</span>ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªLinear Layerï¼Œå°†å…¶æ˜ å°„åˆ°ä¸€ä¸ªéšè—ç©ºé—´ï¼Œå˜æˆ <span class="math inline">\((N, d_{model})\)</span>ã€‚ è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†Transformerå¯ä»¥æ¥å—çš„è¾“å…¥ã€‚</p>
<div class="sourceCode" id="cb2" data-code-line-numbers="1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>flat_patch <span class="op">=</span> einops.rearrange( patches, <span class="st">"n ph pw c -&gt; n (ph pw c)"</span>) </span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>mlp <span class="op">=</span> nn.Linear(PATCH_SIZE <span class="op">*</span> PATCH_SIZE <span class="op">*</span> <span class="dv">3</span>, d_model)</span>
<span id="cb2-4"><a href="#cb2-4"></a>patch_embedding <span class="op">=</span> mlp(flat_patch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†Transformerå¯ä»¥æ¥å—çš„ä»»æ„é•¿åº¦çš„è¾“å…¥ã€‚ä¸è¿‡åœ¨å®é™…æ“ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šç”¨ä»¥ä¸Šçš„æ–¹å¼ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªå·ç§¯å±‚æ¥å®ç°è¿™ä¸ªPatch Embeddingçš„è¿‡ç¨‹ã€‚åŸå› æœ‰äºŒ:</p>
<ol type="1">
<li><strong>æ•ˆç‡æ›´é«˜</strong>: å°†Patchify + Flatten + Linear åˆæˆä¸€ä¸ªå·ç§¯å±‚ï¼Œå¯ä»¥å‡å°‘ä¸­é—´çš„å†…å­˜è¯»å†™ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li><strong>ä»£ç æ›´ç®€æ´</strong>: ç”¨ä¸€ä¸ªå·ç§¯å±‚å°±å¯ä»¥å®ç°æ‰€æœ‰çš„åŠŸèƒ½ï¼Œä»£ç é‡æ›´å°‘ï¼Œæ›´æ˜“è¯»æ‡‚ã€‚</li>
</ol>
<p>å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ª å·ç§¯å±‚ï¼Œå‚æ•°è®¾ç½®ä¸º:</p>
<ul>
<li>kernel_size = PATCH_SIZE ï¼ˆå·ç§¯æ ¸è¦†ç›–ä¸€ä¸ª patchï¼‰</li>
<li>stride = PATCH_SIZE ï¼ˆä¸é‡å åœ°ç§»åŠ¨ï¼Œç›¸å½“äºåˆ‡ patchï¼‰</li>
<li>in_channels = 3ï¼ˆRGBï¼‰</li>
<li>out_channels = d_model</li>
</ul>
<p>é‚£ä¹ˆå·ç§¯ä¼š:</p>
<ol type="1">
<li>æŠŠè¾“å…¥å›¾ç‰‡åˆ†æˆ <code>PATCH_SIZE x PATCH_SIZE</code> çš„ä¸é‡å å—ï¼ˆå› ä¸º stride = kernel_sizeï¼‰ã€‚</li>
<li>å¯¹æ¯ä¸ª patch åšä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼ˆå› ä¸º<strong>å·ç§¯æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å±€éƒ¨åŒºåŸŸåšåŠ æƒæ±‚å’Œ</strong>ï¼Œç›¸å½“äº Linearï¼‰ã€‚</li>
<li>è¾“å‡ºçš„ shape è‡ªåŠ¨å°±æ˜¯ (batch, num_patches, d_model)ã€‚</li>
</ol>
<p>è¿™æ­£å¥½ç­‰ä»·äº åˆ‡ patch + flatten + Linear çš„ç»„åˆ.</p>
<p>ä»£ç å¦‚ä¸‹:</p>
<div class="sourceCode" id="cb3" data-code-line-numbers="8,9,10,11,12,13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> PatchEmbedder(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.num_patches_per_side <span class="op">=</span> config.image_size <span class="op">//</span> config.patch_size</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> <span class="va">self</span>.num_patches_per_side<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Conv2d( </span>
<span id="cb3-9"><a href="#cb3-9"></a>            in_channels<span class="op">=</span>config.num_channels, </span>
<span id="cb3-10"><a href="#cb3-10"></a>            out_channels<span class="op">=</span>config.d_model, </span>
<span id="cb3-11"><a href="#cb3-11"></a>            kernel_size<span class="op">=</span>config.patch_size, </span>
<span id="cb3-12"><a href="#cb3-12"></a>            stride<span class="op">=</span>config.patch_size, </span>
<span id="cb3-13"><a href="#cb3-13"></a>        ) </span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb3-16"><a href="#cb3-16"></a>        <span class="co"># x: (B, C, H, W)</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>        x <span class="op">=</span> <span class="va">self</span>.proj(x)  <span class="co"># (B, D, H/P, W/P)</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>        x <span class="op">=</span> x.flatten(<span class="dv">2</span>)  <span class="co"># (B, D, N)</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>        x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, N, D)</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>ç”¨å·ç§¯çš„å¥½å¤„ï¼Œé™¤äº†å¯ä»¥æ›´é«˜æ•ˆçš„å®ç°Patch Embeddingï¼Œä»£ç æ›´åŠ ç®€æ´ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜ <code>stride</code> æ¥ä½¿ä¸€äº›Patch Overlappingï¼Œè·å¾—ä¸€ä¸ªå¤šå°ºåº¦çš„ç»“æ„ã€‚ï¼ˆå°½ç®¡è¿™ä¸ªåœ¨ViTä¸­æ²¡æœ‰æåˆ°ï¼Œä½†æ˜¯æˆ‘è§‰å¾—æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹ï¼‰</p>
<div class="callout-tldr">
<p><tag style="color:blue">TAKEAWAY:</tag> <br> ViT çš„æ ¸å¿ƒæ”¹åŠ¨åªæœ‰ä¸¤æ­¥:<strong>patchifyï¼ˆåˆ‡å—ï¼‰+ embeddingï¼ˆçº¿æ€§æŠ•å½±ï¼‰</strong>ï¼Œå…¶ä½™å‡ ä¹å°±æ˜¯åŸå°ä¸åŠ¨çš„ Transformer Encoder</p>
</div>
<p>å¯ä»¥è¡¥ä¸€å¥ç‚¹æ˜:<strong>â€œViT åŸç‰ˆç”¨ non-overlap patchï¼ˆæ›´çº¯ï¼‰ï¼Œåç»­å¾ˆå¤šå·¥ä½œä¼šç”¨ overlap/conv stem æ¥è¡¥ inductive biasâ€</strong>ï¼ŒæŠŠè¯»è€…å¾€åç»­è®ºæ–‡è‡ªç„¶å¼•è¿‡å»ã€‚</p>
<p>æˆ‘ä»¬è®¡ç®—ä¸€ä¸‹ï¼Œé€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥å‡å°‘å¤šå°‘Tokensçš„æ•°é‡:</p>
<ul>
<li>å›¾åƒå¤§å° <span class="math inline">\(224 \times 224\)</span>ï¼Œ<span class="math inline">\(P=16\)</span> â†’ <span class="math inline">\(N= \frac{224\times 224}{16 ^{ 2}}=196\)</span></li>
<li><span class="math inline">\(P=8\)</span> â†’ <span class="math inline">\(N= \frac{224\times 224}{8 ^{ 2}}=784\)</span>ï¼ˆæ³¨æ„åŠ›çŸ©é˜µå˜æˆ 16 å€ï¼‰</li>
<li>æ‰€ä»¥ <strong>patch size / token æ•°ç›´æ¥å†³å®šè®­ç»ƒçš„æ•ˆç‡</strong></li>
</ul>
</section>
<section id="position-encoding" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="position-encoding"><span class="header-section-number">2.2</span> Position Encoding</h2>
<p>å°†å›¾ç‰‡è½¬åŒ–ä¸º Transformer çš„è¾“å…¥ä¹‹åï¼Œæ¥ä¸‹æ¥Transformerä¸­çš„å¦ä¸€ä¸ªç»„ä»¶å°±æ˜¯ä¼ å…¥ Position Informationã€‚ æˆ‘ä»¬çŸ¥é“åœ¨<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> ä¸­ï¼Œä»–ä»¬ç”¨çš„æ˜¯ <strong>Sine-cosine position embedding</strong>ï¼Œåœ¨é‚£ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæåˆ°äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–ä¸åŒçš„Position Encodingçš„åŠæ³•ï¼ŒViTç”¨çš„å°±æ˜¯å¦ä¸€ç§åŠæ³•ï¼Œ<strong>Learned Position Embedding</strong>ã€‚Learned Position Embeddingçš„æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼Œ<u>å¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬ç»™ä»–ä¸€ä¸ªindexï¼Œå°†è¿™ä¸ªindexä¼ å…¥ä¸€ä¸ª Embedding Matrixï¼Œ æˆ‘ä»¬å°±å¾—åˆ°ä¸€ä¸ªPosition Embedding</u>ã€‚ä¸è¿‡ä¸Token Embeddingä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°æ‰€æœ‰çš„Positionï¼Œä¹Ÿæ•´ä¸ªmatrixï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸ç”¨å®šindexï¼Œç›´æ¥å®šä¹‰æ•´ä¸ªEmbeddingï¼Œç„¶åå°†å®ƒä¼ å…¥Transformerä¸­ã€‚</p>
<div class="sourceCode" id="cb4" data-code-line-numbers="5,6,7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> PosEmbedder(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> nn.Parameter( </span>
<span id="cb4-6"><a href="#cb4-6"></a>            torch.randn(<span class="dv">1</span>, (config.image_size <span class="op">//</span> config.patch_size) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, config.d_model) </span>
<span id="cb4-7"><a href="#cb4-7"></a>        ) <span class="co"># +1 for cls token </span></span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, config.d_model))</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb4-12"><a href="#cb4-12"></a>        B <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb4-13"><a href="#cb4-13"></a>        cls_tokens <span class="op">=</span> <span class="va">self</span>.cls_token.expand(B, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a>        x <span class="op">=</span> torch.cat((cls_tokens, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.position_embeddings</span>
<span id="cb4-17"><a href="#cb4-17"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout-question" title="ä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingï¼Ÿ">
<p>ä¸ºä»€ä¹ˆViTè¦ç”¨Learned Position Embeddingå‘¢ï¼Ÿåœ¨ViTè¿™ç¯‡æ–‡ç« ä¸­ï¼Œä»–ä»¬å°è¯•è¿‡ä¸åŒçš„Position Embeddingï¼Œæ¯”å¦‚:</p>
<ul>
<li>No Positional Information</li>
<li>1-dimensional Positional Embedding</li>
<li>2-dimensional Positional Embedding</li>
<li>Relative Positional Embedding</li>
</ul>
<p>å‘ç°ï¼Œé™¤äº†No Positional Informationä¹‹å¤–ï¼Œå…¶ä½™3ç§åœ¨Image Classificationä¸­çš„è¡¨ç°ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚</p>
<div id="fig-positional-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/position-encoding-exp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-positional-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Compare different Position Encoding methods in Image Classification Task
</figcaption>
</figure>
</div>
<p>è®ºæ–‡ä¸­è¡¨ç¤ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€éœ€è¦çš„Positionçš„ä¿¡æ¯è¾ƒå°ï¼Œå¯¹äºä¸åŒç§ç±»çš„Position Embeddingçš„æ–¹æ³•ï¼Œå­¦ä¹ è¿™ä¸ªPosition Informationçš„èƒ½åŠ›ï¼Œéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚</p>
<blockquote class="blockquote">
<p>We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, <em>in patch-level inputs, the spatial dimensions are much smaller</em> than the original pixel-level inputs, e.g., <span class="math inline">\(14 \times 14\)</span> as opposed to <span class="math inline">\(224 \times 224\)</span>, and <em>learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.</em> <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 </cite></p>
</blockquote>
<p>ä¸è¿‡ï¼Œå°½ç®¡Positionçš„æ–¹æ³•ä¸é‡è¦ï¼Œä½†æ˜¯ä¸åŒçš„è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯ä¼šå½±å“åˆ°å­¦ä¹ åˆ°çš„Position Information, ä¸‹å›¾æ‰€ç¤º:</p>
<div id="fig-position-diff-hyper" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-diff-hyper-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/pos-info-hyper.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-diff-hyper-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of different Position Information learned under different hyper-parameters.
</figcaption>
</figure>
</div>
</div>
<section id="position-interpolation" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="position-interpolation"><span class="header-section-number">2.2.1</span> Position Interpolation</h3>
<p>å½“æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªPre-Trainingçš„æ¨¡å‹ï¼Œæˆ‘ä»¬æƒ³ç”¨å®ƒFine-Tuningåˆ°ä¸€ä¸ªä¸åŒå›¾ç‰‡å¤§å°çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬æ”¹æ€ä¹ˆåšå‘¢?</p>
<p>ç¬¬ä¸€ä¸ªæ–¹æ³•å½“ç„¶æ˜¯ï¼Œ<em>Resize æˆ‘ä»¬çš„å›¾ç‰‡</em>ï¼Œåˆ°ViT Pre-trainingçš„å›¾ç‰‡å¤§å°ï¼Œä½†æ˜¯ï¼Œè¿™ä¸ªèƒ½å¯¼è‡´è¾ƒå¤§çš„å›¾ç‰‡ï¼Œå¤±å»å¾ˆå¤šç»†èŠ‚ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼ŒåŒæ—¶è®©æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±éœ€è¦Extend Position Encodingï¼Œå› ä¸ºå½“Patch Sizeä¸å˜ï¼Œå›¾ç‰‡å¤§å°å˜äº†çš„è¯ï¼Œäº§ç”Ÿçš„Number of Patches ä¹Ÿæ˜¯ä¼šæ”¹å˜çš„ï¼Œæˆ‘ä»¬éœ€è¦åšçš„æ˜¯: æ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œå¢å¤§æˆ–è€…å‡å°Positionçš„æ•°é‡ã€‚ è¿™å°±æ˜¯æ‰€è°“çš„<strong>Position Interpolation</strong>ã€‚</p>
<blockquote class="blockquote">
<p>The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform <strong>2D interpolation</strong> of the pre-trained position embeddings, according to their location in the original image <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 </cite></p>
</blockquote>
<p>æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç æ˜¯æ€ä¹ˆå®ç°Position Interpolationçš„:</p>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> interpolate_pos_encoding(<span class="va">self</span>, x, w, h):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    npatch <span class="op">=</span> x.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>    N <span class="op">=</span> <span class="va">self</span>.pos_embed.shape[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="cf">if</span> npatch <span class="op">==</span> N <span class="kw">and</span> w <span class="op">==</span> h:</span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="cf">return</span> <span class="va">self</span>.pos_embed</span>
<span id="cb5-6"><a href="#cb5-6"></a>    class_pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed[:, <span class="dv">0</span>]</span>
<span id="cb5-7"><a href="#cb5-7"></a>    patch_pos_embed <span class="op">=</span> <span class="va">self</span>.pos_embed[:, <span class="dv">1</span>:]</span>
<span id="cb5-8"><a href="#cb5-8"></a>    dim <span class="op">=</span> x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-9"><a href="#cb5-9"></a>    w0 <span class="op">=</span> w <span class="op">//</span> <span class="va">self</span>.patch_embed.patch_size</span>
<span id="cb5-10"><a href="#cb5-10"></a>    h0 <span class="op">=</span> h <span class="op">//</span> <span class="va">self</span>.patch_embed.patch_size</span>
<span id="cb5-11"><a href="#cb5-11"></a>    </span>
<span id="cb5-12"><a href="#cb5-12"></a>    patch_pos_embed <span class="op">=</span> F.interpolate(</span>
<span id="cb5-13"><a href="#cb5-13"></a>        patch_pos_embed.reshape(</span>
<span id="cb5-14"><a href="#cb5-14"></a>            <span class="dv">1</span>, </span>
<span id="cb5-15"><a href="#cb5-15"></a>            <span class="bu">int</span>(math.sqrt(N)), </span>
<span id="cb5-16"><a href="#cb5-16"></a>            <span class="bu">int</span>(math.sqrt(N)), </span>
<span id="cb5-17"><a href="#cb5-17"></a>            dim</span>
<span id="cb5-18"><a href="#cb5-18"></a>        ).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb5-19"><a href="#cb5-19"></a>        scale_factor<span class="op">=</span>(w0 <span class="op">/</span> math.sqrt(N), h0 <span class="op">/</span> math.sqrt(N)),</span>
<span id="cb5-20"><a href="#cb5-20"></a>        mode<span class="op">=</span><span class="st">'bicubic'</span>,</span>
<span id="cb5-21"><a href="#cb5-21"></a>    )</span>
<span id="cb5-22"><a href="#cb5-22"></a>    patch_pos_embed <span class="op">=</span> patch_pos_embed.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, dim)</span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="cf">return</span> torch.cat((class_pos_embed.unsqueeze(<span class="dv">0</span>), patch_pos_embed), dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout-tldr">
<p><tag style="color:blue">TAKEAWAY:</tag> <br> 2D interpolation of the pre-trained position embeddings</p>
<ul>
<li>ViT åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ç”¨å›ºå®šè¾“å…¥åˆ†è¾¨ç‡ï¼ˆæ¯”å¦‚ 224Ã—224ï¼‰ â†’ ç”Ÿæˆå›ºå®šæ•°é‡çš„ patchï¼ˆæ¯”å¦‚ 16Ã—16 patch â†’ 196 ä¸ª patchï¼‰ã€‚</li>
<li>ä½†åœ¨ fine-tuning æ—¶ï¼Œè¾“å…¥å›¾ç‰‡å¯èƒ½å¤§å°ä¸ä¸€æ ·ï¼Œæ¯”å¦‚ 384Ã—384ï¼Œè¿™æ—¶ patch æ•°é‡å°±å˜äº†ã€‚</li>
<li>è¿™ä¼šå¯¼è‡´åŸæœ¬çš„ ä½ç½®ç¼–ç  (position embeddings) å’Œæ–°çš„ patch æ•°é‡å¯¹ä¸ä¸Šã€‚</li>
<li>è§£å†³åŠæ³•:å¯¹é¢„è®­ç»ƒå¥½çš„ä½ç½®ç¼–ç åš äºŒç»´æ’å€¼ (2D interpolation)ï¼Œæ ¹æ® patch åœ¨åŸå›¾ä¸­çš„ç©ºé—´ä½ç½®ï¼ŒæŠŠä½ç½®ç¼–ç æ‹‰ä¼¸/ç¼©æ”¾åˆ°æ–°çš„åˆ†è¾¨ç‡ã€‚</li>
</ul>
</div>
</section>
</section>
<section id="cls-tokens-mlp-head" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="cls-tokens-mlp-head"><span class="header-section-number">2.3</span> <code>[CLS]</code> Tokens &amp; MLP Head</h2>
<p>åœ¨ <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬äº†è§£åˆ°:æ¯è¾“å…¥ä¸€ä¸ªtokenï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„tokenã€‚è¿™å°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªpatchï¼ŒTransformerä¼šè¾“å‡ºå¯¹åº”çš„Tokensï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªtokenä½œä¸ºæˆ‘ä»¬å›¾ç‰‡çš„è¡¨ç¤ºå‘¢ã€‚ BERT <span class="citation" data-cites="BERTPretrainingDeep2019devlin">(<a href="#ref-BERTPretrainingDeep2019devlin" role="doc-biblioref">Devlin et al. 2019</a>)</span>ï¼Œ ç”¨äº†ä¸€ä¸ª <code>[CLS]</code>, æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­ã€‚åŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ·»åŠ ä¸€ä¸ª <code>[CLS]</code> token, æ¥è¡¨ç¤ºä¸€å¼ å›¾ç‰‡ã€‚åŒæ—¶ï¼Œå¯¹äº <code>[CLS]</code> token, æˆ‘ä»¬ä¹Ÿè¦åœ¨ç»™ä»–ä¸€ä¸ªè¡¨ç¤ºä½ç½®çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨Position Encodingä¸Šï¼Œæˆ‘ä»¬æœ‰ <code>(config.image_size // config.patch_size) ** 2 + 1,</code> ä½ç½®ä¿¡æ¯ï¼Œå…¶ä¸­ <code>+1</code> å°±æ˜¯ <code>[CLS]</code> çš„ä½ç½®ä¿¡æ¯ã€‚</p>
<p>æ€»ç»“ä¸€ä¸‹ <code>[CLS]</code> token çš„ä½œç”¨å°±æ˜¯ç”¨æ¥èšåˆæ‰€æœ‰çš„Patchçš„æ¶ˆæ¯ï¼Œç„¶åç”¨æ¥Image çš„Representationã€‚</p>
<p>æˆ‘ä»¬æƒ³ä¸€ä¸‹ï¼Œé™¤äº†åŠ ä¸€ä¸ª <code>[CLS]</code> tokenï¼Œä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å…¶ä»–åŠæ³•æ¥è¡¨ç¤ºå›¾ç‰‡å—ã€‚æœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•å°±æ˜¯ï¼Œå°†æ‰€æœ‰çš„patchçš„æ¶ˆæ¯æ”¶é›†èµ·æ¥ï¼Œç„¶åå»ä¸€ä¸ªå¹³å‡å€¼æ¥è¡¨ç¤ºè¿™ä¸ªå›¾ç‰‡ã€‚ç±»ä¼¼äºä¼ ç»Ÿçš„ConvNet(e.g.&nbsp;ResNet) æˆ‘ä»¬å¯ä»¥é€šè¿‡ <code>AvgPooling</code> æ¥å®ç°ã€‚ ä¸è¿‡è®ºæ–‡ä¸­æåˆ°ï¼Œ å¯¹äºä¸¤ç§ä¸åŒçš„Image Representationï¼Œéœ€è¦æœ‰ä¸åŒçš„Learning Rate æ¥è®­ç»ƒè¿™ä¸ªç½‘ç»œã€‚ é€šè¿‡ä¸‹å›¾ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œä¸ç”¨çš„æ”¶é›†ä¿¡æ¯çš„æ–¹æ³•ï¼Œéœ€è¦ä¸åŒçš„learning rate <img src="assets/gap-vs-cls-learning-rate.png" id="fig-gap-vs-cls-lr" class="img-fluid"></p>
<div class="callout-tldr">
<p>ç»“æ„ä¸ŠäºŒè€…éƒ½å¯è¡Œï¼Œä½†éœ€è¦ä¸åŒ LR / recipeã€‚ å®è·µé‡Œå¾ˆå¤šå®ç°é»˜è®¤ç”¨ <code>CLS</code>ï¼ˆä¸ BERT å¯¹é½ã€ä¸‹æ¸¸æ›´ç»Ÿä¸€ï¼‰ï¼Œä¹Ÿæœ‰ç”¨ GAP çš„å˜ä½“æ¯”å¦‚æˆ‘ä»¬æ¥ä¸‹æ¥è¦å­¦çš„ Swin Transformer <span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span></p>
</div>
<p>æœ‰äº†Image Representä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸ªä¼ å…¥ä¸€ä¸ªç®€å•çš„MLPï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªClassifierã€‚MLPçš„è¾“å…¥æ˜¯hidden dimï¼Œè¾“å‡ºåˆ™æ˜¯æˆ‘ä»¬Number of Classesã€‚ä¸åŒçš„Index è¡¨ç¤ºä¸åŒçš„Classsesã€‚</p>
<blockquote class="blockquote">
<p>An initial attempt at using only image-patch embeddings, <strong>globally average-pooling (GAP)</strong> them, followed by a linear classifierâ€”just like ResNetâ€™s final feature mapâ€”performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate, <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 </cite></p>
</blockquote>
<blockquote class="blockquote">
<p>Both during pre-training and fine-tuning, a classification head is attached to <span class="math inline">\(\mathrm{z}_{L}^{0}\)</span>. The classification head is implemented by a MLP with <u>one hidden layer at pre-training time and by a single linear layer at fine-tuning time</u>. <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 </cite></p>
</blockquote>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">class</span> MLPHead(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(config.d_model, config.num_classes)</span>
<span id="cb6-6"><a href="#cb6-6"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_rate)</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb6-9"><a href="#cb6-9"></a>        cls <span class="op">=</span> x[:, <span class="dv">0</span>, :]</span>
<span id="cb6-10"><a href="#cb6-10"></a>        cls <span class="op">=</span> <span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc1(cls)))</span>
<span id="cb6-11"><a href="#cb6-11"></a>        cls <span class="op">=</span> <span class="va">self</span>.fc2(cls)</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a>        <span class="cf">return</span> cls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="transformer-encoder-block" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="transformer-encoder-block"><span class="header-section-number">2.4</span> Transformer Encoder Block</h2>
<p>è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®²å®Œäº†ViT, ä¸<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a>çš„ä¸»è¦ä¸åŒä¹‹å¤„ã€‚æ¥ä¸‹æ¥ï¼Œå°±æ˜¯Transformerçš„Encoderã€‚ <img src="assets/ViT-Encoder.png" id="fig-vit-encoder" class="img-fluid"></p>
<p>è¿™éƒ¨åˆ†ï¼Œå’ŒTransformeråŸæœ¬çš„Encoderå¾ˆç±»ä¼¼ï¼Œåªä¸è¿‡æœ‰å‡ å¤„ä¸åŒ:</p>
<ul>
<li><strong>Pre-Norm</strong>: åœ¨ViTåŒï¼Œè¾“å…¥å…ˆè¿›è¡Œä¸€ä¸ªLayerNormï¼Œç„¶ååœ¨ä¼ å…¥MHAæˆ–è€…MLPä¸­ï¼Œåè§‚åœ¨TransformeråŸæœ¬çš„Encoderä¸­ï¼Œæˆ‘ä»¬æ˜¯å…ˆå°†MHAæˆ–è€…MLPçš„è¾“å‡ºä¸è¾“å…¥åŠ åœ¨ä¸€èµ·ï¼Œä¹‹åå†è¿›è¡Œä¸€ä¸ªNormalizationã€‚è¿™å«åš<strong>Post-Norm</strong></li>
<li>MLPçš„å®ç°:åœ¨Transformer Encoderä¸­ï¼Œç”¨çš„æ˜¯ <code>ReLU</code>, è€Œåœ¨ViTä¸­ï¼Œç”¨çš„æ˜¯ <code>GELU</code></li>
</ul>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ä¸€æ ·çš„ã€‚ä¸€ä¸‹æ˜¯ViT Encoderçš„å®ç°:</p>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a>        <span class="va">self</span>.mha <span class="op">=</span> MHA(config)</span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FFN(config)</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(config.d_model)</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb7-10"><a href="#cb7-10"></a>        attn, _ <span class="op">=</span> <span class="va">self</span>.mha(<span class="va">self</span>.norm1(x))</span>
<span id="cb7-11"><a href="#cb7-11"></a>        x <span class="op">=</span> x <span class="op">+</span> attn</span>
<span id="cb7-12"><a href="#cb7-12"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.norm2(x))</span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="multi-heads-attention" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="multi-heads-attention"><span class="header-section-number">2.4.1</span> Multi-Heads Attention</h3>
<p>è¿˜æœ‰ä¸€ä¸ªå°±æ˜¯Attentionæ¨¡å—ï¼ŒAttentionæ¨¡å—ä¸<a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a>ä¸­çš„æ˜¯ä¸€æ¨¡ä¸€æ ·ï¼Œåœ¨è¿™é‡Œå°±ä¸è¿‡å¤šçš„èµ˜è¿°äº†ã€‚</p>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> MHA(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb8-3"><a href="#cb8-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_heads</span>
<span id="cb8-5"><a href="#cb8-5"></a>        <span class="va">self</span>.d_model <span class="op">=</span> config.d_model</span>
<span id="cb8-6"><a href="#cb8-6"></a>        <span class="cf">assert</span> config.d_model <span class="op">%</span> config.num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a>        <span class="va">self</span>.qkv_linear <span class="op">=</span> nn.Linear(config.d_model, config.d_model <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb8-10"><a href="#cb8-10"></a>        <span class="va">self</span>.out_linear <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>        <span class="va">self</span>.attention_dropout <span class="op">=</span> nn.Dropout(config.attention_dropout_rate)</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb8-15"><a href="#cb8-15"></a>        B, N, C <span class="op">=</span> x.shape  <span class="co"># Batch size, Number of tokens, Embedding dimension</span></span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a>        q, k, v <span class="op">=</span> (</span>
<span id="cb8-18"><a href="#cb8-18"></a>            <span class="va">self</span>.qkv_linear(x).reshape(B, N, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="va">self</span>.d_k).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>).unbind(<span class="dv">0</span>)</span>
<span id="cb8-19"><a href="#cb8-19"></a>        )</span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a>        scores <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> torch.sqrt(</span>
<span id="cb8-22"><a href="#cb8-22"></a>            torch.tensor(<span class="va">self</span>.d_k, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb8-23"><a href="#cb8-23"></a>        )  <span class="co"># (B, num_heads, N, N)</span></span>
<span id="cb8-24"><a href="#cb8-24"></a>        attn_weight <span class="op">=</span> torch.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, num_heads, N, N)</span></span>
<span id="cb8-25"><a href="#cb8-25"></a>        attn <span class="op">=</span> <span class="va">self</span>.attention_dropout(attn_weight)</span>
<span id="cb8-26"><a href="#cb8-26"></a></span>
<span id="cb8-27"><a href="#cb8-27"></a>        context <span class="op">=</span> torch.matmul(attn, v)  <span class="co"># (B, num_heads, N, d_k)</span></span>
<span id="cb8-28"><a href="#cb8-28"></a>        context <span class="op">=</span> context.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, N, C)  <span class="co"># (B, N, d_model)</span></span>
<span id="cb8-29"><a href="#cb8-29"></a></span>
<span id="cb8-30"><a href="#cb8-30"></a>        out <span class="op">=</span> <span class="va">self</span>.out_linear(context)  <span class="co"># (B, N, d_model)</span></span>
<span id="cb8-31"><a href="#cb8-31"></a></span>
<span id="cb8-32"><a href="#cb8-32"></a>        <span class="cf">return</span> out, attn_weight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="cnn-vs.-vit-inductive-bias" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="cnn-vs.-vit-inductive-bias"><span class="header-section-number">2.5</span> CNN vs.&nbsp;ViT: Inductive bias</h2>
<p>è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»å®Œäº†Vision Transformerï¼Œæˆ‘ä»¬æ¥ä»Inductive Biasçš„æ–¹é¢ï¼Œçœ‹çœ‹ CNN å’Œ ViT æœ‰ä»€ä¹ˆä¸åŒ</p>
<div class="callout callout-style-default callout-note callout-titled" title="ä»€ä¹ˆæ˜¯Inductive Bias">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ä»€ä¹ˆæ˜¯Inductive Bias
</div>
</div>
<div class="callout-body-container callout-body">
<p>åœ¨æ·±åº¦å­¦ä¹ é‡Œï¼ŒInductive Biasï¼ˆå½’çº³åç½®ï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨å­¦ä¹ ä¹‹å‰ï¼Œå› <strong>ç»“æ„æˆ–è®¾è®¡è€Œè‡ªå¸¦çš„å‡è®¾æˆ–å…ˆéªŒ</strong> ï¼Œæ¯”å¦‚ Convolution Layer, å®ƒå°±æ˜¯å‡è®¾ç›¸é‚»çš„pixelä¹‹é—´ï¼Œæ˜¯æœ‰ä¸€å®šè”ç³»çš„ï¼Œå› æ­¤å¯ä»¥ç”¨ä¸€ä¸ªKernelæ¥å°†å­¦ä¹ è¿™äº›å…³ç³»ã€‚</p>
</div>
</div>
<p>å¯¹äºå›¾åƒæ¥è¯´ï¼Œå¸¸è§çš„å…ˆéªŒå°±æ˜¯:</p>
<ul>
<li>å±€éƒ¨åƒç´ æ˜¯ç›¸å…³çš„ï¼ˆlocalityï¼‰</li>
<li>ç›¸é‚»åŒºåŸŸçš„æ¨¡å¼æœ‰è§„å¾‹ï¼ˆ2D neighborhoodï¼‰</li>
<li>ç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒå“ªé‡Œï¼Œè¯†åˆ«æ–¹å¼åº”è¯¥ä¸€æ ·ï¼ˆTranslation Equivarianceï¼‰</li>
</ul>
<p>é‚£ä¹ˆï¼ŒCNN çš„ç»“æ„æ€ä¹ˆä½“ç°è¿™äº›åç½®ï¼Ÿ 1. å±€éƒ¨æ€§ (Locality): - å·ç§¯æ ¸ï¼ˆä¾‹å¦‚ 3Ã—3ï¼‰åªå’Œå±€éƒ¨åƒç´ æ‰“äº¤é“ï¼Œè€Œä¸æ˜¯å…¨å›¾ã€‚ - è¿™æ„å‘³ç€æ¨¡å‹â€œç›¸ä¿¡â€å›¾åƒçš„é‡è¦ç‰¹å¾æ¥è‡ªå±€éƒ¨é‚»åŸŸï¼Œè€Œä¸æ˜¯é¥è¿œåŒºåŸŸã€‚ 2. äºŒç»´é‚»åŸŸç»“æ„ (2D structure): - å·ç§¯æ“ä½œæ˜¯æ²¿ç€ å›¾åƒçš„äºŒç»´ç½‘æ ¼è¿›è¡Œçš„ï¼Œå¤©ç„¶åˆ©ç”¨äº†å›¾åƒçš„è¡Œåˆ—ç»“æ„ã€‚ - è¿™å’Œæ–‡æœ¬ï¼ˆåºåˆ— 1Dï¼‰ä¸ä¸€æ ·ï¼ŒCNN æ˜ç¡®çŸ¥é“è¾“å…¥æ˜¯ 2D æ’åˆ—çš„ã€‚ 3. å¹³ç§»ç­‰å˜æ€§ (Translation equivariance): - å·ç§¯æ ¸çš„å‚æ•°åœ¨æ•´å¼ å›¾å…±äº«ã€‚ - æ‰€ä»¥çŒ«åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°â€œçŒ«è€³æœµâ€ã€‚ - è¿™è®© CNN è‡ªåŠ¨å…·æœ‰â€œè¯†åˆ«ä½ç½®æ— å…³â€çš„èƒ½åŠ›ã€‚</p>
<p>è¿™äº›æ€§è´¨ä¸æ˜¯æ¨¡å‹é€šè¿‡è®­ç»ƒå­¦å‡ºæ¥çš„ï¼Œè€Œæ˜¯å› ä¸ºå·ç§¯æ“ä½œæœ¬èº«çš„æ•°å­¦ç»“æ„å°±å¸¦æ¥çš„ï¼ˆä¹Ÿæ˜¯æˆ‘ä»¬äººä¸ºè®¾è®¡çš„ï¼‰:</p>
<ul>
<li>kernel çš„å±€éƒ¨è¿æ¥ â†’ å±€éƒ¨æ€§</li>
<li>kernel æ»‘åŠ¨è¦†ç›–å…¨å›¾ â†’ å¹³ç§»ç­‰å˜æ€§</li>
<li>æ“ä½œåœ¨äºŒç»´ç©ºé—´å®šä¹‰ â†’ é‚»åŸŸç»“æ„</li>
</ul>
<p>æ‰€ä»¥ï¼Œå“ªæ€•æˆ‘ä»¬ä¸ç»™ CNN å–‚å¤ªå¤šæ•°æ®ï¼Œå®ƒä¹Ÿä¼šåˆ©ç”¨è¿™äº›åç½®å»å­¦ä¹ ç‰¹å¾ã€‚</p>
<p>è€Œå¯¹äº ViT æ¥è¯´ï¼Œå…¶å½’çº³åç½®éå¸¸å¼±ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–æ•°æ®å’Œè®­ç»ƒæ¥å­¦ä¹ ï¼Œä¸è¿‡å®ƒä¹Ÿæœ‰åˆ©ç”¨äº†ä¸€äº›å›¾ç‰‡çš„Inductive Bias:</p>
<ol type="1">
<li>Patch åˆ‡åˆ† (Patchification) â€¢ ViT å”¯ä¸€çš„â€œå›¾åƒå…ˆéªŒâ€ä¹‹ä¸€å°±æ˜¯æŠŠè¾“å…¥å›¾ç‰‡åˆ‡æˆ patchã€‚ â€¢ è¿™ä¸€æ“ä½œéšå«äº†:å›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´ç»“æ„ï¼Œå¯ä»¥è¢«åˆ†å—å¤„ç†ã€‚</li>
<li>ä½ç½®ç¼–ç  (Positional Embeddings) â€¢ Transformer æœ¬èº«åªå¤„ç†åºåˆ—ï¼Œæ²¡æœ‰ç©ºé—´ç»“æ„çš„æ¦‚å¿µã€‚ â€¢ ViT é€šè¿‡åŠ ä½ç½®ç¼–ç å‘Šè¯‰æ¨¡å‹ patch åœ¨å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®ã€‚ â€¢ åœ¨è¾“å…¥åˆ†è¾¨ç‡å˜åŒ–æ—¶ï¼Œä¼šåš äºŒç»´æ’å€¼ (2D interpolation) æ¥é€‚é…ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§äººå·¥å¼•å…¥çš„ 2D å…ˆéªŒã€‚</li>
<li>å…¶ä»–éƒ¨åˆ† â€¢ é™¤äº†ä»¥ä¸Šä¸¤ç‚¹ï¼ŒViT çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ å…¨å±€çš„ (global)ï¼Œæ²¡æœ‰å±€éƒ¨æ€§çº¦æŸã€‚ â€¢ æ²¡æœ‰åƒ CNN é‚£æ ·å†…ç½®çš„å¹³ç§»ç­‰å˜æ€§æˆ–å±€éƒ¨é‚»åŸŸç»“æ„ã€‚</li>
</ol>
<p>è¿™æ ·å°±æ˜¯ä¸ºä»€ä¹ˆViTéœ€è¦æ›´å¤šæ•°æ®å’Œè®¡ç®—æ‰èƒ½å­¦åˆ°åŒæ ·çš„ç©ºé—´å½’çº³è§„å¾‹ã€‚</p>
</section>
<section id="vit-model-variants" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="vit-model-variants"><span class="header-section-number">2.6</span> ViT Model Variants</h2>
<p>ViT æœ‰3ç§ä¸åŒçš„åŸºæœ¬å˜å½¢ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤º</p>
<p><img src="assets/Vision-Transformer-ViT-variance.png" class="img-fluid"></p>
<p>ViTçš„åå­—é€šå¸¸è¡¨ç¤ºä¸º: ViT-L/16: æ„æ€æ˜¯ï¼ŒViT-Largeï¼Œç„¶åç”¨çš„16 Patch Sizeã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPatch Sizeè¶Šå¤§ï¼Œæˆ‘ä»¬å¾—åˆ°çš„tokenså°±è¶Šå°‘ï¼Œä¹Ÿå°±æ˜¯éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶å®ç°, ä½†é€šå¸¸éœ€è¦æ›´å¤§çš„å›¾ç‰‡æ¥è®­ç»ƒã€‚</p>
</section>
<section id="experiment" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="experiment"><span class="header-section-number">2.7</span> Experiment</h2>
<p>æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹åŸè®ºæ–‡æ˜¯å¦‚ä½•è®­ç»ƒçš„: - <strong>é¢„è®­ç»ƒï¼ˆæ‰€æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬ ResNetï¼‰</strong>:ä½œè€…ç»Ÿä¸€ä½¿ç”¨ Adamï¼ˆ<span class="math inline">\(\beta_{1} = 0.9, \beta_{2}=0.999\)</span>ï¼‰ï¼Œbatch size = 4096ï¼Œå¹¶ä½¿ç”¨è¾ƒå¤§çš„ weight decay = 0.1ã€‚ä½œè€…æŒ‡å‡ºè¿™å¯¹æ‰€æœ‰æ¨¡å‹çš„è¿ç§»è¡¨ç°éƒ½æœ‰å¸®åŠ© - <strong>å­¦ä¹ ç‡ç­–ç•¥</strong>:é‡‡ç”¨çº¿æ€§ warmup + çº¿æ€§ decayï¼ˆç»†èŠ‚è§ Appendix B.1ï¼‰ã€‚ - <strong>å¾®è°ƒï¼ˆfine-tuningï¼‰</strong>:å¯¹æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€æ”¹ç”¨å¸¦ momentum çš„ SGDï¼Œbatch size 512. - <strong>æƒé‡å¹³å‡</strong>:åŒæ—¶ä½¿ç”¨ Polyak averagingï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼Œç³»æ•° 0.9999ï¼‰ä»¥è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚</p>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥è®­ç»ƒæˆ‘ä»¬å®šä¹‰çš„ViTã€‚å…·ä½“çš„ä»£ç å¯ä»¥åœ¨ <a href="https://github.com/YYZhang2025/100-AI-Code/blob/main/02_ViT.ipynb">è¿™é‡Œ</a> æŸ¥çœ‹ã€‚ ### Dataset æˆ‘ä»¬ç”¨CIFAT-10çš„è®­ç»ƒé›†æ¥è®­ç»ƒViTã€‚</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>æˆ‘ä»¬ä¹‹å‰æåˆ°äº†ViTè¦åœ¨å¤§è§„æ¨¡çš„æ•°æ®é›†ä¸Šæ‰å¯ä»¥å‘æŒ¥å®ƒçš„èƒ½åŠ›ï¼Œç”±äºèµ„æºæœ‰é™ï¼Œæˆ‘ä»¬åªåœ¨è¿™å±•ç¤ºViTçš„è®­ç»ƒæµç¨‹ï¼Œåœ¨äº†è§£äº†è¿™ä¸ªè®­ç»ƒæµç¨‹ä¹‹åï¼Œå¾ˆå®¹æ˜“æ‹“å±•åˆ°å…¶ä»–çš„å¤§æ•°æ®é›†ã€‚</p>
</div>
</div>
<div class="sourceCode" id="cb9" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>IMG_MEAN <span class="op">=</span> [<span class="fl">0.4914</span>, <span class="fl">0.4822</span>, <span class="fl">0.4465</span>]</span>
<span id="cb9-2"><a href="#cb9-2"></a>IMG_STD <span class="op">=</span> [<span class="fl">0.2470</span>, <span class="fl">0.2435</span>, <span class="fl">0.2616</span>]</span>
<span id="cb9-3"><a href="#cb9-3"></a>IMG_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>data_transform <span class="op">=</span> transforms.Compose(</span>
<span id="cb9-7"><a href="#cb9-7"></a>    [</span>
<span id="cb9-8"><a href="#cb9-8"></a>        transforms.Resize(</span>
<span id="cb9-9"><a href="#cb9-9"></a>            (IMG_SIZE, IMG_SIZE),</span>
<span id="cb9-10"><a href="#cb9-10"></a>            interpolation<span class="op">=</span>transforms.InterpolationMode.BILINEAR,</span>
<span id="cb9-11"><a href="#cb9-11"></a>        ),</span>
<span id="cb9-12"><a href="#cb9-12"></a>        transforms.RandomHorizontalFlip(),</span>
<span id="cb9-13"><a href="#cb9-13"></a>        transforms.ToTensor(),</span>
<span id="cb9-14"><a href="#cb9-14"></a>        transforms.Normalize(IMG_MEAN, IMG_STD),</span>
<span id="cb9-15"><a href="#cb9-15"></a>    ]</span>
<span id="cb9-16"><a href="#cb9-16"></a>)</span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>train_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb9-19"><a href="#cb9-19"></a>    root<span class="op">=</span>train_config.data_dir, download<span class="op">=</span><span class="va">True</span>, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>data_transform</span>
<span id="cb9-20"><a href="#cb9-20"></a>)</span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="cf">if</span> train_config.debug:</span>
<span id="cb9-22"><a href="#cb9-22"></a>    train_dataset <span class="op">=</span> torch.utils.data.Subset(train_dataset, <span class="bu">range</span>(<span class="dv">1000</span>))</span>
<span id="cb9-23"><a href="#cb9-23"></a></span>
<span id="cb9-24"><a href="#cb9-24"></a></span>
<span id="cb9-25"><a href="#cb9-25"></a>dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb9-26"><a href="#cb9-26"></a>    train_dataset,</span>
<span id="cb9-27"><a href="#cb9-27"></a>    batch_size<span class="op">=</span>train_config.batch_size,</span>
<span id="cb9-28"><a href="#cb9-28"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-29"><a href="#cb9-29"></a>    num_workers<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb9-30"><a href="#cb9-30"></a>    pin_memory<span class="op">=</span><span class="va">True</span> <span class="cf">if</span> train_config.device.<span class="bu">type</span> <span class="op">==</span> <span class="st">"cuda"</span> <span class="cf">else</span> <span class="va">False</span>,</span>
<span id="cb9-31"><a href="#cb9-31"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="optimizer-loss-function" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="optimizer-loss-function"><span class="header-section-number">2.7.1</span> Optimizer &amp; Loss Function</h3>
<p>è®ºæ–‡ä¸­åº”ç”¨äº† Adam Optimizerï¼Œæˆ‘ä»¬åœ¨æ­¤ä¹Ÿç”¨Adamã€‚å› ä¸ºæˆ‘ä»¬è®­ç»ƒçš„æ˜¯Image Classification Taskï¼Œæ‰€ä»¥æŸå¤±å‡½æ•°æ˜¯æ˜¯Cross Entropy Loss:</p>
<div class="sourceCode" id="cb10" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb10-2"><a href="#cb10-2"></a>optimizer <span class="op">=</span> torch.optim.Adam(</span>
<span id="cb10-3"><a href="#cb10-3"></a>    model.parameters(),</span>
<span id="cb10-4"><a href="#cb10-4"></a>    lr<span class="op">=</span>train_config.lr,</span>
<span id="cb10-5"><a href="#cb10-5"></a>    betas<span class="op">=</span>train_config.betas,</span>
<span id="cb10-6"><a href="#cb10-6"></a>    weight_decay<span class="op">=</span>train_config.weight_decay,</span>
<span id="cb10-7"><a href="#cb10-7"></a>)</span>
<span id="cb10-8"><a href="#cb10-8"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(</span>
<span id="cb10-9"><a href="#cb10-9"></a>    optimizer, train_config.num_epochs, eta_min<span class="op">=</span>train_config.min_lr</span>
<span id="cb10-10"><a href="#cb10-10"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="result" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="result"><span class="header-section-number">2.7.2</span> Result</h3>
<p><img src="assets/loss-curve.png" class="img-fluid"></p>
<p><img src="assets/acc-curve.png" class="img-fluid"></p>
<p>æˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œè¿™ä¸ªToy ViT ç©¶ç«Ÿè®­ç»ƒçš„æ€ä¹ˆæ ·:</p>
<div class="sourceCode" id="cb11" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>N_ROWS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>N_COLS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>N_IMGS <span class="op">=</span> N_ROWS <span class="op">*</span> N_COLS</span>
<span id="cb11-4"><a href="#cb11-4"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb11-5"><a href="#cb11-5"></a>    root<span class="op">=</span>train_config.data_dir,</span>
<span id="cb11-6"><a href="#cb11-6"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-7"><a href="#cb11-7"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb11-8"><a href="#cb11-8"></a>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>IDX_TO_CLASS <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> test_dataset.class_to_idx.items()}</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a>org_imgs, true_labels <span class="op">=</span> test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]</span>
<span id="cb11-12"><a href="#cb11-12"></a>transformed_imgs <span class="op">=</span> torch.stack([data_transform(Image.fromarray(img)) <span class="cf">for</span> img <span class="kw">in</span> org_imgs])</span>
<span id="cb11-13"><a href="#cb11-13"></a>pred_labels <span class="op">=</span> model(tensor_to_device(transformed_imgs, device<span class="op">=</span>train_config.device)).argmax(dim<span class="op">=</span><span class="dv">1</span>).cpu()</span>
<span id="cb11-14"><a href="#cb11-14"></a></span>
<span id="cb11-15"><a href="#cb11-15"></a></span>
<span id="cb11-16"><a href="#cb11-16"></a>fig, axes <span class="op">=</span> plt.subplots(N_ROWS, N_COLS, figsize<span class="op">=</span>(N_COLS <span class="op">*</span> <span class="dv">2</span>, N_ROWS <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_ROWS):</span>
<span id="cb11-18"><a href="#cb11-18"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N_COLS):</span>
<span id="cb11-19"><a href="#cb11-19"></a>        idx <span class="op">=</span> i <span class="op">*</span> N_COLS <span class="op">+</span> j</span>
<span id="cb11-20"><a href="#cb11-20"></a>        axes[i, j].imshow(org_imgs[idx])</span>
<span id="cb11-21"><a href="#cb11-21"></a>        axes[i, j].axis(<span class="st">"off"</span>)</span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="cf">if</span> true_labels[idx] <span class="op">==</span> pred_labels[idx].item():</span>
<span id="cb11-23"><a href="#cb11-23"></a>            axes[i, j].set_title(</span>
<span id="cb11-24"><a href="#cb11-24"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb11-25"><a href="#cb11-25"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-26"><a href="#cb11-26"></a>                color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb11-27"><a href="#cb11-27"></a>            )</span>
<span id="cb11-28"><a href="#cb11-28"></a>        <span class="cf">else</span>:</span>
<span id="cb11-29"><a href="#cb11-29"></a>            axes[i, j].set_title(</span>
<span id="cb11-30"><a href="#cb11-30"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb11-31"><a href="#cb11-31"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb11-32"><a href="#cb11-32"></a>                color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb11-33"><a href="#cb11-33"></a>            )</span>
<span id="cb11-34"><a href="#cb11-34"></a>plt.tight_layout()</span>
<span id="cb11-35"><a href="#cb11-35"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="assets/vis-pred.png" class="img-fluid"></p>
<p>æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹çœ‹Attention Mapçš„</p>
<div class="sourceCode" id="cb12" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>N_ROWS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>N_COLS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>N_IMGS <span class="op">=</span> N_ROWS <span class="op">*</span> N_COLS</span>
<span id="cb12-4"><a href="#cb12-4"></a>test_dataset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb12-5"><a href="#cb12-5"></a>    root<span class="op">=</span>train_config.data_dir,</span>
<span id="cb12-6"><a href="#cb12-6"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-7"><a href="#cb12-7"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-8"><a href="#cb12-8"></a>)</span>
<span id="cb12-9"><a href="#cb12-9"></a>IDX_TO_CLASS <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> test_dataset.class_to_idx.items()}</span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a>org_imgs, true_labels <span class="op">=</span> test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]</span>
<span id="cb12-12"><a href="#cb12-12"></a>transformed_imgs <span class="op">=</span> torch.stack([data_transform(Image.fromarray(img)) <span class="cf">for</span> img <span class="kw">in</span> org_imgs]).to(</span>
<span id="cb12-13"><a href="#cb12-13"></a>    train_config.device</span>
<span id="cb12-14"><a href="#cb12-14"></a>)</span>
<span id="cb12-15"><a href="#cb12-15"></a></span>
<span id="cb12-16"><a href="#cb12-16"></a><span class="co"># pred_labels = model(tensor_to_device(transformed_imgs, device=train_config.device)).argmax(dim=1).cpu()</span></span>
<span id="cb12-17"><a href="#cb12-17"></a>TARGET_LAYER <span class="op">=</span> <span class="dv">5</span>  <span class="co"># 0-based index</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>HEAD <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-19"><a href="#cb12-19"></a>GAMMA <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>FLOOR <span class="op">=</span> <span class="fl">0.15</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a><span class="kw">def</span> to_vit_attention_vis(img_uint8, attn_map, gamma<span class="op">=</span><span class="fl">0.7</span>, floor<span class="op">=</span><span class="fl">0.15</span>):</span>
<span id="cb12-24"><a href="#cb12-24"></a>    <span class="co">"""</span></span>
<span id="cb12-25"><a href="#cb12-25"></a><span class="co">    gamma:     &gt;0, smaller -&gt; sharper spotlight</span></span>
<span id="cb12-26"><a href="#cb12-26"></a><span class="co">    floor:     how visible the dark region is (0 = pure black background)</span></span>
<span id="cb12-27"><a href="#cb12-27"></a><span class="co">    """</span></span>
<span id="cb12-28"><a href="#cb12-28"></a>    img <span class="op">=</span> img_uint8.astype(np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb12-29"><a href="#cb12-29"></a></span>
<span id="cb12-30"><a href="#cb12-30"></a>    <span class="co"># normalize attention to [0,1]</span></span>
<span id="cb12-31"><a href="#cb12-31"></a>    a <span class="op">=</span> attn_map.astype(np.float32)</span>
<span id="cb12-32"><a href="#cb12-32"></a>    a <span class="op">=</span> a <span class="op">-</span> a.<span class="bu">min</span>()</span>
<span id="cb12-33"><a href="#cb12-33"></a>    a <span class="op">=</span> a <span class="op">/</span> (a.<span class="bu">max</span>() <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-35"><a href="#cb12-35"></a>    <span class="co"># make it more "spotlight-like"</span></span>
<span id="cb12-36"><a href="#cb12-36"></a>    a <span class="op">=</span> a<span class="op">**</span>gamma  <span class="co"># sharpen</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>    a <span class="op">=</span> floor <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> floor) <span class="op">*</span> a  <span class="co"># keep some visibility outside</span></span>
<span id="cb12-38"><a href="#cb12-38"></a>    a <span class="op">=</span> np.clip(a, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb12-39"><a href="#cb12-39"></a></span>
<span id="cb12-40"><a href="#cb12-40"></a>    <span class="cf">return</span> img <span class="op">*</span> a[..., <span class="va">None</span>]  <span class="co"># darken outside attention</span></span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a></span>
<span id="cb12-43"><a href="#cb12-43"></a>model.<span class="bu">eval</span>()</span>
<span id="cb12-44"><a href="#cb12-44"></a>x <span class="op">=</span> model.backbone.patch_embedder(transformed_imgs)</span>
<span id="cb12-45"><a href="#cb12-45"></a>x <span class="op">=</span> model.backbone.pos_embedder(x)</span>
<span id="cb12-46"><a href="#cb12-46"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(TARGET_LAYER):</span>
<span id="cb12-47"><a href="#cb12-47"></a>    x <span class="op">=</span> model.backbone.encoder_layers[i](x)</span>
<span id="cb12-48"><a href="#cb12-48"></a></span>
<span id="cb12-49"><a href="#cb12-49"></a>_, attn_weights <span class="op">=</span> model.backbone.encoder_layers[TARGET_LAYER].mha(</span>
<span id="cb12-50"><a href="#cb12-50"></a>    model.backbone.encoder_layers[TARGET_LAYER].norm1(x)</span>
<span id="cb12-51"><a href="#cb12-51"></a>)</span>
<span id="cb12-52"><a href="#cb12-52"></a><span class="cf">if</span> HEAD <span class="op">&gt;=</span> <span class="dv">0</span>:</span>
<span id="cb12-53"><a href="#cb12-53"></a>    attn_weights <span class="op">=</span> attn_weights[:, HEAD, <span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># (B, N)</span></span>
<span id="cb12-54"><a href="#cb12-54"></a><span class="cf">else</span>:</span>
<span id="cb12-55"><a href="#cb12-55"></a>    attn_weights <span class="op">=</span> attn_weights.mean(dim<span class="op">=</span><span class="dv">1</span>)[:, <span class="dv">0</span>, <span class="dv">1</span>:]  <span class="co"># (B, N)</span></span>
<span id="cb12-56"><a href="#cb12-56"></a></span>
<span id="cb12-57"><a href="#cb12-57"></a><span class="co"># Reshape attention weights to (B, H, W)</span></span>
<span id="cb12-58"><a href="#cb12-58"></a>num_patches_per_side <span class="op">=</span> model_config.image_size <span class="op">//</span> model_config.patch_size</span>
<span id="cb12-59"><a href="#cb12-59"></a>attn_maps <span class="op">=</span> attn_weights.reshape(<span class="op">-</span><span class="dv">1</span>, num_patches_per_side, num_patches_per_side)  <span class="co"># (B, H, W)</span></span>
<span id="cb12-60"><a href="#cb12-60"></a></span>
<span id="cb12-61"><a href="#cb12-61"></a><span class="co"># Upsample attention maps to image size</span></span>
<span id="cb12-62"><a href="#cb12-62"></a>attn_maps_upsampled <span class="op">=</span> F.interpolate(</span>
<span id="cb12-63"><a href="#cb12-63"></a>    attn_maps.unsqueeze(<span class="dv">1</span>),</span>
<span id="cb12-64"><a href="#cb12-64"></a>    size<span class="op">=</span>(model_config.image_size, model_config.image_size),</span>
<span id="cb12-65"><a href="#cb12-65"></a>    mode<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb12-66"><a href="#cb12-66"></a>    align_corners<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-67"><a href="#cb12-67"></a>).squeeze(<span class="dv">1</span>)  <span class="co"># (B, H, W)</span></span>
<span id="cb12-68"><a href="#cb12-68"></a></span>
<span id="cb12-69"><a href="#cb12-69"></a><span class="co"># Move attention map to CPU for visualization</span></span>
<span id="cb12-70"><a href="#cb12-70"></a>attn_maps_upsampled <span class="op">=</span> attn_maps_upsampled.cpu().detach().numpy()</span>
<span id="cb12-71"><a href="#cb12-71"></a></span>
<span id="cb12-72"><a href="#cb12-72"></a><span class="co"># Visualize Overlaid Attention Maps</span></span>
<span id="cb12-73"><a href="#cb12-73"></a>fig, axes <span class="op">=</span> plt.subplots(N_ROWS, N_COLS, figsize<span class="op">=</span>(N_COLS <span class="op">*</span> <span class="dv">2</span>, N_ROWS <span class="op">*</span> <span class="dv">2</span>))</span>
<span id="cb12-74"><a href="#cb12-74"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N_ROWS):</span>
<span id="cb12-75"><a href="#cb12-75"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N_COLS):</span>
<span id="cb12-76"><a href="#cb12-76"></a>        idx <span class="op">=</span> i <span class="op">*</span> N_COLS <span class="op">+</span> j</span>
<span id="cb12-77"><a href="#cb12-77"></a>        vis <span class="op">=</span> to_vit_attention_vis(</span>
<span id="cb12-78"><a href="#cb12-78"></a>            org_imgs[idx],</span>
<span id="cb12-79"><a href="#cb12-79"></a>            attn_maps_upsampled[idx],</span>
<span id="cb12-80"><a href="#cb12-80"></a>            gamma<span class="op">=</span>GAMMA,</span>
<span id="cb12-81"><a href="#cb12-81"></a>            floor<span class="op">=</span>FLOOR,</span>
<span id="cb12-82"><a href="#cb12-82"></a>        )</span>
<span id="cb12-83"><a href="#cb12-83"></a></span>
<span id="cb12-84"><a href="#cb12-84"></a>        axes[i, j].imshow(vis)</span>
<span id="cb12-85"><a href="#cb12-85"></a>        axes[i, j].axis(<span class="st">"off"</span>)</span>
<span id="cb12-86"><a href="#cb12-86"></a>        <span class="cf">if</span> true_labels[idx] <span class="op">==</span> pred_labels[idx].item():</span>
<span id="cb12-87"><a href="#cb12-87"></a>            axes[i, j].set_title(</span>
<span id="cb12-88"><a href="#cb12-88"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb12-89"><a href="#cb12-89"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-90"><a href="#cb12-90"></a>                color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb12-91"><a href="#cb12-91"></a>            )</span>
<span id="cb12-92"><a href="#cb12-92"></a>        <span class="cf">else</span>:</span>
<span id="cb12-93"><a href="#cb12-93"></a>            axes[i, j].set_title(</span>
<span id="cb12-94"><a href="#cb12-94"></a>                <span class="ss">f"</span><span class="sc">{</span>IDX_TO_CLASS[true_labels[idx]]<span class="sc">}</span><span class="ss"> (Pred: </span><span class="sc">{</span>IDX_TO_CLASS[pred_labels[idx].item()]<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb12-95"><a href="#cb12-95"></a>                fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-96"><a href="#cb12-96"></a>                color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb12-97"><a href="#cb12-97"></a>            )</span>
<span id="cb12-98"><a href="#cb12-98"></a>plt.tight_layout()</span>
<span id="cb12-99"><a href="#cb12-99"></a>plt.suptitle(</span>
<span id="cb12-100"><a href="#cb12-100"></a>    <span class="ss">f"Attention Maps Over Images On Layer </span><span class="sc">{</span>TARGET_LAYER<span class="sc">}</span><span class="ss">  </span><span class="sc">{</span><span class="st">'Head '</span> <span class="op">+</span> <span class="bu">str</span>(HEAD) <span class="cf">if</span> HEAD <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'Avg Head'</span><span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb12-101"><a href="#cb12-101"></a>    y<span class="op">=</span><span class="fl">1.02</span>,</span>
<span id="cb12-102"><a href="#cb12-102"></a>)</span>
<span id="cb12-103"><a href="#cb12-103"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="assets/attn.png" class="img-fluid"></p>
</section>
<section id="training-recipe" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="training-recipe"><span class="header-section-number">2.7.3</span> Training Recipe</h3>
<p>æ¥ä¸‹æ¥æˆ‘æä¾›å‡ ä¸ªå¯èƒ½çš„æå‡Accuracyçš„æ–¹æ³•ï¼ˆç”±äºæ—¶é—´ç°å®ï¼Œæš‚æ—¶æ²¡æœ‰èƒ½å®ç°ï¼Œæœ‰å…´è¶£çš„è¯»è€…æ¬¢è¿è‡ªè¡Œå°è¯•ï¼‰:</p>
<ul>
<li><strong>ä¼˜åŒ–å™¨</strong>:AdamWï¼ˆè€Œé Adamï¼‰+ weight decay</li>
<li><strong>å­¦ä¹ ç‡ç­–ç•¥</strong>:warmup + cosineï¼Œbatch size å¯¹ LR çš„çº¿æ€§ç¼©æ”¾è§„åˆ™</li>
<li><strong>å¢å¼º</strong>:RandAugment / Mixup / CutMix</li>
<li><strong>æ­£åˆ™</strong>:DropPathï¼ˆstochastic depthï¼‰ã€Label smoothing</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="ViT on small dataï¼ˆImageNet-1kçº§åˆ«ï¼‰å¸¸ç”¨ recipe">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ViT on small dataï¼ˆImageNet-1kçº§åˆ«ï¼‰å¸¸ç”¨ recipe
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>AdamW + cosine + warmup + strong augï¼ˆRA/Mixup/CutMixï¼‰+ DropPath + label smoothing</p>
</section>
<section id="training-summary" class="level3" data-number="2.7.4">
<h3 data-number="2.7.4" class="anchored" data-anchor-id="training-summary"><span class="header-section-number">2.7.4</span> Training Summary</h3>
<p>ä»ç»“æœæ¥çœ‹ï¼Œè¿™ä¸ªViTè¡¨ç°çš„å¹¶ä¸æ˜¯å¾ˆå¥½ï¼Œç”šè‡³ä¸å¦‚ç®€å•çš„Convolution Layerã€‚ä¸è¿‡è¿™ç§ç»“æœæ˜¯åœ¨æˆ‘ä»¬é¢„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é‡å¤ªå°‘äº†ï¼ŒViTè¿˜ä¸èƒ½ä»æ•°æ®ä¸­å­¦åˆ°æœ‰æ•ˆçš„ä¿¡æ¯ã€‚ ä¸è¿‡å‡ºä¹æˆ‘æ„æ–™çš„æ˜¯ï¼ŒAttention Mapå‡ ä¹æ˜¯å¹³å‡çš„ï¼Œ æˆ‘ä»¬æœŸå¾…çš„æ˜¯ï¼Œç±»ä¼¼äºè®ºæ–‡ä¸­çš„Attention Mapã€‚</p>
<p><img src="assets/Vision-Transformer-attention-map.png" class="img-fluid"></p>
<p>å…¶ä¸­ä¸€ä¸ªè§£é‡Šå°±æ˜¯ï¼Œæˆ‘ä»¬è®­ç»ƒçš„å›¾ç‰‡å¤ªå°äº†ï¼Œ<span class="math inline">\(32 \times 32\)</span>ï¼Œå¯¼è‡´æ¯ä¸ªTokenæ”¶é›†åˆ°çš„ä¿¡æ¯å¾ˆå¹³å‡ï¼Œè¿™å°±å¯¼è‡´äº†Attention Mapçœ‹èµ·æ¥åœ¨æ¯ä¸ªåœ°æ–¹éƒ½æ˜¯ä¸€æ ·ã€‚</p>
<p>è§£å†³åŠæ³•å°±æ˜¯:</p>
<ol type="1">
<li>ç”¨Grad-CAM<span class="citation" data-cites="GradCAMVisualExplanations2020selvaraju">(<a href="#ref-GradCAMVisualExplanations2020selvaraju" role="doc-biblioref">Selvaraju et al. 2020</a>)</span> æˆ–è€… æ˜¯ Attention Rollout<a href="@QuantifyingAttentionFlow2020abnar.qmd"><span class="citation" data-cites="QuantifyingAttentionFlow2020abnar">Abnar and Zuidema (<span>2020</span>)</span></a></li>
<li>æé«˜å›¾ç‰‡çš„Resolutionï¼Œæ¯”å¦‚ç”¨ImageNetæ¥è®­ç»ƒ</li>
</ol>
<p>åœ¨è¿™é‡Œå°±ä¸å…·ä½“å±•å¼€äº†ï¼Œæœ‰å…´è¶£çš„åŒå­¦è‡ªè¡ŒæŸ¥çœ‹ã€‚</p>
<p>ä»å®éªŒç»“æœæ¥çœ‹ï¼Œè¿™ä¸ª toy ViT çš„è¡¨ç°ç¡®å®ä¸ç®—ç†æƒ³ï¼Œç”šè‡³ä¸å¦‚ä¸€ä¸ªç®€å•çš„å·ç§¯ç½‘ç»œã€‚è¿™å…¶å®åœ¨é¢„æœŸä¹‹å†…:<u>ViT çš„å½’çº³åç½®æ›´å¼±ï¼ˆç¼ºå°‘å·ç§¯çš„å±€éƒ¨æ€§ä¸å¹³ç§»ç­‰å˜æ€§ï¼‰ï¼Œåœ¨æ•°æ®é‡è¾ƒå°ã€è®­ç»ƒ recipe ä¸å¤Ÿå¼ºçš„æƒ…å†µä¸‹æ›´å®¹æ˜“æ¬ æ‹Ÿåˆæˆ–æ³›åŒ–ä¸è¶³</u>ï¼Œå› æ­¤å¾ˆéš¾åœ¨ CIFAR-10 è¿™ç±»å°è§„æ¨¡æ•°æ®ä¸Šå åˆ°ä¾¿å®œã€‚</p>
<p>æ¯”è¾ƒâ€œåç›´è§‰â€çš„æ˜¯ï¼Œæˆ‘ä»¬å¯è§†åŒ–å¾—åˆ°çš„ <strong>Attention Map å‡ ä¹æ¥è¿‘å‡åŒ€åˆ†å¸ƒ</strong>ï¼Œè€Œä¸æ˜¯åƒåŸè®ºæ–‡é‚£æ ·å‘ˆç°å‡ºæ›´æ¸…æ™°çš„è¯­ä¹‰èšç„¦ï¼ˆä¾‹å¦‚å¯¹ç‰©ä½“åŒºåŸŸçš„æ³¨æ„åŠ›æ›´å¼ºï¼‰:</p>
<p><img src="assets/Vision-Transformer-attention-map.png" class="img-fluid"></p>
<p>ä¸€ç§åˆç†çš„è§£é‡Šæ˜¯:æˆ‘ä»¬çš„è¾“å…¥åˆ†è¾¨ç‡åªæœ‰ (<span class="math inline">\(32\times32\)</span>)ï¼Œåœ¨å¸¸è§ patch è®¾ç½®ä¸‹ token æ•°é‡éå¸¸æœ‰é™ï¼ˆä¾‹å¦‚ (<span class="math inline">\(P=4\)</span>) æ—¶ä¹Ÿåªæœ‰ (<span class="math inline">\(8\times8=64\)</span>) ä¸ª patchï¼‰ã€‚åœ¨è¿™ç§ä½åˆ†è¾¨ç‡ã€ä½ token æ•°çš„è®¾å®šé‡Œï¼Œæ¯ä¸ª token è¦†ç›–çš„åŒºåŸŸç›¸å¯¹â€œç²—â€ï¼Œå¹¶ä¸”æ—©æœŸè®­ç»ƒé˜¶æ®µæ¨¡å‹å¾€å¾€æ›´å€¾å‘äºå­¦ä¹ å…¨å±€å¹³å‡çš„ç›¸å…³æ€§æ¥æœ€å°åŒ–æŸå¤±ï¼Œå¯¼è‡´æ³¨æ„åŠ›æƒé‡çœ‹èµ·æ¥æ›´å¹³å‡ã€‚å¦ä¸€ä¸ªæ³¨æ„çš„ç‚¹å°±æ˜¯:<strong>å•å±‚æ³¨æ„åŠ›æƒé‡æœ¬èº«æœªå¿…ç­‰ä»·äºâ€œå¯è§£é‡Šæ€§â€</strong>ï¼Œå³ä½¿æ¨¡å‹åœ¨åšå‡ºæ­£ç¡®å†³ç­–ï¼Œä¹Ÿå¯èƒ½å‡ºç°æ³¨æ„åŠ›å›¾ä¸å¤Ÿå°–é”çš„ç°è±¡ã€‚</p>
<p>å¦‚æœå¸Œæœ›å¾—åˆ°æ›´æœ‰ä¿¡æ¯é‡çš„å¯è§£é‡Šç»“æœï¼Œé€šå¸¸æœ‰ä¸¤æ¡æ›´ç¨³å¦¥çš„è·¯å¾„:</p>
<ol type="1">
<li>ä½¿ç”¨æ›´å¯é çš„å¯è§£é‡Šæ–¹æ³•ï¼Œä¾‹å¦‚ <strong>Grad-CAM</strong> <span class="citation" data-cites="GradCAMVisualExplanations2020selvaraju">(<a href="#ref-GradCAMVisualExplanations2020selvaraju" role="doc-biblioref">Selvaraju et al. 2020</a>)</span>ï¼Œæˆ–è€…ç»“åˆå¤šå±‚æ³¨æ„åŠ›çš„ <strong>Attention Rollout</strong> <span class="citation" data-cites="QuantifyingAttentionFlow2020abnar">(<a href="#ref-QuantifyingAttentionFlow2020abnar" role="doc-biblioref">Abnar and Zuidema 2020</a>)</span>ï¼Œè€Œä¸æ˜¯åªè§‚å¯ŸæŸä¸€å±‚/æŸä¸€å¤´çš„ attentionã€‚</li>
<li>æé«˜è¾“å…¥åˆ†è¾¨ç‡ä¸è®­ç»ƒè§„æ¨¡ï¼ˆä¾‹å¦‚åœ¨ ImageNet æˆ–æ›´å¤§æ•°æ®ä¸Šè®­ç»ƒ/é¢„è®­ç»ƒåå†è¿ç§»ï¼‰ï¼Œè®©æ¨¡å‹æœ‰æœºä¼šå­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„ç©ºé—´ç»“æ„ä¸æ›´ç¨³å®šçš„è¯­ä¹‰å¯¹é½ã€‚</li>
</ol>
<p>è¿™é‡Œå°±ä¸å±•å¼€å®ç°ç»†èŠ‚äº†ï¼Œæœ‰å…´è¶£çš„åŒå­¦å¯ä»¥æ ¹æ®ä¸Šè¿°è®ºæ–‡è¿›ä¸€æ­¥å°è¯•ä¸å¯¹æ¯”ã€‚</p>
</section>
</section>
</section>
<section id="others" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Others</h1>
<section id="self-supervised-pre-training" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="self-supervised-pre-training"><span class="header-section-number">3.1</span> Self-Supervised Pre-Training</h2>
<p>é™¤äº†åš Image Classificationï¼ŒViT å›¢é˜Ÿä¹Ÿå°è¯•äº†è‡ªç›‘ç£é¢„è®­ç»ƒï¼ˆSelf-Supervised Pre-Trainingï¼‰ã€‚ä»–ä»¬é‡‡ç”¨äº†ä¸€ç§éå¸¸â€œç±»ä¼¼äºBERT<span class="citation" data-cites="BERTPretrainingDeep2019devlin">(<a href="#ref-BERTPretrainingDeep2019devlin" role="doc-biblioref">Devlin et al. 2019</a>)</span>â€çš„æ€è·¯:<strong>Masked Patch Prediction</strong>â€”â€”å…ˆæŠŠå›¾åƒåˆ‡æˆ patch tokensï¼Œç„¶åéšæœºâ€œè…èš€ï¼ˆcorruptï¼‰â€ä¸€éƒ¨åˆ† tokenï¼Œè®©æ¨¡å‹å»é¢„æµ‹è¢«è…èš€éƒ¨åˆ†çš„å†…å®¹ã€‚</p>
<div class="callout-tldr">
<p>æ ¸å¿ƒæ˜¯æŠŠ ViT å½“æˆâ€œè§†è§‰ç‰ˆ BERTâ€:éšæœºé®ä½ï¼ˆæˆ–æ›¿æ¢ï¼‰ä¸€éƒ¨åˆ† patch tokenï¼Œå†é¢„æµ‹è¢«é®ä½ patch çš„ç›®æ ‡ï¼ˆè¿™é‡Œç”¨ç¦»æ•£é¢œè‰²ä½œä¸ºé¢„æµ‹æ ‡ç­¾ï¼‰ã€‚</p>
</div>
<p><img src="assets/Vision-Transformer-Self-Supervised.png" class="img-fluid"></p>
<p>ä»–ä»¬å¯¹ <strong>50% çš„ patch embedding</strong> åš corruptionï¼Œå¹¶é‡‡ç”¨ä¸ BERT ç±»ä¼¼çš„ 80/10/10 ç­–ç•¥: - 80%:ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„ <code>[mask]</code> embedding æ›¿æ¢ - 10%:æ›¿æ¢æˆå¦ä¸€å—éšæœº patch çš„ embedding - 10%:ä¿æŒä¸å˜ï¼ˆä½†ä»ç„¶ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼‰</p>
<p>è¿™ç§è®¾è®¡çš„ç›´è§‰æ˜¯:æ—¢è¦è®©æ¨¡å‹å­¦ä¼šâ€œæ ¹æ®ä¸Šä¸‹æ–‡è¡¥å…¨ç¼ºå¤±ä¿¡æ¯â€ï¼Œåˆè¦é¿å…æ¨¡å‹è¿‡åº¦ä¾èµ–æŸä¸€ç§å›ºå®šçš„ mask æ¨¡å¼ã€‚</p>
<p>ä»–ä»¬æœ€ç»ˆé€‰æ‹©äº†ä¸€ä¸ªéå¸¸è½»é‡ä½†æœ‰æ•ˆçš„é¢„æµ‹ç›®æ ‡:<br>
å¯¹æ¯ä¸ªè¢«è…èš€ patchï¼Œé¢„æµ‹å…¶ <strong>3-bit mean color</strong>ï¼ˆä¸€å…± <span class="math inline">\(2^9 = 512\)</span> ç§é¢œè‰²ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ª <strong>512-way åˆ†ç±»é—®é¢˜</strong>ï¼‰ã€‚</p>
<p>åŒæ—¶ä»–ä»¬ä¹Ÿå¯¹æ¯”è¿‡å‡ ç§ç›®æ ‡è®¾å®š: 1) åªé¢„æµ‹ä¸€ä¸ª mean 3-bit colorï¼ˆ512 åˆ†ç±»ï¼Œ<strong>1 ä¸ªé¢„æµ‹</strong>ï¼‰<br>
2) æŠŠ 16Ã—16 patch ä¸‹é‡‡æ ·æˆ 4Ã—4ï¼Œå†å¯¹æ¯ä¸ªå°æ ¼é¢„æµ‹ 3-bit colorï¼ˆ512 åˆ†ç±»ï¼Œ<strong>16 ä¸ªå¹¶è¡Œé¢„æµ‹</strong>ï¼‰<br>
3) ç›´æ¥å¯¹å®Œæ•´ patch åšåƒç´ çº§ L2 å›å½’ï¼ˆRGB é€šé“ä¸Šçš„å¯†é›†å›å½’ï¼‰</p>
<p>ç»“æœæ¯”è¾ƒæœ‰æ„æ€:ä¸‰ç§æ–¹å¼éƒ½èƒ½å¸¦æ¥ä¸é”™æ•ˆæœï¼Œä½† <strong>åƒç´  L2 å›å½’ç•¥å·®</strong>.</p>
<blockquote class="blockquote">
<p>We employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable <code>[mask]</code> embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%). This setup is very similar to the one used for language by BERT. Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations <cite> An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.14 </cite></p>
</blockquote>
<p>åç»­çš„å·¥ä½œä¹Ÿè¯´æ˜äº†è¿™ç§æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œæ¯”å¦‚SiMIM <span class="citation" data-cites="SimMIMSimpleFramework2022xie">(<a href="#ref-SimMIMSimpleFramework2022xie" role="doc-biblioref">Xie et al. 2022</a>)</span></p>
</section>
</section>
<section id="key-concepts" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Key Concepts</h1>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>Content</th>
<th>Explain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Patchify</td>
<td>æŠŠ <span class="math inline">\((H\times W)\)</span> å›¾åƒåˆ‡æˆ <span class="math inline">\((P\times P)\)</span> å°å—å¹¶å±•å¹³ï¼Œå¾—åˆ° token åºåˆ—ï¼Œæ˜¯ ViT çš„æ ¸å¿ƒâ€œè§†è§‰â†’åºåˆ—â€æ¥å£ã€‚</td>
</tr>
<tr class="even">
<td>Patch Embedding</td>
<td>ç”¨çº¿æ€§æŠ•å½± (<span class="math inline">\(E\)</span>) æŠŠæ¯ä¸ª patch ä» <span class="math inline">\((P^2C)\)</span> æ˜ å°„åˆ° Transformer ç»´åº¦ (<span class="math inline">\(D\)</span>)ã€‚</td>
</tr>
<tr class="odd">
<td>[CLS] Token</td>
<td>åœ¨åºåˆ—å‰åŠ å…¥å¯å­¦ä¹  tokenï¼Œç”¨å…¶è¾“å‡ºè¡¨ç¤ºæ•´å›¾å¹¶æ¥åˆ†ç±»å¤´ã€‚</td>
</tr>
<tr class="even">
<td>Positional Embedding</td>
<td>ViT ç”¨å¯å­¦ä¹  1D ä½ç½®ç¼–ç ï¼›åˆ†è¾¨ç‡å˜åŒ–æ—¶å¯¹ä½ç½®ç¼–ç åš 2D æ’å€¼ä»¥é€‚é…æ–° token ç½‘æ ¼ã€‚</td>
</tr>
<tr class="odd">
<td>Pre-LN Transformer Encoder</td>
<td>LayerNorm æ”¾åœ¨å­å±‚å‰ï¼Œé…åˆæ®‹å·®:MSA å’Œ MLP äº¤æ›¿å †å ã€‚</td>
</tr>
<tr class="even">
<td>Inductive Bias</td>
<td>CNN çš„å±€éƒ¨æ€§/å¹³ç§»ç­‰å˜æ€§ï¼›ViT å½’çº³åç½®æ›´å¼±ï¼Œæ›´ä¾èµ–æ•°æ®è§„æ¨¡ä¸è®­ç»ƒé…æ–¹ã€‚</td>
</tr>
<tr class="odd">
<td>Model Scaling (B/L/H)</td>
<td>Base/Large/Huge ä¸‰æ¡£å‚æ•°ä¸æ·±åº¦ï¼Œpatch è¶Šå°åºåˆ—è¶Šé•¿ã€è®¡ç®—æ›´è´µã€‚</td>
</tr>
</tbody>
</table>
</section>
<section id="qa" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Q&amp;A</h1>
<section id="question-1" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="question-1"><span class="header-section-number">5.1</span> Question 1</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 1:</span> <strong>ä¸ºä»€ä¹ˆ Vision Transformer éœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®ï¼Ÿ</strong></p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å› ä¸º ViT <strong>ç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„å½’çº³åç½®ï¼ˆinductive biasï¼‰</strong>ï¼Œä¾‹å¦‚å±€éƒ¨æ€§ï¼ˆlocalityï¼‰å’Œå¹³ç§»ä¸å˜æ€§ï¼ˆtranslation equivarianceï¼‰ï¼Œè¿™äº›èƒ½åŠ›éœ€è¦é€šè¿‡å¤§é‡æ•°æ®æ¥å­¦ä¹ ã€‚</p>
</div>
</div>
</div>
</section>
<section id="question-2" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="question-2"><span class="header-section-number">5.2</span> Question 2</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 2</span>:Eq.(1) é‡Œä¸ºä»€ä¹ˆè¦åŠ  [CLS] tokenï¼Œå®ƒå’Œå…¨å±€å¹³å‡æ± åŒ–æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>[CLS] token ç»™æ¨¡å‹ä¸€ä¸ªâ€œä¸“é—¨èšåˆä¿¡æ¯çš„æ§½ä½â€ï¼Œé€šè¿‡æ³¨æ„åŠ›ä¸»åŠ¨ä»æ‰€æœ‰ patch æ‹‰å–ä¿¡æ¯ï¼›è€Œ GAP æ˜¯è¢«åŠ¨å¹³å‡ã€‚è®ºæ–‡åœ¨é™„å½•å¯¹æ¯”è¿‡ä¸¤è€…è¡¨ç°æ¥è¿‘ï¼Œä½†å­¦ä¹ ç‡ç­‰é…æ–¹å¯èƒ½éœ€è¦ä¸åŒè°ƒæ•´ã€‚<a href="https://arxiv.org/pdf/2010.11929">arXiv+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-3" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="question-3"><span class="header-section-number">5.3</span> Question 3</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 3</span>:ViT ç”¨ 1D position embedding ä¸ä¼šä¸¢æ‰ 2D ç»“æ„å—ï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>ä¸¢æ‰äº†â€œæ˜¾å¼ 2D å½’çº³åç½®â€ï¼Œä½†ä½œè€…å‘ç°æ›´å¤æ‚çš„ 2D-aware ä½ç½®ç¼–ç å¹¶æ²¡æœ‰å¸¦æ¥æ˜¾è‘—æ”¶ç›Šï¼›ViT ä¾é æ•°æ®ä¸è®­ç»ƒä»å¤´å­¦ä¹ ç©ºé—´å…³ç³»ã€‚çœŸæ­£éœ€è¦ 2D çš„åœ°æ–¹ä¸»è¦åœ¨<strong>åˆ†è¾¨ç‡è¿ç§»æ—¶çš„ä½ç½®ç¼–ç æ’å€¼</strong>ã€‚<a href="https://arxiv.org/pdf/2010.11929">arXiv+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-4" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="question-4"><span class="header-section-number">5.4</span> Question 4</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 4</span>:ä¸ºä»€ä¹ˆè®ºæ–‡å¼ºè°ƒâ€œè§„æ¨¡è®­ç»ƒèƒœè¿‡å½’çº³åç½®â€ï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>è®ºæ–‡å®éªŒæ˜¾ç¤º:å°æ•°æ®é¢„è®­ç»ƒæ—¶å¼º CNNï¼ˆå¦‚ BiT ResNetï¼‰æ›´ç¨³ï¼›éšç€é¢„è®­ç»ƒæ•°æ®ä» ImageNet â†’ ImageNet-21k â†’ JFT-300M å¢å¤§ï¼ŒViT å¤§æ¨¡å‹çš„è¿ç§»æ€§èƒ½æ˜¾è‘—æå‡å¹¶åè¶… CNNï¼Œè¯´æ˜å¯¹ ViT è€Œè¨€æ•°æ®è§„æ¨¡æ˜¯å…³é”®ç“¶é¢ˆã€‚<a href="https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/attention/transformers/transformers_for_image_recognition%2C_dosovitskiy%2C_arxiv_2020.pdf">UofT Computer Science+1</a></p>
</div>
</div>
</div>
</section>
<section id="question-5" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="question-5"><span class="header-section-number">5.5</span> Question 5</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 5</span>:patch size é€‰ 16 è¿˜æ˜¯ 32 çš„ä¸»è¦æƒè¡¡æ˜¯ä»€ä¹ˆï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>patch è¶Šå°ï¼ˆå¦‚ 16ï¼‰â†’ token æ•° NNN è¶Šå¤§ â†’ æ³¨æ„åŠ›æ›´è´µä½†ç»†ç²’åº¦æ›´å¼ºï¼›patch è¶Šå¤§ï¼ˆå¦‚ 32ï¼‰â†’ æ›´çœç®—åŠ›ä½†å¯èƒ½æŸå¤±ç»†èŠ‚ã€‚è®ºæ–‡ä¹Ÿæ˜ç¡®æŒ‡å‡ºåºåˆ—é•¿åº¦ä¸ P2P^2P2 æˆåæ¯”ï¼Œå› æ­¤å° patch æ›´æ˜‚è´µã€‚</p>
</div>
</div>
</div>
</section>
<section id="question-6" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="question-6"><span class="header-section-number">5.6</span> Question 6</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 6</span>:ä¸ºä»€ä¹ˆ ViT åœ¨å°æ•°æ®é›†ä¸Šé€šå¸¸ä¸å¦‚ CNNï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å› ä¸º CNN é€šè¿‡å·ç§¯å’Œæƒé‡å…±äº«å†…ç½®äº†å¼ºå…ˆéªŒï¼Œè€Œ ViT éœ€è¦ä»æ•°æ®ä¸­å­¦ä¹ è¿™äº›å…ˆéªŒï¼Œåœ¨å°æ•°æ®æ¡ä»¶ä¸‹ä¸å¤Ÿé«˜æ•ˆã€‚</p>
</div>
</div>
</div>
</section>
<section id="question-7" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="question-7"><span class="header-section-number">5.7</span> Question 7</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 7</span>:åˆ†è¾¨ç‡å¾®è°ƒæ—¶ä¸ºä»€ä¹ˆè¦å¯¹ä½ç½®ç¼–ç åš 2D æ’å€¼ï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å› ä¸º token ç½‘æ ¼å¤§å°å˜äº†ï¼ˆNNN å˜äº†ï¼‰ï¼Œé¢„è®­ç»ƒçš„ EposE_{pos}Eposâ€‹ ä¸èƒ½ç›´æ¥å¯¹é½æ–°ä½ç½®ï¼›2D æ’å€¼è®©ä½ç½®ç¼–ç åœ¨ç©ºé—´ä¸Šâ€œå¹³æ»‘ä¼¸ç¼©â€ï¼Œä»è€Œå¤ç”¨é¢„è®­ç»ƒçŸ¥è¯†ã€‚</p>
</div>
</div>
</div>
</section>
<section id="question-8" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="question-8"><span class="header-section-number">5.8</span> Question 8</h2>
<div class="callout-question">
<p><span style="color:rgb(255, 0, 0)">Question 8</span>:ä¸ºä»€ä¹ˆ ViT éœ€è¦æŠŠå›¾åƒåˆ‡æˆ patchï¼Œè€Œä¸æ˜¯ç›´æ¥æŠŠæ¯ä¸ªåƒç´ å½“ tokenï¼Ÿ</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>åƒç´ çº§ token ä¼šè®©åºåˆ—é•¿åº¦å˜æˆ <span class="math inline">\(H \times W\)</span>ï¼Œè‡ªæ³¨æ„åŠ›å¤æ‚åº¦ <span class="math inline">\(\mathcal{O}(n^{2})\)</span> ç›´æ¥çˆ†ç‚¸ï¼›patchify æŠŠ NNN é™åˆ° <span class="math inline">\(\frac{HW}{P^2}\)</span>â€‹ï¼Œè®©æ ‡å‡†å…¨å±€æ³¨æ„åŠ›åœ¨å¯æ¥å—çš„ç®—åŠ›ä¸‹è¿è¡Œï¼ŒåŒæ—¶ä¿ç•™ç«¯åˆ°ç«¯å­¦ä¹ ç©ºé—´ç»“æ„çš„èƒ½åŠ›ã€‚</p>
</div>
</div>
</div>
</section>
</section>
<section id="related-resource-further-reading" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Related resource &amp; Further Reading</h1>
<p>åœ¨äº†è§£äº†ä»€ä¹ˆæ˜¯Vision Transformerä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹è¿™äº›è¿˜æœ‰é‚£äº›æå‡ï¼Œæ¯”å¦‚è¿™ç¯‡æ–‡ç« ï¼Œä»‹ç»äº†ä¸€äº›è®­ç»ƒViTçš„æŠ€å·§</p>
<p><a href="https://arxiv.org/pdf/2106.10270">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</a> è®ºæ–‡ä¸­æ€»èŠ‚äº†ä¸‰ç±»æ”¹è¿›ç­–ç•¥:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>ç±»åˆ«</th>
<th>ä»£è¡¨æ–¹æ³•</th>
<th>æ•ˆæœ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>æ•°æ®å¢å¼º (Data Augmentation)</strong></td>
<td>RandAugment, Mixup, CutMix, Random Erasing</td>
<td>æé«˜æ³›åŒ–ï¼ŒæŠµæ¶ˆæ•°æ®é‡ä¸è¶³</td>
</tr>
<tr class="even">
<td><strong>æ­£åˆ™åŒ– (Regularization)</strong></td>
<td>DropPathï¼ˆStochastic Depthï¼‰, Label Smoothing, Gradient Clipping</td>
<td>ç¨³å®šè®­ç»ƒã€é˜²æ­¢è¿‡æ‹Ÿåˆ</td>
</tr>
<tr class="odd">
<td><strong>ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦</strong></td>
<td>AdamW + Cosine LR + Warmup</td>
<td>å¯¹ ViT ç‰¹åˆ«å…³é”®</td>
</tr>
</tbody>
</table>
<section id="å‡å°‘tokensçš„æŠ€å·§" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="å‡å°‘tokensçš„æŠ€å·§"><span class="header-section-number">6.1</span> å‡å°‘Tokensçš„æŠ€å·§</h2>
<section id="patch-merge" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="patch-merge"><span class="header-section-number">6.1.1</span> Patch Merge</h3>
<p>ç±»ä¼¼äº Swin Transformer<span class="citation" data-cites="SwinTransformerHierarchical2021liu">(<a href="#ref-SwinTransformerHierarchical2021liu" role="doc-biblioref">Liu et al. 2021</a>)</span> çš„åšæ³•:åœ¨ä¸åŒå±‚å°†ç›¸é‚» patch åˆå¹¶ï¼ˆä¾‹å¦‚ 2Ã—2 â†’ 1ï¼‰ï¼Œå‡å°‘ token æ•°ï¼Œä½¿æ¨¡å‹å±‚çº§åŒ–ã€‚</p>
</section>
<section id="pixel-shuffle" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="pixel-shuffle"><span class="header-section-number">6.1.2</span> Pixel Shuffle</h3>
<p><img src="assets/Vision-Transformer-Pixel-Shuffle.png" class="img-fluid"></p>
</section>
</section>
<section id="vision-language-model" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="vision-language-model"><span class="header-section-number">6.2</span> Vision Language Model</h2>
<p>æˆ‘ä»¬ä»¥åŠå­¦ä¹ äº†ViT for computer Visionï¼Œ <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html">Transformer</a> for NLPï¼Œ æ¥ä¸‹æ¥æœ‰ä»€ä¹ˆåŠæ³•è®©è¿™ä¸¤ç§æ¨¡å‹ç»“åˆèµ·æ¥å‘¢ï¼Ÿ CLIP <span class="citation" data-cites="LearningTransferableVisual2021radford">(<a href="#ref-LearningTransferableVisual2021radford" role="doc-biblioref">Radford et al. 2021</a>)</span>: å°† ViT èåˆåˆ° vision-language é¢„è®­ç»ƒä¸­ã€‚æˆ‘ä»¬ä¹‹åä¼šå­¦ä¹ è¿™ç¯‡æ–‡ç« ã€‚</p>
</section>
<section id="video-transformer" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="video-transformer"><span class="header-section-number">6.3</span> Video Transformer</h2>
<p>åœ¨å­¦ä¹ äº†å¦‚ä½•å°†Transformeråº”ç”¨åˆ°Imageä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ›´è¿‘ä¸€æ­¥ï¼Œçœ‹çœ‹å¦‚ä½•å°†Transformeråº”ç”¨åˆ°Video æ¨¡æ€ä¸­ã€‚ViViT<span class="citation" data-cites="ViViTVideoVision2021arnab">(<a href="#ref-ViViTVideoVision2021arnab" role="doc-biblioref">Arnab et al. 2021</a>)</span> çš„æå‡ºï¼Œå°±æ˜¯å°†Transformeråº”ç”¨åˆ°Videoã€‚ä¸­ï¼Œæˆ‘ä»¬ä¹‹åä¼šå­¦ä¹ åˆ°è¿™ä¸€ç¯‡ã€‚</p>
</section>
<section id="native-resolution" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="native-resolution"><span class="header-section-number">6.4</span> Native Resolution</h2>
<p>åœ¨ViT ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†å›¾ç‰‡Resizeåˆ°ç›¸åŒå¤§å°çš„å›¾ç‰‡ï¼Œé‚£æˆ‘ä»¬å°±æƒ³ä¸Resizeï¼Œèƒ½ä¸èƒ½ç›´æ¥è®­â€œä¸åŒå¤§å°/ä¸åŒé•¿å®½æ¯”â€çš„å›¾ï¼Ÿ NaViT <span class="citation" data-cites="PatchPackNaViT2023dehghani">(<a href="#ref-PatchPackNaViT2023dehghani" role="doc-biblioref">Dehghani et al. 2023</a>)</span>æå‡ºæ¯å¼ å›¾åœ¨â€œåŸå§‹åˆ†è¾¨ç‡/åŸå§‹é•¿å®½æ¯”â€ä¸‹åˆ‡æˆ patch token åºåˆ—ï¼Œç„¶åæŠŠå¤šå¼ å›¾çš„ token åºåˆ—â€œæ‰“åŒ…ï¼ˆpackingï¼‰â€åˆ°åŒä¸€æ¡å›ºå®šé•¿åº¦åºåˆ—é‡Œè®­ç»ƒï¼Œç”¨ <strong>attention mask</strong> ä¿è¯ä¸åŒå›¾ç‰‡ä¹‹é—´äº’ä¸â€œä¸²é—¨â€ã€‚</p>
<p><img src="assets/Vision-Transformer-NaViT.png" class="img-fluid"></p>
<ul>
<li><strong>Patch nâ€™ Pack</strong>:æŠŠå¤šå¼ ä¸åŒåˆ†è¾¨ç‡å›¾ç‰‡çš„ patch token åºåˆ—æ‹¼æˆä¸€ä¸ªå®šé•¿ packed sequenceï¼Œä»¥å‡å°‘ padding/resize æµªè´¹<br>
</li>
<li><strong>Attention Mask</strong>:ä½¿ç”¨ block-diagonal maskï¼Œè®©åŒä¸€å¼ å›¾çš„ token æ‰èƒ½äº’ç›¸ attentionï¼Œä¸åŒå›¾ä¹‹é—´å®Œå…¨éš”ç¦»</li>
<li><strong>ä½ç½®ç¼–ç </strong>:é‡‡ç”¨ factorizedï¼ˆx/y åˆ†è§£ï¼‰ä½ç½®ç¼–ç ï¼Œå¹¶å¯ç”¨ fractional åæ ‡æå‡å¯¹æœªè§åˆ†è¾¨ç‡çš„æ³›åŒ–</li>
</ul>
<table class="table">
<colgroup>
<col style="width: 5%">
</colgroup>
<tbody>
<tr class="odd">
<td># Summary æ­å–œä½ çœ‹åˆ°äº†è¿™é‡Œï¼ŒViTçš„è®ºæ–‡æ¶æ„æ€æƒ³å¾ˆç®€å•ï¼Œä½†å°±æ˜¯è¿™ç§å¤§é“è‡³ç®€çš„æ–¹å¼ï¼Œæ‰è®©å®ƒè¿™ä¸ªå·¥ä½œå˜å¾—å‡ºå½©ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç»†çœ‹ViTçš„è®ºæ–‡ï¼Œè¿˜æ˜¯æœ‰å¾ˆå¤šå¯å­¦ä¹ çš„åœ°æ–¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å›é¡¾ä¸€ä¸‹ã€‚</td>
</tr>
<tr class="even">
<td>åœ¨æœ¬ç« ä¸­ï¼Œä»å·²ç†Ÿæ‚‰ Transformer çš„å‰æå‡ºå‘ï¼Œå›´ç»• <strong>â€œå¦‚ä½•å°† Transformer åº”ç”¨äºè®¡ç®—æœºè§†è§‰â€</strong> è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œç³»ç»Ÿæ¢³ç†äº† Vision Transformerï¼ˆViTï¼‰çš„æ•´ä½“è®¾è®¡ä¸å®ç°æ€è·¯ã€‚æ–‡ç« é¦–å…ˆä»‹ç»äº† ViT çš„åŸºæœ¬æ¶æ„ï¼Œè¯´æ˜å…¶æœ¬è´¨æ˜¯å°†äºŒç»´å›¾åƒé€šè¿‡ <strong>Patchify</strong> è½¬åŒ–ä¸ºä¸€ç»´åºåˆ—ï¼Œå¹¶ç»“åˆ <strong>Patch Embeddingã€Position Embedding ä¸ [CLS] token</strong>ï¼Œä½¿å›¾åƒèƒ½å¤Ÿè¢«æ ‡å‡†çš„ Transformer Encoder å¤„ç†ã€‚éšåä»‹ç»äº† Patch Embedding çš„åŠ¨æœºä¸å®ç°:<em>ä»åƒç´ çº§ç›´æ¥å±•å¼€æ‰€å¸¦æ¥çš„åºåˆ—è¿‡é•¿ä¸è®¡ç®—å¤æ‚åº¦é—®é¢˜å‡ºå‘ï¼Œå¼•å‡ºå°†ç›¸é‚»åƒç´ ç»„åˆæˆ patch çš„å¿…è¦æ€§</em>ï¼Œå¹¶ç»™å‡ºäº†åŸºäº <code>einops</code> çš„ç›´è§‚å®ç°ä»¥åŠä½¿ç”¨ <strong>Conv2d ç­‰ä»·å®ç°</strong> çš„å·¥ç¨‹åŒ–å†™æ³•ã€‚</td>
</tr>
<tr class="odd">
<td>åœ¨ä½ç½®ç¼–ç éƒ¨åˆ†ï¼Œå¯¹æ¯”äº† ViT ä¸­é‡‡ç”¨çš„ <strong>Learned Position Embedding</strong> ä¸ä¼ ç»Ÿ Transformer çš„æ­£ä½™å¼¦ä½ç½®ç¼–ç ï¼Œå¹¶ç»“åˆè®ºæ–‡å®éªŒè¯´æ˜ï¼Œåœ¨ patch-level è¾“å…¥ä¸‹ï¼Œä¸åŒä½ç½®ç¼–ç ç­–ç•¥åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚å¹¶ä¸æ˜¾è‘—ï¼›åŒæ—¶ä»‹ç»äº†åœ¨ fine-tuning åˆ°ä¸åŒè¾“å…¥åˆ†è¾¨ç‡æ—¶ï¼Œå¦‚ä½•é€šè¿‡ <strong>äºŒç»´æ’å€¼ï¼ˆ2D interpolationï¼‰</strong> æ‰©å±•é¢„è®­ç»ƒä½ç½®ç¼–ç ä»¥é€‚é…æ–°çš„ patch æ•°é‡ã€‚éšåä»è¡¨ç¤ºå­¦ä¹ çš„è§’åº¦è®¨è®ºäº† <strong>[CLS] token</strong> åœ¨ ViT ä¸­çš„ä½œç”¨ï¼Œå¹¶ä¸å…¨å±€å¹³å‡æ± åŒ–ï¼ˆGAPï¼‰è¿›è¡Œå¯¹æ¯”ï¼ŒæŒ‡å‡ºä¸¤ç§æ–¹å¼çš„æ€§èƒ½å·®å¼‚ä¸»è¦æ¥æºäºè®­ç»ƒæ—¶å­¦ä¹ ç‡ç­‰è¶…å‚æ•°è®¾ç½®ï¼Œè€Œéç»“æ„æœ¬èº«ã€‚</td>
</tr>
<tr class="even">
<td>åœ¨æ¨¡å‹ç»“æ„å±‚é¢ï¼Œå¼ºè°ƒäº† ViT Encoder ä¸åŸå§‹ Transformer Encoder çš„å…³é”®å·®å¼‚ï¼ŒåŒ…æ‹¬ <strong>Pre-Norm è®¾è®¡</strong> ä»¥åŠä½¿ç”¨ <strong>GELU</strong> æ¿€æ´»å‡½æ•°ã€‚æ¥ç€ä» <strong>Inductive Bias</strong> çš„è§†è§’å¯¹æ¯”äº† CNN ä¸ ViT:CNN é€šè¿‡å·ç§¯å¤©ç„¶å¼•å…¥å±€éƒ¨æ€§ã€äºŒç»´ç»“æ„å’Œå¹³ç§»ç­‰å˜æ€§ç­‰å…ˆéªŒï¼Œè€Œ ViT çš„å½’çº³åç½®è¾ƒå¼±ï¼Œæ›´å¤šä¾èµ–æ•°æ®å’Œè®­ç»ƒè¿‡ç¨‹æ¥å­¦ä¹ è¿™äº›ç©ºé—´è§„å¾‹ï¼Œè¿™ä¹Ÿè§£é‡Šäº† ViT åœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°å—é™ã€ä½†åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¡ä»¶ä¸‹æ½œåŠ›æ˜¾è‘—çš„åŸå› ã€‚æœ€åï¼Œé€šè¿‡åœ¨ CIFAR-10 ä¸Šè®­ç»ƒä¸€ä¸ª toy ViT çš„å®éªŒï¼Œå±•ç¤ºäº†æ¨¡å‹çš„åˆ†ç±»æ•ˆæœä¸æ³¨æ„åŠ›å¯è§†åŒ–ï¼Œå¹¶åˆ†æäº† attention map æ¥è¿‘å‡åŒ€çš„ç°è±¡å¯èƒ½ä¸ <strong>ä½åˆ†è¾¨ç‡å’Œæ•°æ®è§„æ¨¡æœ‰é™</strong> æœ‰å…³ï¼ŒåŒæ—¶ç»™å‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›ä¸æ‰©å±•çš„æ–¹å‘ï¼Œä¸ºåç»­æ›´é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ä»¥åŠå¤šæ¨¡æ€ã€è§†é¢‘æ¨¡å‹çš„å­¦ä¹ å¥ å®šåŸºç¡€ã€‚</td>
</tr>
</tbody>
</table>
<p>åˆ›ä½œä¸æ˜“ï¼Œå¦‚æœä½ è§‰å¾—å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿è¯·æˆ‘å–æ¯å’–å•¡/æ”¯ä»˜å®çº¢åŒ…ï¼Œæ”¯æŒæˆ‘ç»§ç»­åˆ›ä½œï¼ä½ ä»¬çš„æ”¯æŒæ˜¯æˆ‘æœ€å¤§çš„åŠ¨åŠ›ï¼ :) <br> <img src="../../../style/AliPay.jpg" class="img-fluid" width="300"></p>
</section>
</section>
<section id="appendix" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Appendix</h1>
<section id="axial-attentionè½´å‘æ³¨æ„åŠ›" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="axial-attentionè½´å‘æ³¨æ„åŠ›"><span class="header-section-number">7.1</span> Axial Attentionï¼ˆè½´å‘æ³¨æ„åŠ›ï¼‰</h2>
<p>åœ¨å¤„ç† å›¾åƒæˆ–è§†é¢‘ è¿™ç±»é«˜ç»´è¾“å…¥æ—¶ï¼Œå¦‚æœç›´æ¥å¯¹æ‰€æœ‰åƒç´ åš å…¨å±€ self-attentionï¼Œå¤æ‚åº¦æ˜¯ <span class="math inline">\(\mathcal{O}(H^2 W^2)\)</span>å½“å›¾åƒå¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªä»£ä»·å¤ªé«˜ã€‚ æ ¸å¿ƒæƒ³æ³•:æŠŠäºŒç»´ attention æ‹†æˆä¸¤æ¬¡ä¸€ç»´ attentionï¼ˆæ²¿ç€å›¾åƒçš„ä¸¤ä¸ªâ€œè½´â€åˆ†åˆ«åšï¼‰ã€‚ 1. Row-wise Attentionï¼ˆè¡Œæ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€æ°´å¹³æ–¹å‘ï¼ˆå®½åº¦è½´ Wï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€è¡Œçš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦:<span class="math inline">\(\mathcal{O}(H \cdot W^2)\)</span>ã€‚ 2. Column-wise Attentionï¼ˆåˆ—æ³¨æ„åŠ›ï¼‰ â€¢ æ²¿ç€å‚ç›´æ–¹å‘ï¼ˆé«˜åº¦è½´ Hï¼‰åšæ³¨æ„åŠ›ï¼Œæ¯ä¸€åˆ—çš„åƒç´ äº’ç›¸å…³æ³¨ã€‚ â€¢ å¤æ‚åº¦: <span class="math inline">\(\mathcal{O}(W \cdot H^2)\)</span>ã€‚</p>
<p>ç»„åˆèµ·æ¥ï¼Œç›¸å½“äºåœ¨ H å’Œ W ä¸¤ä¸ªè½´ä¸Šéƒ½åšäº†å…¨å±€ä¾èµ–å»ºæ¨¡ã€‚ <img src="assets/axial-attention.png" id="fig-axial-attention" class="img-fluid"></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-QuantifyingAttentionFlow2020abnar" class="csl-entry" role="listitem">
Abnar, Samira, and Willem Zuidema. 2020. <span>â€œQuantifying <span>Attention Flow</span> in <span>Transformers</span>.â€</span> May 31, 2020. <a href="https://doi.org/10.48550/arXiv.2005.00928">https://doi.org/10.48550/arXiv.2005.00928</a>.
</div>
<div id="ref-ViViTVideoVision2021arnab" class="csl-entry" role="listitem">
Arnab, Anurag, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÄiÄ‡, and Cordelia Schmid. 2021. <span>â€œ<span>ViViT</span>: <span>A Video Vision Transformer</span>.â€</span> November 1, 2021. <a href="https://doi.org/10.48550/arXiv.2103.15691">https://doi.org/10.48550/arXiv.2103.15691</a>.
</div>
<div id="ref-PatchPackNaViT2023dehghani" class="csl-entry" role="listitem">
Dehghani, Mostafa, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, et al. 2023. <span>â€œPatch nâ€™ <span>Pack</span>: <span>NaViT</span>, a <span>Vision Transformer</span> for Any <span>Aspect Ratio</span> and <span>Resolution</span>.â€</span> July 12, 2023. <a href="https://doi.org/10.48550/arXiv.2307.06304">https://doi.org/10.48550/arXiv.2307.06304</a>.
</div>
<div id="ref-BERTPretrainingDeep2019devlin" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>â€œ<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.â€</span> May 24, 2019. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-ImageWorth16x162021dosovitskiy" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>â€œAn <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image Recognition</span> at <span>Scale</span>.â€</span> June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.
</div>
<div id="ref-SwinTransformerHierarchical2021liu" class="csl-entry" role="listitem">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>â€œSwin <span>Transformer</span>: <span>Hierarchical Vision Transformer</span> Using <span>Shifted Windows</span>.â€</span> August 17, 2021. <a href="https://doi.org/10.48550/arXiv.2103.14030">https://doi.org/10.48550/arXiv.2103.14030</a>.
</div>
<div id="ref-LearningTransferableVisual2021radford" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>â€œLearning <span>Transferable Visual Models From Natural Language Supervision</span>.â€</span> February 26, 2021. <a href="https://doi.org/10.48550/arXiv.2103.00020">https://doi.org/10.48550/arXiv.2103.00020</a>.
</div>
<div id="ref-GradCAMVisualExplanations2020selvaraju" class="csl-entry" role="listitem">
Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2020. <span>â€œGrad-<span>CAM</span>: <span>Visual Explanations</span> from <span>Deep Networks</span> via <span class="nocase">Gradient-based Localization</span>.â€</span> <em>International Journal of Computer Vision</em> 128 (2): 336â€“59. <a href="https://doi.org/10.1007/s11263-019-01228-7">https://doi.org/10.1007/s11263-019-01228-7</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>â€œAttention <span>Is All You Need</span>.â€</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-SimMIMSimpleFramework2022xie" class="csl-entry" role="listitem">
Xie, Zhenda, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 2022. <span>â€œ<span>SimMIM</span>: <span>A Simple Framework</span> for <span>Masked Image Modeling</span>.â€</span> April 17, 2022. <a href="https://doi.org/10.48550/arXiv.2111.09886">https://doi.org/10.48550/arXiv.2111.09886</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/sta210-s22\.github\.io\/website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="pagination-link" aria-label="Transformer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Transformer</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/DLFaC/index.html" class="pagination-link" aria-label="ğŸ“š DLFaC">
        <span class="nav-page-text">ğŸ“š DLFaC</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with â¤ï¸ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>