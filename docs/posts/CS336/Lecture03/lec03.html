<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Lecture03 介绍了现代大语言模型的核心架构与超参数设计。课程对比了原始 Transformer 与主流 LLaMA-like 架构，总结了 pre-norm、RMSNorm、SwiGLU、RoPE 等关键设计的经验共识，并结合大量近期模型实践，讲解了 MLP 宽度比例、注意力头配置、模型深宽比与词表规模等超参数选择原则。同时还介绍了 z-loss、QK-Norm、MQA/GQA 等用于提升训练稳定性和推理效率的关键技巧。">

<title>Lecture 03: LM Model Architecture &amp; Hyperparameters – Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/CS336/Lecture04/lec04.html" rel="next">
<link href="../../../posts/CS336/Lecture02/lec02.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-c0eab5a31fbea23c8affb95fb4fbb9c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // 可访问性（键盘）
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/CS336/index.html">Stanford CS336: LLM from Scratch</a></li><li class="breadcrumb-item"><a href="../../../posts/CS336/Lecture03/lec03.html">Lecture 03: Transformer LM Architecture</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../../logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/sta210-s22" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">100 AI Papers</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformer-架构回顾" id="toc-transformer-架构回顾" class="nav-link active" data-scroll-target="#transformer-架构回顾"><span class="header-section-number">1</span> Transformer 架构回顾</a></li>
  <li><a href="#现代大语言模型架构改进" id="toc-现代大语言模型架构改进" class="nav-link" data-scroll-target="#现代大语言模型架构改进"><span class="header-section-number">2</span> 现代大语言模型架构改进</a>
  <ul>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization"><span class="header-section-number">2.1</span> Normalization</a>
  <ul>
  <li><a href="#position-of-normalization" id="toc-position-of-normalization" class="nav-link" data-scroll-target="#position-of-normalization"><span class="header-section-number">2.1.1</span> Position of Normalization</a>
  <ul class="collapse">
  <li><a href="#post-norm" id="toc-post-norm" class="nav-link" data-scroll-target="#post-norm"><span class="header-section-number">2.1.1.1</span> Post-Norm</a></li>
  <li><a href="#pre-norm" id="toc-pre-norm" class="nav-link" data-scroll-target="#pre-norm"><span class="header-section-number">2.1.1.2</span> Pre-Norm</a></li>
  <li><a href="#other-positions" id="toc-other-positions" class="nav-link" data-scroll-target="#other-positions"><span class="header-section-number">2.1.1.3</span> Other Positions</a></li>
  </ul></li>
  <li><a href="#normalization-types" id="toc-normalization-types" class="nav-link" data-scroll-target="#normalization-types"><span class="header-section-number">2.1.2</span> Normalization Types</a></li>
  </ul></li>
  <li><a href="#activations" id="toc-activations" class="nav-link" data-scroll-target="#activations"><span class="header-section-number">2.2</span> Activations</a>
  <ul>
  <li><a href="#gated-activationsglu" id="toc-gated-activationsglu" class="nav-link" data-scroll-target="#gated-activationsglu"><span class="header-section-number">2.2.1</span> Gated Activations(*GLU)</a></li>
  </ul></li>
  <li><a href="#serial-parallel-mlp" id="toc-serial-parallel-mlp" class="nav-link" data-scroll-target="#serial-parallel-mlp"><span class="header-section-number">2.3</span> Serial &amp; Parallel MLP</a></li>
  <li><a href="#position-encoding" id="toc-position-encoding" class="nav-link" data-scroll-target="#position-encoding"><span class="header-section-number">2.4</span> Position Encoding</a>
  <ul>
  <li><a href="#rotary-position-embedding-rope" id="toc-rotary-position-embedding-rope" class="nav-link" data-scroll-target="#rotary-position-embedding-rope"><span class="header-section-number">2.4.1</span> Rotary Position Embedding (RoPE)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#hyper-parameters" id="toc-hyper-parameters" class="nav-link" data-scroll-target="#hyper-parameters"><span class="header-section-number">3</span> Hyper-Parameters</a>
  <ul>
  <li><a href="#mlp-width" id="toc-mlp-width" class="nav-link" data-scroll-target="#mlp-width"><span class="header-section-number">3.1</span> MLP Width</a></li>
  <li><a href="#attention-heads" id="toc-attention-heads" class="nav-link" data-scroll-target="#attention-heads"><span class="header-section-number">3.2</span> Attention Heads</a></li>
  <li><a href="#aspect-ratio" id="toc-aspect-ratio" class="nav-link" data-scroll-target="#aspect-ratio"><span class="header-section-number">3.3</span> Aspect Ratio</a></li>
  <li><a href="#vocabulary-size" id="toc-vocabulary-size" class="nav-link" data-scroll-target="#vocabulary-size"><span class="header-section-number">3.4</span> Vocabulary Size</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">3.5</span> Regularization</a></li>
  </ul></li>
  <li><a href="#stability-tricks" id="toc-stability-tricks" class="nav-link" data-scroll-target="#stability-tricks"><span class="header-section-number">4</span> Stability Tricks</a></li>
  <li><a href="#attentions" id="toc-attentions" class="nav-link" data-scroll-target="#attentions"><span class="header-section-number">5</span> Attentions</a>
  <ul>
  <li><a href="#grouped-query-attention-gqa-multi-query-attention-mqa" id="toc-grouped-query-attention-gqa-multi-query-attention-mqa" class="nav-link" data-scroll-target="#grouped-query-attention-gqa-multi-query-attention-mqa"><span class="header-section-number">5.1</span> Grouped Query Attention (GQA) / Multi-Query Attention (MQA)</a></li>
  <li><a href="#sparse-sliding-window-attention" id="toc-sparse-sliding-window-attention" class="nav-link" data-scroll-target="#sparse-sliding-window-attention"><span class="header-section-number">5.2</span> Sparse / Sliding Window Attention</a></li>
  <li><a href="#sliding-window-attention" id="toc-sliding-window-attention" class="nav-link" data-scroll-target="#sliding-window-attention"><span class="header-section-number">5.3</span> Sliding Window Attention</a></li>
  <li><a href="#interleaved-attention" id="toc-interleaved-attention" class="nav-link" data-scroll-target="#interleaved-attention"><span class="header-section-number">5.4</span> Interleaved Attention</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/CS336/index.html">Stanford CS336: LLM from Scratch</a></li><li class="breadcrumb-item"><a href="../../../posts/CS336/Lecture03/lec03.html">Lecture 03: Transformer LM Architecture</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Lecture 03: LM Model Architecture &amp; Hyperparameters</h1>
</div>

<div>
  <div class="description">
    Lecture03 介绍了现代大语言模型的核心架构与超参数设计。课程对比了原始 Transformer 与主流 LLaMA-like 架构，总结了 pre-norm、RMSNorm、SwiGLU、RoPE 等关键设计的经验共识，并结合大量近期模型实践，讲解了 MLP 宽度比例、注意力头配置、模型深宽比与词表规模等超参数选择原则。同时还介绍了 z-loss、QK-Norm、MQA/GQA 等用于提升训练稳定性和推理效率的关键技巧。
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Lecture 03 介绍了现代大模型的核心架构与不同的超参数设计。这节课是理解大模型的基础，内容比较多，建议多看几遍。 本节课的目标是了解下图中的内容:</p>
<div id="fig-lecture03-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lecture03-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/lec03-overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lecture03-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: 现代的大语言模型架构。
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>课题的前置条件是理解 Transformer<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span> 模型。 对于那些不理解的同学，可以先学习一下我的这篇文章 <a href="https://yyzhang2025.github.io/posts/PapersWithCode/01-transformer/Transformer.html">100 Paper with Code: Transformer</a>。 在这篇文章中，我详细介绍了 Transformer 的架构与原理，并且配有代码实现，方便大家理解。</p>
</div>
</div>
<section id="transformer-架构回顾" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Transformer 架构回顾</h1>
<p>我们先回顾一下 Transformer 的基本架构。 Transformer 由 Vaswani 等人在 2017 年提出<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>，其核心思想是使用自注意力机制来捕捉序列数据中的长距离依赖关系。 Transformer 由 Encoder 和 Decoder 两部分组成，但在大语言模型中，我们通常只使用 Decoder 部分。</p>
<p>Decoder 由多个相同的层堆叠而成，每一层包括以下几个主要组件:</p>
<ul>
<li>位置编码 (Positional Encoding): 用于引入序列中单词的位置信息，因为自注意力机制本身不具备顺序信息。</li>
<li>多头自注意力机制 (Multi-Head Self-Attention): 允许模型在不同的表示子空间中关注输入序列的不同部分。</li>
<li>前馈神经网络 (Feed-Forward Neural Network): 通常由两个线性变换和一个非线性激活函数组成，用于对每个位置的表示进行进一步处理。</li>
<li>残差连接 (Residual Connections)： 有助于缓解深层网络中的梯度消失问题。</li>
<li>层归一化 (Layer Normalization)： 用于稳定训练过程，提高模型的收敛速度。</li>
</ul>
<p>接下来，我们看看现代大语言模型在 Transformer 基础上做了哪些改进。</p>
</section>
<section id="现代大语言模型架构改进" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> 现代大语言模型架构改进</h1>
<section id="normalization" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="normalization"><span class="header-section-number">2.1</span> Normalization</h2>
<div class="callout-question">
<p>为什么需要 Normalization？</p>
<p>Normalization 技术在深度学习中起到了稳定训练过程和加速收敛的作用。 它通过调整神经网络层的输入分布，减少了内部协变量偏移 (Internal Covariate Shift)，从而使得模型在训练过程中更加稳定。此外，Normalization 还可以帮助缓解梯度消失和梯度爆炸问题，提高模型的泛化能力。</p>
</div>
<section id="position-of-normalization" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="position-of-normalization"><span class="header-section-number">2.1.1</span> Position of Normalization</h3>
<section id="post-norm" class="level4" data-number="2.1.1.1">
<h4 data-number="2.1.1.1" class="anchored" data-anchor-id="post-norm"><span class="header-section-number">2.1.1.1</span> Post-Norm</h4>
<p>在Transformer中，Normalization放置在Sublayer之后，也就是所谓的 Post-Norm 结构。用数学公式表示为:</p>
<p><span id="eq-post-norm"><span class="math display">\[
\text{Post-Norm: } \quad \text{Norm}(x + \text{Sublayer}(x))
\tag{1}\]</span></span></p>
<p>然而，<span class="citation" data-cites="LayerNormalization2016ba">(<a href="#ref-LayerNormalization2016ba" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span> 指出，Post-Norm存在以下的问题:</p>
<ul>
<li>梯度不稳定，特别是在初始化阶段： 论文使用平均场理论分析指出，在 Post-Norm 结构 下（即 LayerNorm 放在残差连接之后），靠近输出层的参数在初始化时梯度期望值很大。 这会导致训练初期梯度爆炸，从而影响模型的稳定性和收敛速度。</li>
<li>依赖复杂的 warm-up 超参数调优： 由于 Post-Norm 结构在训练初期容易出现梯度不稳定的问题，因此需要使用复杂的学习率 warm-up 策略来缓解这一问题。 这增加了模型训练的复杂性和调优难度。</li>
</ul>
<blockquote class="blockquote">
<p>Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, <u>the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable.</u> The warm-up stage is practically helpful for avoiding this problem. <cite> On Layer Normalization in the Transformer Architecture P.1 </cite></p>
</blockquote>
</section>
<section id="pre-norm" class="level4" data-number="2.1.1.2">
<h4 data-number="2.1.1.2" class="anchored" data-anchor-id="pre-norm"><span class="header-section-number">2.1.1.2</span> Pre-Norm</h4>
<p>为了缓解 Post-Norm 的问题，<span class="citation" data-cites="LayerNormalization2016ba">(<a href="#ref-LayerNormalization2016ba" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span> 提出了 Pre-Norm 结构，即将 Normalization 放置在 Sublayer 之前。 用数学公式表示为:</p>
<p><span id="eq-pre-norm"><span class="math display">\[
\text{Pre-Norm: } \quad x + \text{Sublayer}(\text{Norm}(x))
\tag{2}\]</span></span></p>
<p>下图展示了 Post-Norm 与 Pre-Norm 结构的对比:</p>
<div id="fig-post-norm-pre-norm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-post-norm-pre-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/post-norm-pre-norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-post-norm-pre-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Post Norm 与 Pre-Norm 结构对比。
</figcaption>
</figure>
</div>
<p>Pre-Norm 有以下优点:</p>
<ul>
<li>提高训练稳定性： Pre-Norm 结构通过在每个子层之前进行归一化，减少了梯度爆炸和梯度消失的风险，从而提高了训练的稳定性。</li>
<li>简化超参数调优： 由于 Pre-Norm 结构在训练过程中更加稳定，因此不再需要复杂的学习率 warm-up 策略，简化了模型的训练过程和超参数调优。</li>
</ul>
<blockquote class="blockquote">
<p>We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications. <cite> On Layer Normalization in the Transformer Architecture P.1 </cite></p>
</blockquote>
<p>接下来我们来看一下为什么Pre-Norm能有这么多的好处：</p>
<ul>
<li>Gradient Attenuation</li>
</ul>
<div id="fig-pre-post-norm-gradient-attenuation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pre-post-norm-gradient-attenuation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/pre-post-norm-gradient-attenuation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pre-post-norm-gradient-attenuation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 上图展示了 Pre-Norm 与 Post-Norm Transformer 在梯度行为上的差异。可以看到，在初始化阶段，<u>Post-Norm 会导致靠近输出层的梯度期望值显著增大，而底层梯度被明显削弱</u>，呈现出严重的梯度失衡（gradient attenuation across layers），从而使训练过程极不稳定，必须依赖学习率 warm-up 进行缓解。相比之下，P<u>re-Norm 在初始化时各层梯度规模保持一致且稳定，避免了梯度衰减与爆炸问题</u>。
</figcaption>
</figure>
</div>
<ul>
<li>Gradient Spike / Noise</li>
</ul>
<p>实验表明 <span class="citation" data-cites="TransformersTearsImproving2019nguyen">(<a href="#ref-TransformersTearsImproving2019nguyen" role="doc-biblioref">Nguyen and Salazar 2019</a>)</span></p>
<div id="fig-pre-post-norm-graidnet-spike" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pre-post-norm-graidnet-spike-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/pre-post-norm-graidnet-spike.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-pre-post-norm-graidnet-spike-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4
</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>Post-norm produces noisy gradients with many sharp spikes, even towards the end of training. On the other hand, Pre-norm has fewer noisy gradients with smaller sizes, even without warmup. <cite> Transformers without Tears: Improving the Normalization of Self-Attention P.6 </cite></p>
</blockquote>
</section>
<section id="other-positions" class="level4" data-number="2.1.1.3">
<h4 data-number="2.1.1.3" class="anchored" data-anchor-id="other-positions"><span class="header-section-number">2.1.1.3</span> Other Positions</h4>
<p>除了 Pre-Norm 和 Post-Norm 之外，还有一些其他的 Normalization 位置设计，例如:</p>
<ul>
<li>Sandwich Norm: 将 Normalization 放置在每个子层的输入和输出之间。</li>
<li>Double Norm: 在每个子层的输入和输出都进行归一化。</li>
</ul>
<p>从这些实验结果来看，我们可以得出结论： - 尽量保持Residual Connection两端的信号稳定是非常重要的, 这可以保证梯度在网络中顺利传播。</p>
</section>
</section>
<section id="normalization-types" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="normalization-types"><span class="header-section-number">2.1.2</span> Normalization Types</h3>
<p>除了 Normalization 的位置设计之外，Normalization 的形式也是一个重要的设计选择。 常见的 Normalization 形式包括:</p>
<ul>
<li>LayerNorm<span class="citation" data-cites="LayerNormalization2016ba">(<a href="#ref-LayerNormalization2016ba" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>: 对每个样本的特征维度进行归一化，适用于序列数据。</li>
<li>RMSNorm<span class="citation" data-cites="RootMeanSquare2019zhang">(<a href="#ref-RootMeanSquare2019zhang" role="doc-biblioref">Zhang and Sennrich 2019</a>)</span>: 只使用均方根（Root Mean Square）来进行归一化，省略了均值的计算，减少了计算开销。 RMSNorm 在某些情况下可以提供与 LayerNorm 相似的性能，但计算更高效。</li>
</ul>
<p>原始的Transformer中使用的是LayerNorm， 但是现代大语言模型中，RMSNorm被广泛采用， 例如在 LLaMA、GPT-4、PaLM 等模型中都使用了 RMSNorm。</p>
<p>RMSNorm的存在的优势是，</p>
<ul>
<li>计算效率更高： 由于 RMSNorm 省略了均值的计算，因此在计算上更加高效，特别是在大规模模型中，这种效率提升尤为显著。</li>
<li>性能相似： 实验表明，在许多任务中，RMSNorm 可以提供与 LayerNorm 相似的性能，尤其是在大语言模型中。</li>
</ul>
</section>
</section>
<section id="activations" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="activations"><span class="header-section-number">2.2</span> Activations</h2>
<p>Activation Functions给模型引入了非线性，使得神经网络能够学习复杂的函数映射关系。 常见的激活函数包括ReLU、Sigmoid、Tanh等。</p>
<p><img src="./assets/activations.png" class="img-fluid"></p>
<p>现代LLM中，最常见的激活函数是 Gated Activations家族，例如 SwiGLU，接下来，我们看看这些架构</p>
<section id="gated-activationsglu" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="gated-activationsglu"><span class="header-section-number">2.2.1</span> Gated Activations(*GLU)</h3>
<p>Gated Activations 通过引入门控机制，允许模型在前馈神经网络中动态调整信息流，从而提高模型的表达能力和性能。在传统的FFN中，输入通过两个线性变换和一个非线性激活函数进行处理。而在 Gated Activations 中，输入被分成两部分，一部分通过激活函数处理，另一部分通过门控机制进行调节，最终两部分的输出进行元素级乘法操作。用数学公式表示为:</p>
<p><span class="math display">\[
\text{FF} = \textcolor{red}{\max (0, XW_1 )} \ctimes (XW_2 )
\]</span></p>
<p>Gated Activations 主要改变的就是红色部分，用数学表达就是： <span class="math display">\[
\text{Gated FF} = \textcolor{red}{(\max (0, XW_1 ) \odot XV )}W_2
\]</span></p>
<p>常见的 Gated Activations 包括:</p>
<ul>
<li>GeGLU:</li>
</ul>
<p><span class="math display">\[
\text{GeGLU: } \quad \text{Gated FF} = (\text{GELU}(XW_1 ) \odot XV )W_2
\]</span></p>
<ul>
<li>SwiGLU: <span class="math display">\[
\text{SwiGLU: } \quad \text{Gated FF} = (\text{SiLU}(XW_1 ) \odot XV )W_2
\]</span></li>
</ul>
<p>实验表明，Gated Activations 在许多任务中都优于传统的激活函数，特别是在大语言模型中，例如 LLaMA 和 GPT-4 等模型中都采用了 SwiGLU 作为前馈神经网络的激活函数。</p>
<p>不过需要主要的一点是，Gated Activation引入了一个额外的线性变换矩阵V，这会增加模型的参数量和计算开销。 因此，在实际应用中，为了保持模型数量的不变，通常我们将 <span class="math inline">\(d_ff\)</span> 设定为 <span class="math inline">\(\frac{8}{3} d_{model}\)</span>， 这样就可以在引入Gated Activation的同时，保持模型的参数量不变。</p>
</section>
</section>
<section id="serial-parallel-mlp" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="serial-parallel-mlp"><span class="header-section-number">2.3</span> Serial &amp; Parallel MLP</h2>
<p>传统的Transformer Block是串行的结构 <a href="#fig-lecture03-overview" class="quarto-xref">Figure&nbsp;1</a>， 即先经过Attention模块， 然后再经过MLP模块。 这种设计虽然简单，但在某些情况下可能会限制模型的表达能力。有研究提出了并行的MLP设计，将Attention和MLP模块并行处理，然后将它们的输出进行融合。这种设计可以提高模型的表达能力和计算效率。 用数学公式表示为:</p>
<p><span class="math display">\[
y = x + \text{MLP}(Norm(x)) + \text{Attention}(Norm(x))
\]</span></p>
<p>通过并行这两层，可以让模型训练的更快速，同时可以减少模型的参数，比如Norm层只需要一层。</p>
<p>不过，目前主流的大语言模型仍然采用串行的Transformer Block设计，但并行的MLP设计为未来的模型架构提供了一个有趣的方向。</p>
</section>
<section id="position-encoding" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="position-encoding"><span class="header-section-number">2.4</span> Position Encoding</h2>
<p>位置编码 (Positional Encoding) 用于引入序列中单词的位置信息，因为自注意力机制本身不具备顺序信息。 传统的Transformer使用的是绝对位置编码，例如正弦和余弦函数编码<span class="citation" data-cites="AttentionAllYou2023vaswani">(<a href="#ref-AttentionAllYou2023vaswani" role="doc-biblioref">Vaswani et al. 2023</a>)</span>。常见的位置编码方法包括:</p>
<ul>
<li>Absolute Position Encoding: 使用固定的编码方式为每个位置分配一个唯一的向量表示。</li>
<li>Relative Position Encoding: 通过计算单词之间的相对位置来引入位置信息，增强模型对序列中单词相对关系的理解能力。</li>
<li>Rotary Position Embedding (RoPE)<span class="citation" data-cites="RoFormerEnhancedTransformer2023su">(<a href="#ref-RoFormerEnhancedTransformer2023su" role="doc-biblioref">Su et al. 2023</a>)</span>: 通过旋转位置向量来引入位置信息，增强模型对长距离依赖关系的捕捉能力。</li>
</ul>
<p>接下来我们重点介绍 RoPE。</p>
<section id="rotary-position-embedding-rope" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="rotary-position-embedding-rope"><span class="header-section-number">2.4.1</span> Rotary Position Embedding (RoPE)</h3>
<div id="fig-" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig--caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/rope.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig--caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
</figure>
</div>
<p>RoPE 通过对查询和键的向量进行旋转来引入位置信息。 具体来说，RoPE 将位置编码表示为一个旋转矩阵，然后将查询和键的向量与该旋转矩阵相乘，从而引入位置信息。 用数学公式表示为:</p>
<p><span class="math display">\[
\text{RoPE}(Q, K, P) = (Q R(P), K R(P))
\]</span> 其中，<span class="math inline">\(R(P)\)</span> 是位置编码对应的旋转矩阵。</p>
<p>用数学表示就是： <span class="math display">\[
R_{\Theta,m}^{d} \mathbf{x}
=
\begin{pmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_{d-1}\\
x_d
\end{pmatrix}
\otimes
\begin{pmatrix}
\cos(m\theta_{1})\\
\cos(m\theta_{1})\\
\cos(m\theta_{2})\\
\cos(m\theta_{2})\\
\vdots\\
\cos\!\big(m\theta_{d/2}\big)\\
\cos\!\big(m\theta_{d/2}\big)
\end{pmatrix}
+
\begin{pmatrix}
- x_2\\
x_1\\
- x_4\\
x_3\\
\vdots\\
- x_d\\
x_{d-1}
\end{pmatrix}
\otimes
\begin{pmatrix}
\sin(m\theta_{1})\\
\sin(m\theta_{1})\\
\sin(m\theta_{2})\\
\sin(m\theta_{2})\\
\vdots\\
\sin\!\big(m\theta_{d/2}\big)\\
\sin\!\big(m\theta_{d/2}\big)
\end{pmatrix}
\]</span></p>
<p>我非常推荐以下的视频，它很清楚的介绍了RoPE以及它的扩展，有兴趣的同学可以前去查看： <iframe width="100%" height="600" src="https://www.youtube.com/embed/SMBkImDWOyQ?si=t1pYhOoRD5J0RZOW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
</section>
</section>
</section>
<section id="hyper-parameters" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Hyper-Parameters</h1>
<p>训练神经网络就像是炼丹，超参数的选择对于模型的性能有着至关重要的影响。 以下是一些常见的超参数及其选择原则:</p>
<section id="mlp-width" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="mlp-width"><span class="header-section-number">3.1</span> MLP Width</h2>
<p>MLP的宽度通常设置为模型维度的4倍，例如对于一个512维的模型，MLP的宽度通常设置为2048。 这种设计可以提供足够的表达能力，同时不会过度增加计算开销。 当然，我们在之前提到过， 如果使用Gated Activation, 那么MLP的宽度通常设置为 <span class="math inline">\(\frac{8}{3}\)</span> 倍的模型维度。基本上目前主流的大语言模型都是采用这个比例。</p>
<p>除了Gated Activation之外， 也有一些模型使用更宽的MLP， 例如 T5 使用了 64 倍的MLP宽度， 但是这种设计会显著增加计算开销， 因此需要权衡模型性能与计算资源。</p>
<p>至于为什么选择4倍或者 <span class="math inline">\(\frac{8}{3}\)</span> 倍的MLP宽度， 主要是基于经验和实验结果。 研究表明，这些比例可以在保持模型性能的同时，提供足够的表达能力。</p>
<p><img src="./assets/hyper-ffn.png" class="img-fluid"></p>
</section>
<section id="attention-heads" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="attention-heads"><span class="header-section-number">3.2</span> Attention Heads</h2>
<p>模型的Head Dim 通常设置为 <code>d_model / num_heads</code>， 也就是说，Head Dim 与模型维度成反比。 常见的Head Dim设置包括64、128等。不过，Head Dim 不一定就是<code>d_model / num_heads</code>。 不过并没有实验表明，Head Dim 过大或者过小会显著影响模型性能。 因此，在实际应用中，通常根据计算资源和模型规模来选择Head Dim。</p>
</section>
<section id="aspect-ratio" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="aspect-ratio"><span class="header-section-number">3.3</span> Aspect Ratio</h2>
<p>Aspect Ratio 指的是模型的深度（n_layer）与宽度（d_model）之比。 研究表明，较高的Aspect Ratio（即更深的模型）通常可以提供更好的性能，特别是在处理复杂任务时。 然而，过深的模型也可能导致训练困难和过拟合问题。</p>
<p><img src="./assets/aspect-ratio.png" class="img-fluid"></p>
<p>不过过深的模型也会带来一些挑战，例如Parallelism和训练稳定性问题。 因此，在选择Aspect Ratio时，需要权衡模型性能与训练稳定性。</p>
</section>
<section id="vocabulary-size" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="vocabulary-size"><span class="header-section-number">3.4</span> Vocabulary Size</h2>
<p>词表大小（Vocabulary Size）是指模型在训练和推理过程中使用的唯一单词或子词的数量。 词表大小的选择对于模型的性能和计算效率有着重要影响。对于单一个语言的模型，常见的词表大小范围在30,000到100,000之间。 对于多语言模型，词表大小通常更大，以覆盖更多的语言和词汇。</p>
<p><img src="./assets/vocab_size.png" class="img-fluid"></p>
</section>
<section id="regularization" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="regularization"><span class="header-section-number">3.5</span> Regularization</h2>
<p>正则化技术用于防止模型过拟合，提高模型的泛化能力。但是许多人提出了一个问题， 大模型是否还需要正则化？ 当模型足够大时， 它们似乎并不容易过拟合， 因此正则化的必要性受到质疑。</p>
</section>
</section>
<section id="stability-tricks" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Stability Tricks</h1>
<p>在训练大语言模型时，稳定性是一个重要的考虑因素。</p>
<p><img src="./assets/stable_trick.png" class="img-fluid"></p>
<p>我们希望Loss 曲线尽可能的平滑， 类似于橙色曲线，而不希望出现剧烈的波动， 类似于蓝色曲线。 为了实现这一目标， 研究人员提出了一些稳定性技巧。不过我们先来看看为什么会出现不稳定的情况。</p>
<p>其中一个主要原因是由于注意力机制中的点积操作。 在计算注意力权重时，查询和键的点积可能会导致数值过大或过小，并且将这些值传递给Softmax函数时，可能导致值的极端分布，从而引发训练不稳定。</p>
<p>在这种情况下，很自然的想到对点积结果进行归一化处理，从而缓解数值不稳定的问题。 下面我们介绍几种常见的稳定性技巧。</p>
<ul>
<li>QK-Norm： 对查询和键进行归一化处理，确保它们的范数为1，从而稳定点积结果。 用数学公式表示为:</li>
<li>Logit-soft-capping: 对注意力权重的对数进行截断，防止极端值影响Softmax计算。</li>
<li>z-loss: 在计算交叉熵损失时，添加一个正则化项，鼓励模型输出的logits保持在一个合理的范围内，从而提高训练稳定性。</li>
</ul>
</section>
<section id="attentions" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Attentions</h1>
<p>Attention 机制是 Transformer 的核心组件之一。 在大语言模型中，注意力机制的设计对于模型的性能和计算效率有着重要影响。 下面介绍几种常见的注意力机制设计:</p>
<section id="grouped-query-attention-gqa-multi-query-attention-mqa" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="grouped-query-attention-gqa-multi-query-attention-mqa"><span class="header-section-number">5.1</span> Grouped Query Attention (GQA) / Multi-Query Attention (MQA)</h2>
<p><img src="./assets/gqa.png" class="img-fluid"></p>
</section>
<section id="sparse-sliding-window-attention" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sparse-sliding-window-attention"><span class="header-section-number">5.2</span> Sparse / Sliding Window Attention</h2>
<p><img src="./assets/sparse-slding.png" class="img-fluid"></p>
<p><span class="citation" data-cites="GeneratingLongSequences2019child">Child et al. (<a href="#ref-GeneratingLongSequences2019child" role="doc-biblioref">2019</a>)</span></p>
</section>
<section id="sliding-window-attention" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sliding-window-attention"><span class="header-section-number">5.3</span> Sliding Window Attention</h2>
</section>
<section id="interleaved-attention" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="interleaved-attention"><span class="header-section-number">5.4</span> Interleaved Attention</h2>
<p><img src="./assets/interleave.png" class="img-fluid"></p>
</section>
</section>
<section id="summary" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Summary</h1>
<p>这节课的信息量还是比较大的，建议大家多看几遍，主要的就是全面的介绍了现代大语言模型的架构设计与超参数选择原则。 理解这些内容对于后续学习大模型的训练与优化非常重要。 这节课就是简单的介绍了现代LLM的基础选择，由于时间原因， 许多内容都没有深入展开， 比如稳定性技巧， 以及各种Attention的设计。 后续我会专门写一些文章来介绍这些内容， 大家可以持续关注我的博客。</p>
<p>完成了这三节Lecture之后，我们可以实现 Assignment 1了， 大家可以参考我的代码实现: <a href="">CS336-Assignment1</a></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-LayerNormalization2016ba" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>“Layer <span>Normalization</span>.”</span> July 21, 2016. <a href="https://doi.org/10.48550/arXiv.1607.06450">https://doi.org/10.48550/arXiv.1607.06450</a>.
</div>
<div id="ref-GeneratingLongSequences2019child" class="csl-entry" role="listitem">
Child, Rewon, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. <span>“Generating <span>Long Sequences</span> with <span>Sparse Transformers</span>.”</span> April 23, 2019. <a href="https://doi.org/10.48550/arXiv.1904.10509">https://doi.org/10.48550/arXiv.1904.10509</a>.
</div>
<div id="ref-TransformersTearsImproving2019nguyen" class="csl-entry" role="listitem">
Nguyen, Toan Q., and Julian Salazar. 2019. <span>“Transformers Without <span>Tears</span>: <span>Improving</span> the <span>Normalization</span> of <span>Self-Attention</span>.”</span> <a href="https://doi.org/10.5281/zenodo.3525484">https://doi.org/10.5281/zenodo.3525484</a>.
</div>
<div id="ref-RoFormerEnhancedTransformer2023su" class="csl-entry" role="listitem">
Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. <span>“<span>RoFormer</span>: <span>Enhanced Transformer</span> with <span>Rotary Position Embedding</span>.”</span> November 8, 2023. <a href="https://doi.org/10.48550/arXiv.2104.09864">https://doi.org/10.48550/arXiv.2104.09864</a>.
</div>
<div id="ref-AttentionAllYou2023vaswani" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> August 2, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-RootMeanSquare2019zhang" class="csl-entry" role="listitem">
Zhang, Biao, and Rico Sennrich. 2019. <span>“Root <span>Mean Square Layer Normalization</span>.”</span> October 16, 2019. <a href="https://doi.org/10.48550/arXiv.1910.07467">https://doi.org/10.48550/arXiv.1910.07467</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/sta210-s22\.github\.io\/website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/CS336/Lecture02/lec02.html" class="pagination-link" aria-label="Lecture 02: PyTorch Basics &amp; Resource Accounts">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/CS336/Lecture04/lec04.html" class="pagination-link" aria-label="Lecture 04: MoE Architecture">
        <span class="nav-page-text">Lecture 04: MoE Architecture</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>