<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="这节介绍了PPO， 以及DPO所存在的问题，并且引出了Reinforcement Learning from Verified Rewards (RLVR)的新方法GRPO。内容包括Advantage Function、Length Normalization关键技术细节，并通过DeepSeek-R1、Kimi 1.5和Qwen3等案例展示了GRPO在实际中的应用。而Lecture17则通过一个排序任务的具体例子，详细讲解了GRPO算法的实现细节，包括数据生成、奖励函数设计、模型架构以及训练过程，在这里就把两节课合成一节课来写。">

<title>Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR（GRPO） – Learning Note</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" rel="next">
<link href="../../../posts/CS336/Lecture15/lec15.html" rel="prev">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-c0eab5a31fbea23c8affb95fb4fbb9c0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c61ca1b0a9f27cb683675ea1929ac2e3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<script>
    MathJax = {
        loader: {
        load: ['[tex]/boldsymbol']
        },
        tex: {
        tags: "all",
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        packages: {
            '[+]': ['boldsymbol']
        }
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script>
document.addEventListener("DOMContentLoaded", function () {
  document.querySelectorAll(".foldable-header").forEach(header => {
    header.addEventListener("click", () => {
      const block = header.closest(".foldable");
      if (block) {
        block.classList.toggle("is-open");
      }
    });

    // 可访问性（键盘）
    header.setAttribute("tabindex", "0");
    header.addEventListener("keydown", e => {
      if (e.key === "Enter" || e.key === " ") {
        e.preventDefault();
        header.click();
      }
    });
  });
});
</script>
    <style type="text/css">
    .ps-root .ps-algorithm {
      border-top: 2px solid;
      border-bottom: 2px solid;
    }
    .pseudocode-container {
      text-align: left;
    }
    </style>
  
      <style type="text/css">
      .ps-algorithm > .ps-line {
        text-align: left;
      }
      </style>
    

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/CS336/index.html">Stanford CS336: LLM from Scratch</a></li><li class="breadcrumb-item"><a href="../../../posts/CS336/Lecture16&amp;17/lec16.html">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../../index.html" class="sidebar-logo-link">
      <img src="../../../logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/sta210-s22" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Stanford CS336: LLM from Scratch</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture01/lec01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 01: Introduction &amp; BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture02/lec02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 02: PyTorch Basics &amp; Resource Accounts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture03/lec03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 03: Transformer LM Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture04/lec04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 04: MoE Architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture05&amp;06/lec05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 05&amp;06: GPU Optimization, Triton &amp; FlashAttention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture07&amp;08/lec07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 07&amp;08: Parallelism</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture9&amp;11/lec9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 09&amp;11: Scaling Laws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture10/lec10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 10: Inference &amp; Deployment</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture12/lec12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 12: Evaluation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture13&amp;14/lec13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 13&amp;14: Data Collection &amp; Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture15/lec15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/CS336/Lecture16&amp;17/lec16.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">100 AI Papers</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision Transformer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-not-ppo-dpo" id="toc-why-not-ppo-dpo" class="nav-link active" data-scroll-target="#why-not-ppo-dpo"><span class="header-section-number">1</span> Why NOT PPO &amp; DPO ?</a>
  <ul>
  <li><a href="#why-not-ppo" id="toc-why-not-ppo" class="nav-link" data-scroll-target="#why-not-ppo"><span class="header-section-number">1.1</span> Why Not PPO?</a></li>
  <li><a href="#why-not-dpo" id="toc-why-not-dpo" class="nav-link" data-scroll-target="#why-not-dpo"><span class="header-section-number">1.2</span> Why Not DPO?</a></li>
  </ul></li>
  <li><a href="#what-is-rlvr" id="toc-what-is-rlvr" class="nav-link" data-scroll-target="#what-is-rlvr"><span class="header-section-number">2</span> What is RLVR?</a></li>
  <li><a href="#grpo" id="toc-grpo" class="nav-link" data-scroll-target="#grpo"><span class="header-section-number">3</span> GRPO</a>
  <ul>
  <li><a href="#grpo-objective" id="toc-grpo-objective" class="nav-link" data-scroll-target="#grpo-objective"><span class="header-section-number">3.1</span> GRPO Objective</a></li>
  <li><a href="#advantage-calculation-in-grpo" id="toc-advantage-calculation-in-grpo" class="nav-link" data-scroll-target="#advantage-calculation-in-grpo"><span class="header-section-number">3.2</span> Advantage Calculation in GRPO</a></li>
  <li><a href="#grpo-algorithm" id="toc-grpo-algorithm" class="nav-link" data-scroll-target="#grpo-algorithm"><span class="header-section-number">3.3</span> GRPO Algorithm</a></li>
  <li><a href="#bias-in-grpo" id="toc-bias-in-grpo" class="nav-link" data-scroll-target="#bias-in-grpo"><span class="header-section-number">3.4</span> Bias in GRPO</a>
  <ul>
  <li><a href="#biased-gradient" id="toc-biased-gradient" class="nav-link" data-scroll-target="#biased-gradient"><span class="header-section-number">3.4.1</span> Biased Gradient</a></li>
  <li><a href="#length-normalization-bias" id="toc-length-normalization-bias" class="nav-link" data-scroll-target="#length-normalization-bias"><span class="header-section-number">3.4.2</span> Length Normalization Bias</a></li>
  </ul></li>
  <li><a href="#dr.grpo" id="toc-dr.grpo" class="nav-link" data-scroll-target="#dr.grpo"><span class="header-section-number">3.5</span> Dr.GRPO</a></li>
  </ul></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies"><span class="header-section-number">4</span> Case Studies</a>
  <ul>
  <li><a href="#deepseek-r1" id="toc-deepseek-r1" class="nav-link" data-scroll-target="#deepseek-r1"><span class="header-section-number">4.1</span> DeepSeek-R1</a>
  <ul>
  <li><a href="#from-deepseek-v3-to-deepseek-r1-zero" id="toc-from-deepseek-v3-to-deepseek-r1-zero" class="nav-link" data-scroll-target="#from-deepseek-v3-to-deepseek-r1-zero"><span class="header-section-number">4.1.1</span> From DeepSeek-V3 to DeepSeek-R1-Zero</a></li>
  <li><a href="#from-deepseek-r1-zero-to-deepseek-r1" id="toc-from-deepseek-r1-zero-to-deepseek-r1" class="nav-link" data-scroll-target="#from-deepseek-r1-zero-to-deepseek-r1"><span class="header-section-number">4.1.2</span> From DeepSeek-R1-Zero to DeepSeek-R1</a></li>
  <li><a href="#deepseek-distillation" id="toc-deepseek-distillation" class="nav-link" data-scroll-target="#deepseek-distillation"><span class="header-section-number">4.1.3</span> DeepSeek Distillation</a></li>
  </ul></li>
  <li><a href="#kimi-1.5" id="toc-kimi-1.5" class="nav-link" data-scroll-target="#kimi-1.5"><span class="header-section-number">4.2</span> Kimi 1.5</a></li>
  <li><a href="#qwen3" id="toc-qwen3" class="nav-link" data-scroll-target="#qwen3"><span class="header-section-number">4.3</span> Qwen3</a></li>
  </ul></li>
  <li><a href="#grpo-implementation" id="toc-grpo-implementation" class="nav-link" data-scroll-target="#grpo-implementation"><span class="header-section-number">5</span> GRPO Implementation</a>
  <ul>
  <li><a href="#data-goal" id="toc-data-goal" class="nav-link" data-scroll-target="#data-goal"><span class="header-section-number">5.1</span> Data &amp; Goal</a></li>
  <li><a href="#reward" id="toc-reward" class="nav-link" data-scroll-target="#reward"><span class="header-section-number">5.2</span> Reward</a>
  <ul>
  <li><a href="#reward-v2inclusion-adjacent-sorted-pairs课堂采用的更细奖励" id="toc-reward-v2inclusion-adjacent-sorted-pairs课堂采用的更细奖励" class="nav-link" data-scroll-target="#reward-v2inclusion-adjacent-sorted-pairs课堂采用的更细奖励"><span class="header-section-number">5.2.1</span> Reward v2：Inclusion + Adjacent Sorted Pairs（课堂采用的更细奖励）</a></li>
  </ul></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model"><span class="header-section-number">5.3</span> Model</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm"><span class="header-section-number">5.4</span> Algorithm</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">5.5</span> Training</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5.6</span> Results</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6</span> Summary</a></li>
  <li><a href="#in-the-end" id="toc-in-the-end" class="nav-link" data-scroll-target="#in-the-end"><span class="header-section-number">7</span> In the End</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../posts/CS336/index.html">Stanford CS336: LLM from Scratch</a></li><li class="breadcrumb-item"><a href="../../../posts/CS336/Lecture16&amp;17/lec16.html">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR(GRPO)</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Lecture 16 &amp; 17: LLM Alignment SFT &amp; RLVR（GRPO）</h1>
</div>

<div>
  <div class="description">
    这节介绍了PPO， 以及DPO所存在的问题，并且引出了Reinforcement Learning from Verified Rewards (RLVR)的新方法<strong>GRPO</strong>。内容包括Advantage Function、Length Normalization关键技术细节，并通过DeepSeek-R1、Kimi 1.5和Qwen3等案例展示了GRPO在实际中的应用。而Lecture17则通过一个排序任务的具体例子，详细讲解了GRPO算法的实现细节，包括数据生成、奖励函数设计、模型架构以及训练过程，在这里就把两节课合成一节课来写。
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>在之前的课程中，我们学习了PPO以及DPO的算法，我们因此也从GPT得到了ChatGPT。但这距离现在的 <a href="https://en.wikipedia.org/wiki/Reasoning_model">Reasoning Model</a>，还有一段距离。在这节课中，我们将学习新的一系列算法，也就是<strong>RLVR(Reinforcement Learning from Verified Rewards)</strong>，通过这些算法，我们可以从ChatGPT突破到<a href="https://en.wikipedia.org/wiki/OpenAI_o1">ChatGPT-o1</a>。</p>
<p>在了解RLVR之前，我们先来看一下，<span class="hilite-orange">为什么不直接用PPO和DPO来做LLM的推理强化学习。</span></p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>推荐阅读</p>
</div>
<div class="note-container foldable-content">
<p>对于想更深入了解 <span class="hilite-teal">RL in LLM</span> 的同学们，我推荐阅读我这一篇文章 <a href="https://yyzhang2025.github.io/posts/Blogs/LLM-RL/post.html">Reinforcement Learning in LLM</a>。在这篇文章中，我们系统地介绍了 LLM 中的强化学习，包括 PPO、DPO 以及 RLVR 的原理和实现细节。并且对比了不同的RLVR的算法，比如GRPO，GSPO，DAPO等。</p>
</div>
</div>
<section id="why-not-ppo-dpo" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Why NOT PPO &amp; DPO ?</h1>
<section id="why-not-ppo" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="why-not-ppo"><span class="header-section-number">1.1</span> Why Not PPO?</h2>
<p>PPO理论上就一个clip目标，但落地要处理很多组件, 包括：</p>
<ul>
<li>rollout采样/缓存、old logprob、ratio、clip、KL penalty/target KL</li>
<li>advantage估计（GAE）、return计算</li>
</ul>
<p>对LLM来说，<strong>rollout很贵</strong>（推理生成token），所以还会引入“采一次rollout，更新多次”的机制，这又把实现复杂度继续抬高。并且，最主要的问题是：</p>
<div class="highlight">
<p>PPO通常需要一个 <strong>value function</strong> 来计算Advantage.</p>
</div>
<p>这意味着：</p>
<ul>
<li>额外的模型（value model）+内存开销</li>
<li>额外的训练目标（value loss）</li>
</ul>
<blockquote class="blockquote">
<p>“Value model (memory hungry, involves additional tuning)”</p>
</blockquote>
</section>
<section id="why-not-dpo" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="why-not-dpo"><span class="header-section-number">1.2</span> Why Not DPO?</h2>
<p>DPO的强项是<strong>偏好对比数据</strong> <span class="math inline">\((p, y_i, y_j)\)</span>，但在<strong>可验证奖励</strong>的推理RL里，数据常见是：</p>
<ul>
<li>一个prompt，</li>
<li>一个回答，</li>
<li>reward=0/1（对/错）或分数</li>
</ul>
<p>数据不是天然的<strong>pairwise偏好</strong>，而是<strong>pointwise reward</strong>。另外，DPO常见使用方式偏<strong>Off-Policy</strong>：先收集对比数据，再训练。而推理RL通常更像<strong>在线</strong>：模型不断变强 → 不断rollout → 不断用最新策略生成新数据训练（on-policy味道更强）。GRPO/PPO天然适配这种循环。</p>
</section>
</section>
<section id="what-is-rlvr" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> What is RLVR?</h1>
<p>RLVR (Reinforcement Learning from Verified Rewards) 是一类专门为 LLM 推理强化学习设计的算法。它们利用<strong>可验证奖励</strong>（例如数学题的对错）来优化模型的推理能力。 它的流程大致如下 <a href="#fig-rlvf" class="quarto-xref">Figure&nbsp;1 (a)</a>：</p>
<div id="fig-rlhf-vs-rlvr" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rlhf-vs-rlvr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rlhf-vs-rlvr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-rlvf" class="quarto-float quarto-figure quarto-figure-center anchored" style="object-fit:contain;">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-rlvf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/rlvf-overview.png" style="object-fit:contain;;width:100.0%" data-ref-parent="fig-rlhf-vs-rlvr" height="200" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-rlvf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Overview of RLVR
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rlhf-vs-rlvr" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-rlhf" class="quarto-float quarto-figure quarto-figure-center anchored" style="object-fit:contain;">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-rlhf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/rlhf-overview.png" style="object-fit:contain;;width:100.0%" data-ref-parent="fig-rlhf-vs-rlvr" height="200" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-rlhf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Overview of RLHF
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rlhf-vs-rlvr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: RLVR vs RLHF， RLHF 依赖人工偏好数据，而 RLVR 则利用可验证奖励进行训练。 RLVR 利用 <strong>Verified Rewards</strong> 直接优化模型的推理能力，而不需要额外的奖励模型。
</figcaption>
</figure>
</div>
<p>相比于RLHF，RLVR有以下优势：</p>
<ul>
<li><strong>直接利用可验证奖励</strong>：RLVR直接使用可验证的奖励信号（如数学题的对错），而不需要训练复杂的奖励模型。这简化了训练流程，减少了对高质量偏好数据的依赖。</li>
<li><strong>对模型输出的严格约束</strong>：RLVR方法通常设计了专门的目标函数，确保模型在生成推理链时遵循逻辑和事实，从而提高了模型的可靠性和准确性。</li>
<li><strong>更高的样本效率</strong>：通过利用可验证奖励，RLVR方法能够更有效地利用训练数据，提高模型的学习效率，减少所需的训练样本数量。</li>
</ul>
</section>
<section id="grpo" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> GRPO</h1>
<p>回顾了PPO和DPO在LLM推理RL上的局限后，我们来看GRPO<span class="citation" data-cites="DeepSeekMathPushingLimits2024shao">(<a href="#ref-DeepSeekMathPushingLimits2024shao" role="doc-biblioref">Shao et al. 2024</a>)</span>。GRPO与PPO的主要流程与区别如下：</p>
<div id="fig-grpo-ppo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grpo-ppo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="assets/lec16-grpo-ppo.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grpo-ppo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: 对比了 PPO 与 GRPO 的训练流程差异： <br>上半部分（PPO）中，策略模型在生成单个输出 <span class="math inline">\(o\)</span> 后，分别通过<em>Reward Model</em>与<em>Reference Model</em>得到Reward <span class="math inline">\(r\)</span> 与 KL 约束，并依赖<em>Value Model</em>预测的状态价值 <span class="math inline">\(v\)</span>，再用 GAE 计算优势 A 来更新策略； <br>下半部分（GRPO）中，策略模型对同一提示 <span class="math inline">\(q\)</span> 一次性采样一组输出 <span class="math inline">\(\{o_1,\dots,o_G\}\)</span>，仅依赖冻结的Reward Model与Reference Model得到组内奖励 <span class="math inline">\(\{r_1, r_2, \dots, r_G\}\)</span>，通过组内相对计算（Group Computation）直接构造优势 <span class="math inline">\(\{A_1, A_2, \dots, A_G\}\)</span>，从而不再需要价值模型与 GAE，以更简化的结构完成带 KL 约束的策略更新。
</figcaption>
</figure>
</div>
<p>从 <a href="#fig-grpo-ppo" class="quarto-xref">Figure&nbsp;2</a> 可以看出，GRPO与PPO类似，主要的区别在于<strong>Advantage Estimator</strong>的设计。 不过，在了解Advantage Estimator之前，我们先来看一下GRPO的目标函数.</p>
<section id="grpo-objective" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="grpo-objective"><span class="header-section-number">3.1</span> GRPO Objective</h2>
<p>GRPO 的目标函数如下： <span id="eq-grpo"><span class="math display">\[
\mathcal{J}_{\text{GRPO}}(\pi_\theta) = \frac{1}{G}\sum_{i=1}^{G}
\textcolor{red}{\frac{1}{|o_i|}} \sum_{t = 1}^{|o_i|}
\Bigg\{
\min \Big[
     \rho(o_{i,t}) \hat{A}_{i, t},
     \text{clip}\Big(
            \rho(o_{i,t}), 1 - \epsilon_{\text{clip}}, 1 + \epsilon_{\text{clip}}
        \Big) \hat{A}_{i, t}
    \Big]    
\Bigg\} + \textcolor{gray}{\beta \, \text{KL}\big(\pi_\theta || \pi_{\text{ref}}\big)}
\tag{1}\]</span></span></p>
<p>其中：</p>
<p><span id="eq-importance-ratio"><span class="math display">\[
\rho(o_{i,t}) = \frac{\pi_\theta(o_{i,t}|q, o_{i,&lt;t})}{\pi_{\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}
\tag{2}\]</span></span></p>
<p>是<strong>importance sampling ratio</strong>，用来把当前策略和reference policy联系起来。</p>
<p><span class="math inline">\(\textcolor{gray}{\beta \, \text{KL}\big(\pi_\theta || \pi_{\text{ref}}\big)}\)</span> 是 KL penalty，用来<u>防止当前策略偏离参考策略太远</u>。(不过在实际的实现中，我们通常会忽略这个KL penality，因为GRPO的采样本身就是从参考策略采样的，所以偏离不会太大)</p>
<p>对每个 prompt 采样一组（group）回答,</p>
<ol type="1">
<li>用<strong>组内相对奖励</strong>当 advantage（基线），</li>
<li>再做 policy-gradient 更新，同时用 KL 约束不要偏离参考策略( 不过在实际实现中，KL penalty是加在loss里，而不是硬约束 )。</li>
</ol>
<p>我们来看一下Python代码实现：</p>
<div class="sourceCode" id="cb1" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> compute_loss_grpo(</span>
<span id="cb1-2"><a href="#cb1-2"></a>    log_probs: torch.Tensor,      <span class="co"># (B, G, T) log probs under current policy</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    old_log_probs: torch.Tensor,  <span class="co"># (B, G, T) log probs under old policy (detached)</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    response_mask: torch.Tensor,  <span class="co"># (B, G, T) mask for valid response tokens</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>    advantage: torch.Tensor,    <span class="co"># (B, G) advantage per response</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>    clip_range: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span id="cb1-7"><a href="#cb1-7"></a>):</span>
<span id="cb1-8"><a href="#cb1-8"></a>    B, G, T <span class="op">=</span> log_probs.size()</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>    important_ratio <span class="op">=</span> torch.exp(log_probs <span class="op">-</span> old_log_probs)  <span class="co"># (B, G, T)</span></span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co"># Broadcast advantage to token level</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    advantage_tok <span class="op">=</span> advantage.unsqueeze(<span class="op">-</span><span class="dv">1</span>).expand_as(important_ratio)  <span class="co"># (B, G, T)</span></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a>    unclipped <span class="op">=</span> important_ratio <span class="op">*</span> advantage_tok</span>
<span id="cb1-16"><a href="#cb1-16"></a>    clipped <span class="op">=</span> torch.clamp(important_ratio, <span class="dv">1</span> <span class="op">-</span> clip_range, <span class="dv">1</span> <span class="op">+</span> clip_range) <span class="op">*</span> advantage_tok</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a>    pg_loss_tok <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">min</span>(unclipped, clipped)  <span class="co"># (B, G, T)</span></span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a>    <span class="co"># Length normalization (GRPO does this)</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>    len_per_response <span class="op">=</span> response_mask.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, G)</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>    pg_loss_tok <span class="op">=</span> pg_loss_tok <span class="op">*</span> response_mask  <span class="co"># mask out non-response tokens</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>    pg_loss_seq <span class="op">=</span> pg_loss_tok.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">/</span> len_per_response  <span class="co"># (B, G)</span></span>
<span id="cb1-24"><a href="#cb1-24"></a></span>
<span id="cb1-25"><a href="#cb1-25"></a>    <span class="co"># Final loss: mean over batch and group</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>    loss <span class="op">=</span> pg_loss_seq.<span class="bu">sum</span>()</span>
<span id="cb1-27"><a href="#cb1-27"></a>    loss <span class="op">=</span> loss <span class="op">/</span> (B <span class="op">*</span> G)</span>
<span id="cb1-28"><a href="#cb1-28"></a>    <span class="co"># loss = pg_loss_seq.mean()  # alternative: mean over all tokens</span></span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>具体的实现中，我们会把<strong>按回答长度归一化</strong>（<code>pg_loss_seq = pg_loss_tok.sum(dim=-1) / len_per_response</code>），这是GRPO的一个关键设计，我们后面会重点讲。</p>
</section>
<section id="advantage-calculation-in-grpo" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="advantage-calculation-in-grpo"><span class="header-section-number">3.2</span> Advantage Calculation in GRPO</h2>
<p>接下来我们重点看看 GRPO 的 Advantage 计算：</p>
<ul>
<li>对每个 prompt（比如一道数学题）：<br>
</li>
<li>采样 <strong>G 个回答</strong>：<span class="math inline">\(\{y_1, y_2, \dots, y_G\}\)</span></li>
<li>这个 prompt 就是一组（group），组内回答共享同一难度, 并且有了 <span class="math inline">\(\{(p, y_1, r_1), (p, y_2, r_2), \dots, (p, y_G, r_G)\}\)</span>，其中 <span class="math inline">\(r_i\)</span> 是回答 <span class="math inline">\(y_i\)</span> 的奖励。</li>
</ul>
<p>接下来，我们通过<strong>组内统计量</strong>，来计算 Advantage： <span id="eq-grpo-advantage"><span class="math display">\[
\hat{A}_{i, t}=\frac{r_i-\mu}{\sigma + \epsilon } \quad \text{where} \quad \mu=\frac{1}{G}\sum_{j=1}^{G} r_j, \quad \sigma=\sqrt{\frac{1}{G}\sum_{j=1}^{G}(r_j-\mu)^2}
\tag{3}\]</span></span></p>
<p>其中：</p>
<ul>
<li>用 <strong>组内均值 <span class="math inline">\(\mu\)</span></strong> 当 baseline（反映题目难度）</li>
<li>用 <strong>组内方差 <span class="math inline">\(\sigma\)</span></strong> 做归一化（控制更新尺度）</li>
<li><span class="math inline">\(\epsilon\)</span> 是一个小常数，防止除零</li>
</ul>
<p>其中，<span class="math inline">\(\hat{A}_{i, t}\)</span> 是回答 <span class="math inline">\(y_i\)</span> 在时间步 <span class="math inline">\(t\)</span> 的 advantage, 也就是每个action(token <span class="math inline">\(o_{i,t}\)</span>) 对应的 Advantage, 在GRPO中，<strong>同一个回答 <span class="math inline">\(y_i\)</span> 的所有token共享同一个 advantage值</strong>。</p>
<div class="sourceCode" id="cb2" data-code-line-numbers=""><pre class="sourceCode numberSource Python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> compute_advantage(</span>
<span id="cb2-2"><a href="#cb2-2"></a>    rewards: torch.Tensor,  <span class="co"># (B, G)</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    epsilon: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>):</span>
<span id="cb2-5"><a href="#cb2-5"></a>    mu <span class="op">=</span> rewards.mean(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>    sigma <span class="op">=</span> rewards.std(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a>    advantage <span class="op">=</span> (rewards <span class="op">-</span> mu) <span class="op">/</span> (sigma <span class="op">+</span> epsilon)  <span class="co"># (B, G)</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="cf">return</span> advantage</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="grpo-algorithm" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="grpo-algorithm"><span class="header-section-number">3.3</span> GRPO Algorithm</h2>
<p>了解了GRPO的Objective Function (<a href="#eq-grpo" class="quarto-xref">Equation&nbsp;1</a>) 和Advantage计算(<a href="#eq-grpo-advantage" class="quarto-xref">Equation&nbsp;3</a>) 后，我们来看一下GRPO的整体算法流程：</p>
<div id="algo-grpo" class="pseudocode-container quarto-float" data-indent-size="1.2em" data-pseudocode-number="1" data-comment-delimiter="#" data-line-number-punc=":" data-line-number="true" data-no-end="false" data-caption-prefix="Algorithm">
<div class="pseudocode">
\begin{algorithm} \caption{Iterative Group Relative Policy Optimization (GRPO)}\begin{algorithmic} \Require initial policy model $\pi_{\theta_{\text{init}}}$; reward model(s) $r_\phi$; task prompts $\mathcal{D}$; hyperparameters $\epsilon, \beta, \mu$; group size $G$; iterations $I$; steps $M$ \Ensure trained policy $\pi_\theta$ \State $\pi_\theta \gets \pi_{\theta_{\text{init}}}$ \For{$\text{iteration} = 1, \ldots, I$} \State $\pi_{\text{ref}} \gets \pi_\theta$ \Comment{reference (anchor) policy} \For{$\text{step} = 1, \ldots, M$} \State Sample a batch $\mathcal{D}_b$ from $\mathcal{D}$ \State $\pi_{\theta_{\text{old}}} \gets \pi_\theta$ \Comment{snapshot old policy} \ForAll{$q \in \mathcal{D}_b$} \State Sample $G$ outputs $\{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)$ \State Compute rewards $\{r_i\}_{i=1}^{G}$ by $r_i \gets r_\phi(q, o_i)$ \State Compute group-relative advantages $\{\hat{A}_{i,t}\}$ for each token $t$ in each $o_i$ \EndFor \For{$\text{GRPO-iter} = 1, \ldots, \mu$} \State Update $\pi_\theta$ by maximizing the GRPO objective \EndFor \State Update $r_\phi$ via continuous training using a replay mechanism \EndFor \EndFor \State \Return $\pi_\theta$ \end{algorithmic} \end{algorithm}
</div>
</div>
<p>对于每次迭代：</p>
<ul>
<li>先把当前策略 <span class="math inline">\(\pi_\theta\)</span> 作为参考策略 <span class="math inline">\(\pi_{\text{ref}}\)</span>（anchor policy）</li>
<li>然后，对于每个 prompt <span class="math inline">\(q\)</span>：
<ul>
<li>从旧策略 <span class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 采样 <span class="math inline">\(G\)</span> 条回答 <span class="math inline">\(\{o_i\}\)</span></li>
<li>计算每条回答的奖励 <span class="math inline">\(\{r_i\}\)</span>（通过奖励模型）</li>
<li>计算组内相对优势 <span class="math inline">\(\{\hat{A}_{i,t}\}\)</span></li>
<li>最后，用 GRPO 目标函数更新策略 <span class="math inline">\(\pi_\theta\)</span></li>
</ul></li>
</ul>
<p>这种设计避免了 PPO 里对 Value Function 的依赖，从而简化了训练流程， 同时我们看到，对于每个Rollout，我们只更新一次策略(<strong>On-Policy</strong>)，而不是多次，这也降低了实现复杂度。（不过这也是一个很大的Trade-Off，我们可以对于同一组数据多次更新策略（<strong>Off-Policy</strong>），从而提升样本效率，但会增加实现复杂度）</p>
</section>
<section id="bias-in-grpo" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bias-in-grpo"><span class="header-section-number">3.4</span> Bias in GRPO</h2>
<p>接下来，我们来自己分析一下GRPO的Objective Function (<a href="#eq-grpo" class="quarto-xref">Equation&nbsp;1</a>) ，可以发现，它其实存在两个Bias：</p>
<ul>
<li><strong>Biased Gradient</strong>：GRPO的梯度估计是有偏的</li>
<li><strong>Length Normalization Bias</strong>：按回答长度归一化引入的偏置</li>
</ul>
<p>我们来具体分析一下这两个Bias。</p>
<div id="fig-bias-grpo-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-grpo-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/bias-grpo-overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-grpo-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: 该图展示了 GRPO 的两类优化偏置：对同一问题 <span class="math inline">\(q\)</span> 采样多条回答 <span class="math inline">\(o_i\)</span> 后，GRPO 的“有效 advantage”满足 <span class="math inline">\(a_{i,t}=\frac{1}{\mathrm{std}(R)}\cdot \frac{\tilde A_{i,t}}{|o_i|}\)</span>，其中 <span class="math inline">\(\tilde A_{i,t}=R(q,o_i)-\mathrm{mean}(R)\)</span> 是组内中心化（相对）奖励；由于额外除以组内奖励标准差 <span class="math inline">\(\mathrm{std}(R)\)</span> 与回答长度 <span class="math inline">\(|o_i|\)</span>，训练会对<strong>不同问题与不同长度回答施加不同权重</strong>：<tag style="color:blue">蓝色圆点大小表示 <span class="math inline">\(1/\mathrm{std}(R)\)</span> 对“问题级”更新强度的缩放</tag>，<tag style="color:orange">橙色箭头长度表示 <span class="math inline">\(\tilde A_{i,t}/|o_i|\)</span> 对“回答级”更新幅度的缩放（箭头向上为正 advantage、向下为负），从而使优化偏向某些题目与某些长度的输出</tag>。
</figcaption>
</figure>
</div>
<section id="biased-gradient" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="biased-gradient"><span class="header-section-number">3.4.1</span> Biased Gradient</h3>
<p>第一个问题是 GRPO 的梯度估计是<strong>biased</strong>，这主要是因为它使用了<strong>依赖采样结果的 baseline</strong>。 我们知道，要保持一个<strong>un-biased</strong>梯度估计，baseline 只能是<strong>不依赖动作的常数</strong>，它可以是Value Function的估计，或者一个常数，但不能是<strong>依赖采样结果的量</strong>。</p>
<p>具体来说：</p>
<p>Policy Gradient Theorem 告诉我们，<strong>策略梯度</strong>是： <span id="eq-policy-gradient-theorem"><span class="math display">\[
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{ y \sim \pi_\theta(\cdot|x)} \Big[ \textcolor{green}{r(x, y)} \nabla_\theta \log \pi_\theta(y|x) \Big]
\tag{4}\]</span></span></p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE</p>
</div>
<div class="note-container foldable-content">
<p>在这里，<span class="math inline">\(x\)</span> 是环境状态（在LLM中是prompt），<span class="math inline">\(y\)</span> 是动作（在LLM中是token序列），<span class="math inline">\(r(x,y)\)</span> 是奖励函数，<span class="math inline">\(\pi_\theta(y|x)\)</span> 是策略（在LLM中是语言模型）。 为了简化起见，我们忽略了状态分布 <span class="math inline">\(D\)</span>，假设 <span class="math inline">\(x\)</span> 是固定的。实际中，我们会对 <span class="math inline">\(x\)</span> 也取期望： <span class="math display">\[
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\textcolor{red}{x \sim D}, y \sim \pi_\theta(\cdot|x)} \Big[ r(x, y) \nabla_\theta \log \pi_\theta(y|x) \Big]
\]</span></p>
</div>
</div>
<p>如果我们引入一个 baseline <span class="math inline">\(b(x)\)</span>，并且它<strong>不依赖动作</strong>，那么梯度变成： <span id="eq-policy-gradient-with-baseline"><span class="math display">\[
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{ y \sim \pi_\theta(\cdot|x)} \Big[  \textcolor{green}{\Big(r(x, y) - b(x)\Big)} \nabla_\theta \log \pi_\theta(y|x) \Big]
\tag{5}\]</span></span></p>
<p>只要 <span class="math inline">\(b(x)\)</span> 不依赖 <span class="math inline">\(y\)</span>（action, 在LLM中就是token），这个梯度估计就是un-biased。</p>
<p><span id="eq-baseline-zero-expectation"><span class="math display">\[
\begin{split}
\mathbb{E}_{y \sim \pi_\theta(\cdot|x)} \Big[ b(x) \nabla_\theta \log \pi_\theta(y|x) \Big]
&amp; = b(x) \mathbb{E}_{y \sim \pi_\theta(\cdot|x)} \Big[  \nabla_\theta \log \pi_\theta(y|x) \Big]  \\
&amp;= b(x) \sum_y \pi_\theta(y|x)\,\nabla_\theta \log \pi_\theta(y|x) \\
&amp;= b(x) \sum_y \nabla_\theta \pi_\theta(y|x) \quad \text{(Log-Derivative Trick)} \\
&amp;= b(x) \nabla_\theta \sum_y \pi_\theta(y|x) \\
&amp;= b(x) \nabla_\theta 1 \\
&amp;= 0  \\
\end{split}
\tag{6}\]</span></span></p>
<p>但是 GRPO 里的 <span class="math inline">\(\sigma(y_{1:G})\)</span> 依赖于组内所有采样回答的奖励 <span class="math inline">\(\{r_1, r_2, \dots, r_G\}\)</span>，而这些奖励本身又依赖于采样回答 <span class="math inline">\(\{y_1, y_2, \dots, y_G\}\)</span>, 于是，我们的梯度变成：</p>
<p><span id="eq-grpo-biased-gradient"><span class="math display">\[
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{y_{1:G} \sim \pi_\theta}\Big[\underbrace{\frac{r_i-\mu(y_{1:G})}{\sigma(y_{1:G})}}_{\text{依赖采样}}\,\nabla_\theta\log\pi_\theta(y_i|x)\Big]
\tag{7}\]</span></span></p>
<p>我们来看看，这个梯度为什么是有偏的： <span id="eq-biased-baseline"><span class="math display">\[
\mathbb E_{y_{1:G}}\left[\frac{r_i-\mu(y_{1:G})}{\sigma(y_{1:G})}\,\nabla_\theta\log\pi_\theta(y_i|x)\right]
\neq
\underbrace{\mathbb E\left[\frac{r_i-\mu(y_{1:G})}{\sigma(y_{1:G})}\right]}_{\text{不能提出来}}
\cdot
\underbrace{\mathbb E[\nabla_\theta\log\pi_\theta(y_i|x)]}_{=0}
\tag{8}\]</span></span></p>
<p>因为 <span class="math inline">\(\frac{r_i-\mu(y_{1:G})}{\sigma(y_{1:G})}\)</span> 也依赖于采样结果 <span class="math inline">\(y_{1:G}\)</span>，所以与 <span class="math inline">\(\nabla_\theta\log\pi_\theta(y_i|x)\)</span> 不能<strong>拆开期望</strong>，从而导致梯度估计有偏。</p>
<div class="note foldable is-open">
<div class="note-header foldable-header">
<p>NOTE</p>
</div>
<div class="note-container foldable-content">
<p>当两个随机变量 <span class="math inline">\(X, Y\)</span> 独立时，<span class="math inline">\(\mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y]\)</span>；但当 <span class="math inline">\(X, Y\)</span> 不独立时，这个等式不成立。这也就是为什么，在 <a href="#eq-baseline-zero-expectation" class="quarto-xref">Equation&nbsp;6</a> 中，我们可以把 <span class="math inline">\(b(x)\)</span> 提出来，因为它不依赖 <span class="math inline">\(y\)</span>，所以与 <span class="math inline">\(\nabla_\theta \log \pi_\theta(y|x)\)</span> 独立；但在 <a href="#eq-biased-baseline" class="quarto-xref">Equation&nbsp;8</a> 中，<span class="math inline">\(\frac{r_i-\mu(y_{1:G})}{\sigma(y_{1:G})}\)</span> 依赖于采样结果 <span class="math inline">\(y_{1:G}\)</span>，所以与 <span class="math inline">\(\nabla_\theta\log\pi_\theta(y_i|x)\)</span> 不独立，不能拆开期望。</p>
</div>
</div>
<p>这种方式，我们的梯度估计就<strong>不再是un-biased的</strong>，而是<strong>被一个与采样相关的随机因子重新加权</strong>了。</p>
<p>Bias 主要出在了 <span class="math inline">\(\sigma\)</span> 上，我们接下来具体分析它带来的影响:</p>
<ul>
<li>当某个 prompt 的 group 里 <strong>奖励方差很小</strong>（组内Reward全对或者全错）时，<span class="math inline">\(\frac{1}{\sigma}\)</span> 会非常大 → 这个 prompt 对梯度更新的权重被<strong>放大</strong>。</li>
<li>当奖励方差大时，更新被缩小。</li>
</ul>
<p>这就会导致，<strong>训练过程中，模型会更关注“极端题”（全对/全错）</strong>，而不是“中等难度、最能学到东西的题”。</p>
<div class="question foldable">
<div class="question-header foldable-header">
<p>Question: 为什么，<span class="math inline">\(\mu\)</span>（组内均值）不会带来Bias呢？</p>
</div>
<div class="question-container foldable-content">
<p>因为 <span class="math inline">\(\mu\)</span> 只是一个<strong>平移</strong>，它不会改变梯度的权重，只会改变梯度的方向（让模型更关注相对更好的回答）。而 <span class="math inline">\(\sigma\)</span> 是一个<strong>缩放</strong>，它会改变梯度的权重，从而引入Bias。</p>
</div>
</div>
</section>
<section id="length-normalization-bias" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="length-normalization-bias"><span class="header-section-number">3.4.2</span> Length Normalization Bias</h3>
<p>在 <a href="#eq-grpo" class="quarto-xref">Equation&nbsp;1</a> 中，有一个 <strong>按回答长度归一化</strong> 的项 <span class="math inline">\(\frac{1}{|o_i|}\)</span>，也就是把每条回答的 token-loss 平均化。</p>
<p>这会带来<strong>response-level length bias</strong>：当把一条回答的梯度按长度平均时，会改变“同样一条回答”在优化中的有效权重。（很抽象对不对，我们来看两个具体情况）</p>
<ol type="1">
<li><p><strong>优势为正（答对/更好，<span class="math inline">\(\hat{A} &gt; 0\)</span>）</strong> 在这种情况下，更新方向是“提高这条回答概率”，但除以回答长度 <span class="math inline">\(|o_i|\)</span>，这意味着<strong>越短的回答，单位长度的梯度越大</strong>， 每个 token 的贡献更大。于是，模型学到：<strong>正确答案要尽量短</strong>（因为更“划算”）。</p></li>
<li><p><strong>优势为负（答错/更差，<span class="math inline">\(\hat{A} &lt; 0\)</span>）</strong> 在这种情况下，更新方向是“降低这条回答概率”（惩罚），但除以回答长度 <span class="math inline">\(|o_i|\)</span>，这意味着<strong>越长的回答，惩罚被摊薄</strong>，每个 token 的负更新更小。于是，模型学到：<strong>错误答案反而更愿意写长</strong>（因为“受罚更轻”）。</p></li>
</ol>
<p>这就是为什么有时候CoT越来越长，但是Reward还是不升反降的现象——模型学会了通过输出更长的回答来“规避惩罚”，而不是通过提高回答质量来获得更高的奖励。</p>
<div id="fig-deepseek-r1-length-bias" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deepseek-r1-length-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/deepseek-r1-length-bias.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deepseek-r1-length-bias-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: 该张图展示了 DeepSeek-R1-Zero 在训练过程中，模型平均每条回答长度随训练步数的变化。横轴是训练步数（Steps），纵轴是平均每条 response 的长度（token 数）。可以看到，随着训练推进，模型输出长度呈持续上升趋势：从训练初期的几百到上千 token，逐步增长到后期的一万甚至接近两万 token，同时波动幅度也明显扩大。这一现象直观反映了 R1-Zero 在仅使用可验证奖励进行强化学习时，模型倾向于生成越来越长的推理链（CoT），体现出典型的“长度膨胀”行为，而不一定对应更高的实际解题效率。
</figcaption>
</figure>
</div>
<p>接下来，我们来看 Dr.GRPO<span class="citation" data-cites="UnderstandingR1ZeroLikeTraining2025liu">(<a href="#ref-UnderstandingR1ZeroLikeTraining2025liu" role="doc-biblioref">Liu et al. 2025</a>)</span> 是如何解决这两个问题的。</p>
</section>
</section>
<section id="dr.grpo" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="dr.grpo"><span class="header-section-number">3.5</span> Dr.GRPO</h2>
<p>Dr.GRPO 是 GRPO 的一个变体，解决了上面提到的两个问题：</p>
<ul>
<li><strong>去掉 <span class="math inline">\(\frac{1}{\sigma}\)</span></strong>，避免 biased gradient</li>
<li><strong>去掉按回答长度归一化</strong>，避免长度偏置</li>
</ul>
<p>我们来看看改动之后的目标函数:</p>
<p><span id="eq-dr-grpo"><span class="math display">\[
\mathcal{J}_{\text{Dr.GRPO}}(\pi_\theta) = \frac{1}{G}\sum_{i=1}^{G}
\sum_{t = 1}^{|o_i|}
\Bigg\{
\min \Big[
     \rho(o_{i,t}) \hat{A}_{i, t},
     \text{clip}\Big(
            \rho(o_{i,t}), 1 - \epsilon_{\text{clip}}, 1 + \epsilon_{\text{clip}}
        \Big) \hat{A}_{i, t}
    \Big]    
\Bigg\}
\tag{9}\]</span></span></p>
<p>其中，Advantage 变成：</p>
<p><span id="eq-dr-grpo-advantage"><span class="math display">\[
\hat{A}_{i, t} = r_i - \mu \quad \text{where} \quad \mu = \frac{1}{G}\sum_{j=1}^{G} r_j
\tag{10}\]</span></span></p>
<p>也就是只用组内均值当 baseline，不做方差归一化。</p>
<p>另一个改动是<strong>去掉了按回答长度归一化</strong>，也就是不再除以 <span class="math inline">\(|o_i|\)</span>， 直接对所有 token 的 policy-gradient loss 求和。 不过在实现中，会除以一个<code>MAX_TOKENS</code>（最大token数）来稳定训练，但这个是个常数，不会引入长度偏置。</p>
<p>我们来看看代码</p>
<div class="sourceCode" id="cb3" data-code-line-numbers="22,23"><pre class="sourceCode numberSource Python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> compute_loss_grpo(</span>
<span id="cb3-2"><a href="#cb3-2"></a>    log_probs: torch.Tensor,      <span class="co"># (B, G, T) log probs under current policy</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>    old_log_probs: torch.Tensor,  <span class="co"># (B, G, T) log probs under old policy (detached)</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>    response_mask: torch.Tensor,  <span class="co"># (B, G, T) mask for valid response tokens</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>    advantage: torch.Tensor,    <span class="co"># (B, G) advantage per response</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>    clip_range: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span id="cb3-7"><a href="#cb3-7"></a>    max_tokens: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span>,</span>
<span id="cb3-8"><a href="#cb3-8"></a>):</span>
<span id="cb3-9"><a href="#cb3-9"></a>    B, G, T <span class="op">=</span> log_probs.size()</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a>    important_ratio <span class="op">=</span> torch.exp(log_probs <span class="op">-</span> old_log_probs)  <span class="co"># (B, G, T)</span></span>
<span id="cb3-12"><a href="#cb3-12"></a></span>
<span id="cb3-13"><a href="#cb3-13"></a>    <span class="co"># Broadcast advantage to token level</span></span>
<span id="cb3-14"><a href="#cb3-14"></a>    advantage_tok <span class="op">=</span> advantage.unsqueeze(<span class="op">-</span><span class="dv">1</span>).expand_as(important_ratio)  <span class="co"># (B, G, T)</span></span>
<span id="cb3-15"><a href="#cb3-15"></a></span>
<span id="cb3-16"><a href="#cb3-16"></a>    unclipped <span class="op">=</span> important_ratio <span class="op">*</span> advantage_tok</span>
<span id="cb3-17"><a href="#cb3-17"></a>    clipped <span class="op">=</span> torch.clamp(important_ratio, <span class="dv">1</span> <span class="op">-</span> clip_range, <span class="dv">1</span> <span class="op">+</span> clip_range) <span class="op">*</span> advantage_tok</span>
<span id="cb3-18"><a href="#cb3-18"></a></span>
<span id="cb3-19"><a href="#cb3-19"></a>    pg_loss_tok <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">min</span>(unclipped, clipped)  <span class="co"># (B, G, T)</span></span>
<span id="cb3-20"><a href="#cb3-20"></a></span>
<span id="cb3-21"><a href="#cb3-21"></a>    <span class="co"># Length normalization (GRPO does this)</span></span>
<span id="cb3-22"><a href="#cb3-22"></a>    pg_loss_tok <span class="op">=</span> pg_loss_tok <span class="op">*</span> response_mask  <span class="co"># mask out non-response tokens </span></span>
<span id="cb3-23"><a href="#cb3-23"></a>    pg_loss_seq <span class="op">=</span> pg_loss_tok.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>) <span class="op">/</span> max_tokens  <span class="co"># (B, G) </span></span>
<span id="cb3-24"><a href="#cb3-24"></a></span>
<span id="cb3-25"><a href="#cb3-25"></a>    <span class="co"># Final loss: mean over batch and group</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>    loss <span class="op">=</span> pg_loss_seq.<span class="bu">sum</span>()</span>
<span id="cb3-27"><a href="#cb3-27"></a>    loss <span class="op">=</span> loss <span class="op">/</span> (B <span class="op">*</span> G)</span>
<span id="cb3-28"><a href="#cb3-28"></a>    <span class="co"># loss = pg_loss_seq.mean()  # alternative: mean over all tokens</span></span>
<span id="cb3-29"><a href="#cb3-29"></a></span>
<span id="cb3-30"><a href="#cb3-30"></a>    <span class="cf">return</span> loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-drgrpo-length-norm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-drgrpo-length-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/drgrpo-length-norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-drgrpo-length-norm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: 主要看图中红色部分，Dr.GRPO在错误回答时，不再按长度归一化，这样<strong>长回答不会被“惩罚更轻”</strong>，从而避免了模型学会写长错误回答的现象。
</figcaption>
</figure>
</div>
</section>
</section>
<section id="case-studies" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Case Studies</h1>
<p>在了解了GRPO算法后，我们来看看它是如何在实际例子中应用的，包括 DeepSeek-R1、Kimi 1.5 和 Qwen 3 等模型。</p>
<section id="deepseek-r1" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="deepseek-r1"><span class="header-section-number">4.1</span> DeepSeek-R1</h2>
<p>DeepSeek-R1 在2025年春节期间发布（还记得当时铺天盖地全是DeepSeek的新闻，过年都被卷到），它是第一个用 GRPO 做数学推理强化学习的模型。接下来我们来一下DeepSeek R1 <span class="citation" data-cites="DeepSeekR1IncentivizingReasoning2025deepseek-ai">(<a href="#ref-DeepSeekR1IncentivizingReasoning2025deepseek-ai" role="doc-biblioref">DeepSeek-AI et al. 2025</a>)</span>是如何训练的。</p>
<div class="warning foldable is-open">
<div class="warning-header foldable-header">
<p>WARNING: 关于DeepSeek-R1 Paper与课堂上的不同</p>
</div>
<div class="warning-container foldable-content">
<p>DeepSeek R1 在最近（2026年1月）重新Release了它们新的训练细节论文，该笔记基于最新论文内容进行整理。而课程中是基于最初的技术报告进行讲解，二者在细节上可能存在差异。</p>
</div>
</div>
<p>首先，我看一下DeepSeek R1的整体pipeline：</p>
<div id="fig-deepseek-r1-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deepseek-r1-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/deepseek-R1-training-pipeline.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-deepseek-r1-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
<p>DeepSeek R1 并不是一次性通过强化学习训练出来的模型，而是一条<strong>多阶段、逐步增强推理能力</strong>的训练流水线。</p>
<section id="from-deepseek-v3-to-deepseek-r1-zero" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="from-deepseek-v3-to-deepseek-r1-zero"><span class="header-section-number">4.1.1</span> From DeepSeek-V3 to DeepSeek-R1-Zero</h3>
<p>从 DeepSeek V3 Base 出发，研究者首先进行了几乎“纯 RL”的实验（R1-zero）：仅在推理类 prompt 上，用 accuracy 与 format 这类可验证的 outcome-level 奖励进行强化学习，验证一个关键假设——在没有 process supervision 的情况下，RL 是否足以激活模型的推理能力。</p>
<blockquote class="blockquote">
<p>Specifically, we apply the RL technique on the DeepSeek-V3 base to train DeepSeek-R1-Zero. During training, we design a straightforward template, to require DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases to ensure that we can accurately observe the model’s natural progression during the RL process. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.4 </cite></p>
</blockquote>
<p>结果表明推理能力确实可以被激发，同时发现了 “aha moment” 现象：模型在训练过程中会突然开始生成更长、更复杂的 CoT，伴随推理正确率的显著提升。</p>
<blockquote class="blockquote">
<p>Notably, during training, DeepSeek-R1-Zero exhibits an “aha moment” characterized by a sudden increase in the use of the word “wait” during reflections <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.5 </cite></p>
</blockquote>
<p>DeeoSeek R1-zero 的成功验证了 RLVR 思路的可行性：即使没有 process-level supervision，仅凭 outcome-level 的可验证奖励，RL 也能激发 LLM 的推理能力。</p>
<blockquote class="blockquote">
<p>The self-evolution of DeepSeek-R1-Zero underscores the power and beauty of RL: rather than explicitly teaching the model how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.5 </cite></p>
</blockquote>
<p>不过课程中提到，CoT 变长、“aha/backtracking”等现象；可能是来自目标/实现细节的偏置，不一定是“学会更深思考”的必然， “aha”这类文本也可能在底座模型上偶发出现，不必过度神化“涌现”。</p>
<div id="fig-" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig--caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/Aha-moment-dr-grpo.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig--caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: shows two examples to demonstrate that the DeepSeek-V3-Base model already exhibits the so-called “aha moment” even before the RL-tuning (Image Source <span class="citation" data-cites="UnderstandingR1ZeroLikeTraining2025liu">(<a href="#ref-UnderstandingR1ZeroLikeTraining2025liu" role="doc-biblioref">Liu et al. 2025</a>)</span>)
</figcaption>
</figure>
</div>
<p>在这个阶段，DeepSeek R1-zero 的训练采用了 GRPO <a href="#algo-grpo" class="quarto-xref">Algorithm 1</a> 算法，并设计了专门的奖励函数：</p>
<ul>
<li><strong>Outcome-level 奖励</strong>：主要包括 accuracy 奖励（答案正确与否）和 format 奖励（输出格式是否符合要求）</li>
</ul>
</section>
<section id="from-deepseek-r1-zero-to-deepseek-r1" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="from-deepseek-r1-zero-to-deepseek-r1"><span class="header-section-number">4.1.2</span> From DeepSeek-R1-Zero to DeepSeek-R1</h3>
<p>DeepSeek-R1-zero 很好，但是它存在几个问题：</p>
<blockquote class="blockquote">
<p>Although DeepSeek-R1-Zero exhibits strong reasoning capabilities, it faces several issues. DeepSeek-R1-Zero struggles with challenges like <strong>poor readability</strong>, and <strong>language mixing</strong>, as DeepSeek-V3-Base is trained on multiple languages, especially English and Chinese. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.6 </cite></p>
</blockquote>
<p>因此，在R1-zero的基础上，DeepSeek团队设计了 DeepSeek-R1 的训练方案，来提升模型的表达质量与行为 稳定性。具体来说：</p>
<ul>
<li><strong>SFT 冷启动</strong>：利用 R1-zero 采样得到的大量推理轨迹，经过过滤与人工/模型精修后，对 V3 Base 进行 Cold-start Long CoT 的 SFT，得到 R1 Dev 系列模型；</li>
<li><strong>多轮 GRPO 式 RL</strong>：随后再通过多轮 GRPO 式 RL，引入语言一致性奖励、偏好奖励以及更丰富的任务分布，逐步把模型从“只会解题”推向“推理强、表达稳定、行为可控”的产品级模型。</li>
</ul>
<blockquote class="blockquote">
<p>In the initial stage, we collect thousands of <strong>cold-start data</strong> that exhibits a conversational, human-aligned thinking process. RL training is then applied to improve the model performance with the conversational thinking process and language consistency. Subsequently, we apply rejection sampling and SFT once more. This stage incorporates both reasoning and nonreasoning datasets into the SFT process, enabling the model to not only excel in reasoning tasks but also demonstrate advanced writing capabilities. To further align the model with human preferences, we implement a secondary RL stage designed to enhance the model’s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.6 </cite></p>
</blockquote>
<p>同时，DeepSeek R1 也在奖励设计上做了改进：</p>
<ul>
<li><strong>多维奖励设计</strong>：不仅包含可验证的 outcome-level 奖励（accuracy、format），还引入了Language consistency reward、Preference / non-verifiable rewards（通过对比学习训练的偏好模型）等，更全面地衡量模型输出质量；</li>
<li><strong>奖励混合与调优</strong>：通过对不同奖励进行加权混合与调优，确保模型在提升推理能力的同时，也能兼顾表达质量与行为稳定性。</li>
<li><strong>持续的奖励模型训练</strong>：在 RL 训练过程中，持续更新奖励模型，以适应模型能力的提升与任务分布的变化。</li>
</ul>
</section>
<section id="deepseek-distillation" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="deepseek-distillation"><span class="header-section-number">4.1.3</span> DeepSeek Distillation</h3>
<p>在 DeepSeek R1 训练完成后，研究者还进行了蒸馏（distillation），以提升模型的推理效率与实用性.</p>
<blockquote class="blockquote">
<p>To enable broader access to powerful AI at a lower energy cost, we have distilled several smaller models and made them publicly available. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.2 </cite></p>
</blockquote>
<p>具体的方法就是：用 R1 作为 teacher 模型，生成大量高质量的推理样本，然后对更小的 student 模型进行 SFT 蒸馏训练。</p>
<blockquote class="blockquote">
<p>For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. <cite> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.60 </cite></p>
</blockquote>
</section>
</section>
<section id="kimi-1.5" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="kimi-1.5"><span class="header-section-number">4.2</span> Kimi 1.5</h2>
<p>和 DeepSeek R1 更像“配方驱动（SFT→RL→再对齐）”不同，课程把 Kimi 1.5 的经验总结成一句话：<strong>data is king</strong>。Kimi 的核心不是提出全新 RL 算法，而是把“可验证奖励推理 RL”变成一条更工程化的数据管线：先对任务做跨领域分类与分布平衡，避免模型只在单一类型题上变强；同时主动剔除容易通过随机猜测获得奖励的题型（例如选择题、判断题），优先保留短答案、可被规则/判定器高精度验证的任务，以减少 reward hacking 的空间。</p>
<p>最关键的一步是 best-of-n 难度筛选：用当前还不够强的 SFT 模型对每道题采样多次（课上举例类似 best-of-8），然后只保留“采样多次仍然失败”的题（fail best-of-n）。直觉上，这相当于把训练数据限制在“当前模型确实不会，但又可能学会”的可学区间，显式做出一种 curriculum 的雏形：太容易的题学不到东西，太难的题只会产生大量 0 reward；而经过筛选的题更可能提供稳定的学习信号。课程的 takeaway 是：在 RLVR 场景里，数据难度控制往往比算法 tweak 更决定训练效率和最终行为。</p>
</section>
<section id="qwen3" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="qwen3"><span class="header-section-number">4.3</span> Qwen3</h2>
<p>课程在讲 Qwen3 时，强调它的亮点不只是“也用了 RLVR/GRPO”，而是把推理模型在真实使用场景里的一个核心矛盾摆到台面上：推理越强，往往越贵（更长的 CoT、更高的 test-time compute）。因此 Qwen3 的思路更像是把“推理能力”和“推理成本”一起纳入训练目标：一方面沿用 “SFT（长 CoT 冷启动）→ 可验证奖励 RL（提升推理正确性）→ 再做通用对齐” 的主线；另一方面通过训练与数据/奖励设计，让模型学会在不同问题上自适应地选择推理强度——该认真推时能推得深，不需要推理时也能走更短、更便宜的路径。</p>
<p>从课程视角看，Qwen3 体现了一种很实用的产品化取向：推理模型最终要在“正确率”和“成本/延迟”之间做权衡，单纯追求 CoT 变长并不等价于更好。与 DeepSeek R1 更强调“用 RL 激活推理”以及 Kimi 更强调“用数据筛选做 curriculum”相比，Qwen3 更像是在探索：如何把“计算预算可控”变成模型行为的一部分，从而避免 RL 训练把 CoT 拉爆、成本失控的副作用。</p>
</section>
</section>
<section id="grpo-implementation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> GRPO Implementation</h1>
<p>接下来我们通过一个简单的例子，来具体看看GRPO的算法。这个例子是来自 <a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_17.json">Lecture17</a><strong>group baseline / 标准化优势 / clip ratio</strong> 这些 GRPO/PPO 家族的核心机制, 我们会根据 <a href="#algo-grpo" class="quarto-xref">Algorithm 1</a> 中的伪代码，一步步实现 GRPO。</p>
<section id="data-goal" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="data-goal"><span class="header-section-number">5.1</span> Data &amp; Goal</h2>
<p><strong>任务：排序（Sorting）</strong></p>
<ul>
<li><strong>Prompt</strong>：长度为 LLL 的整数序列，例如 <code>[3, 1, 2, 0]</code></li>
<li><strong>Response</strong>：模型输出同样长度 LLL 的序列，希望是排序后的结果 <code>[0, 1, 2, 3]</code></li>
<li><strong>group 结构（GRPO 的关键）</strong>：对同一个 prompt，采样 <span class="math inline">\(k\)</span> 条 response <span class="math inline">\(\{a^{(1)}, a^{(2)}, \dots, a^{(K)}\}\)</span> 形成一组，用于计算组内 baseline（均值/标准差）。</li>
</ul>
<div class="sourceCode" id="cb4" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> gen_prompts(batch_size: <span class="bu">int</span>, seq_len: <span class="bu">int</span>, vocab_size: <span class="bu">int</span>, device) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="cf">return</span> torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>vocab_size, size<span class="op">=</span>(batch_size, seq_len), device<span class="op">=</span>device)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">def</span> sorted_ground_truth(prompt: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="cf">return</span> torch.sort(prompt, dim<span class="op">=-</span><span class="dv">1</span>).values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="reward" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="reward"><span class="header-section-number">5.2</span> Reward</h2>
<p>首先我们需要定义 reward 函数。Lecture 里强调：如果 reward 只给 <code>0/1</code>（完全正确/错误），会导致<strong>稀疏奖励</strong>，训练很容易卡住。因此这里使用 <strong>partial credit</strong>来让学习更平滑。</p>
<section id="reward-v2inclusion-adjacent-sorted-pairs课堂采用的更细奖励" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="reward-v2inclusion-adjacent-sorted-pairs课堂采用的更细奖励"><span class="header-section-number">5.2.1</span> Reward v2：Inclusion + Adjacent Sorted Pairs（课堂采用的更细奖励）</h3>
<p>给定 prompt <span class="math inline">\(x\)</span> 和 response <span class="math inline">\(y\)</span>：</p>
<ol type="1">
<li><strong>Inclusion reward</strong>：prompt 里的 token 在 response 里出现就给分（按计数，多重集）</li>
</ol>
<p><span id="eq-reward-inclusion"><span class="math display">\[
\text{Inclusion reward:} \quad R_{\text{inc}}(x,y)=\sum_{t} \min(\text{count}_x(t), \text{count}_y(t))
\tag{11}\]</span></span></p>
<ol start="2" type="1">
<li><strong>Adjacent sorted pairs</strong>：统计 response 中相邻对满足非降序的个数</li>
</ol>
<p><span id="eq-reward-adjacent-sorted-pairs"><span class="math display">\[
\text{Adjacent sorted pairs:} \quad R_{\text{adj}}(y)=\sum_{i=1}^{L-1}\mathbb{I}[y_i \le y_{i+1}]
\tag{12}\]</span></span></p>
<p>总 reward：</p>
<p><span id="eq-reward-total"><span class="math display">\[
\text{Total reward:} \quad R(x,y)=R_{\text{inc}}(x,y)+R_{\text{adj}}(y)
\tag{13}\]</span></span></p>
<div class="highlight">
<p>课堂也提到：这种 reward 可能存在“漏洞”（reward hack），因此 reward 设计本身就是 RL 的难点之一。</p>
</div>
<div class="sourceCode" id="cb5" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> sort_distance_reward(prompt: <span class="bu">list</span>[<span class="bu">int</span>], response: <span class="bu">list</span>[<span class="bu">int</span>]) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="cf">assert</span> <span class="bu">len</span>(prompt) <span class="op">==</span> <span class="bu">len</span>(response)</span>
<span id="cb5-3"><a href="#cb5-3"></a>    ground_truth <span class="op">=</span> <span class="bu">sorted</span>(prompt)</span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="cf">return</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(response, ground_truth) <span class="cf">if</span> x <span class="op">==</span> y)</span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="kw">def</span> sort_inclusion_ordering_reward(prompt: <span class="bu">list</span>[<span class="bu">int</span>], response: <span class="bu">list</span>[<span class="bu">int</span>]) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="co">"""</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">    Return how close response is to ground_truth = sorted(prompt).</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">    """</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>    <span class="cf">assert</span> <span class="bu">len</span>(prompt) <span class="op">==</span> <span class="bu">len</span>(response)</span>
<span id="cb5-12"><a href="#cb5-12"></a>    <span class="co"># Give one point for each token in the prompt that shows up in the response</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>    inclusion_reward <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> x <span class="kw">in</span> prompt <span class="cf">if</span> x <span class="kw">in</span> response)  <span class="co"># @inspect inclusion_reward</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>    <span class="co"># Give one point for each adjacent pair in response that's sorted</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>    ordering_reward <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(response, response[<span class="dv">1</span>:]) <span class="cf">if</span> x <span class="op">&lt;=</span> y)  <span class="co"># @inspect ordering_reward</span></span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="cf">return</span> inclusion_reward <span class="op">+</span> ordering_reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="model" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="model"><span class="header-section-number">5.3</span> Model</h2>
<p>对于这个任务，我们就定一个简单的模型：</p>
<div class="sourceCode" id="cb6" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">class</span> ToySortPolicy(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span>, embedding_dim: <span class="bu">int</span>, prompt_length: <span class="bu">int</span>, response_length: <span class="bu">int</span>):</span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb6-6"><a href="#cb6-6"></a>        <span class="va">self</span>.emb <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="va">self</span>.enc <span class="op">=</span> nn.Parameter(</span>
<span id="cb6-8"><a href="#cb6-8"></a>            torch.randn(prompt_length, embedding_dim, embedding_dim) <span class="op">/</span> math.sqrt(embedding_dim)</span>
<span id="cb6-9"><a href="#cb6-9"></a>        )</span>
<span id="cb6-10"><a href="#cb6-10"></a>        <span class="va">self</span>.dec <span class="op">=</span> nn.Parameter(</span>
<span id="cb6-11"><a href="#cb6-11"></a>            torch.randn(response_length, embedding_dim, embedding_dim) <span class="op">/</span> math.sqrt(embedding_dim)</span>
<span id="cb6-12"><a href="#cb6-12"></a>        )</span>
<span id="cb6-13"><a href="#cb6-13"></a>        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(embedding_dim, vocab_size)</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, prompt):</span>
<span id="cb6-16"><a href="#cb6-16"></a>        x <span class="op">=</span> <span class="va">self</span>.emb(prompt)  <span class="co"># (B,L,d1)</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>        h <span class="op">=</span> torch.einsum(<span class="st">"bld,ldm-&gt;bm"</span>, x, <span class="va">self</span>.enc)  <span class="co"># (B,d2)</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>        z <span class="op">=</span> torch.einsum(<span class="st">"bm,lmd-&gt;bld"</span>, h, <span class="va">self</span>.dec)  <span class="co"># (B,L,d1)</span></span>
<span id="cb6-19"><a href="#cb6-19"></a>        <span class="cf">return</span> <span class="va">self</span>.out(z)  <span class="co"># (B,L,V)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="algorithm" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">5.4</span> Algorithm</h2>
<div class="sourceCode" id="cb7" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> grpo(cfg: GRPOCfg <span class="op">=</span> GRPOCfg()):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    dev <span class="op">=</span> get_device()</span>
<span id="cb7-3"><a href="#cb7-3"></a>    set_seed(cfg.seed)</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>    policy <span class="op">=</span> ToySortPolicy(cfg.V, cfg.L, cfg.L, cfg.L).to(dev)</span>
<span id="cb7-6"><a href="#cb7-6"></a>    opt <span class="op">=</span> torch.optim.Adam(policy.parameters(), lr<span class="op">=</span>cfg.lr)</span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a>    reward_fn <span class="op">=</span> REWARD_FN_MAP[cfg.reward_fn]</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a>    <span class="co"># reference model (slow-moving anchor for KL)</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>    ref <span class="op">=</span> ToySortPolicy(cfg.V, cfg.L, cfg.L, cfg.L).to(dev)</span>
<span id="cb7-12"><a href="#cb7-12"></a>    ref.load_state_dict(policy.state_dict())</span>
<span id="cb7-13"><a href="#cb7-13"></a>    ref.<span class="bu">eval</span>()</span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(cfg.outer_iters):</span>
<span id="cb7-16"><a href="#cb7-16"></a>        <span class="co"># update reference model slowly</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="cf">if</span> cfg.use_kl <span class="kw">and</span> it <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> (it <span class="op">%</span> cfg.ref_update_every <span class="op">==</span> <span class="dv">0</span>):</span>
<span id="cb7-18"><a href="#cb7-18"></a>            ref.load_state_dict(policy.state_dict())</span>
<span id="cb7-19"><a href="#cb7-19"></a>            ref.<span class="bu">eval</span>()</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>        <span class="co"># ---------- OUTER: rollout + rewards + deltas + freeze old/ref logps</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>        prompt <span class="op">=</span> gen_prompts(cfg.B, cfg.L, cfg.V, dev)  <span class="co"># (B,L)</span></span>
<span id="cb7-23"><a href="#cb7-23"></a></span>
<span id="cb7-24"><a href="#cb7-24"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-25"><a href="#cb7-25"></a>            responses <span class="op">=</span> sample_responses(policy, prompt, cfg.K, cfg.temperature)  <span class="co"># (B,K,L)</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>            rewards <span class="op">=</span> compute_reward(prompt, responses, reward_fn)  <span class="co"># (B,K)</span></span>
<span id="cb7-27"><a href="#cb7-27"></a>            deltas <span class="op">=</span> compute_deltas(rewards, cfg.delta_mode)  <span class="co"># (B,K)</span></span>
<span id="cb7-28"><a href="#cb7-28"></a></span>
<span id="cb7-29"><a href="#cb7-29"></a>            <span class="co"># IMPORTANT: freeze old logps (detach)</span></span>
<span id="cb7-30"><a href="#cb7-30"></a>            old_logp_token <span class="op">=</span> compute_log_probs(prompt, responses, policy).detach()  <span class="co"># (B,K,L)</span></span>
<span id="cb7-31"><a href="#cb7-31"></a></span>
<span id="cb7-32"><a href="#cb7-32"></a>            <span class="co"># freeze ref logps for KL (detach)</span></span>
<span id="cb7-33"><a href="#cb7-33"></a>            ref_logp_token <span class="op">=</span> compute_log_probs(prompt, responses, ref).detach() <span class="cf">if</span> cfg.use_kl <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb7-34"><a href="#cb7-34"></a></span>
<span id="cb7-35"><a href="#cb7-35"></a>        <span class="co"># ---------- INNER: multiple gradient steps on same samples</span></span>
<span id="cb7-36"><a href="#cb7-36"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(cfg.inner_steps):</span>
<span id="cb7-37"><a href="#cb7-37"></a>            opt.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-38"><a href="#cb7-38"></a></span>
<span id="cb7-39"><a href="#cb7-39"></a>            logp_token <span class="op">=</span> compute_log_probs(prompt, responses, policy)  <span class="co"># (B,K,L)</span></span>
<span id="cb7-40"><a href="#cb7-40"></a></span>
<span id="cb7-41"><a href="#cb7-41"></a>            loss <span class="op">=</span> compute_loss(</span>
<span id="cb7-42"><a href="#cb7-42"></a>                log_probs<span class="op">=</span>logp_token,</span>
<span id="cb7-43"><a href="#cb7-43"></a>                old_log_probs<span class="op">=</span>old_logp_token,</span>
<span id="cb7-44"><a href="#cb7-44"></a>                deltas<span class="op">=</span>deltas,</span>
<span id="cb7-45"><a href="#cb7-45"></a>                mode<span class="op">=</span>cfg.loss_mode,</span>
<span id="cb7-46"><a href="#cb7-46"></a>            )</span>
<span id="cb7-47"><a href="#cb7-47"></a></span>
<span id="cb7-48"><a href="#cb7-48"></a>            <span class="cf">if</span> cfg.use_kl:</span>
<span id="cb7-49"><a href="#cb7-49"></a>                loss <span class="op">=</span> loss <span class="op">+</span> cfg.kl_beta <span class="op">*</span> compute_kl_penalty(logp_token, ref_logp_token)</span>
<span id="cb7-50"><a href="#cb7-50"></a></span>
<span id="cb7-51"><a href="#cb7-51"></a>            loss.backward()</span>
<span id="cb7-52"><a href="#cb7-52"></a>            opt.step()</span>
<span id="cb7-53"><a href="#cb7-53"></a></span>
<span id="cb7-54"><a href="#cb7-54"></a>        <span class="cf">if</span> (it <span class="op">%</span> cfg.print_every) <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> it <span class="op">==</span> cfg.outer_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb7-55"><a href="#cb7-55"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-56"><a href="#cb7-56"></a>                <span class="bu">print</span>(</span>
<span id="cb7-57"><a href="#cb7-57"></a>                    <span class="ss">f"iter </span><span class="sc">{</span>it<span class="sc">:04d}</span><span class="ss"> | "</span></span>
<span id="cb7-58"><a href="#cb7-58"></a>                    <span class="ss">f"reward mean </span><span class="sc">{</span>rewards<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss"> "</span></span>
<span id="cb7-59"><a href="#cb7-59"></a>                    <span class="ss">f"(min </span><span class="sc">{</span>rewards<span class="sc">.</span><span class="bu">min</span>()<span class="sc">.</span>item()<span class="sc">:.1f}</span><span class="ss">, max </span><span class="sc">{</span>rewards<span class="sc">.</span><span class="bu">max</span>()<span class="sc">.</span>item()<span class="sc">:.1f}</span><span class="ss">) | "</span></span>
<span id="cb7-60"><a href="#cb7-60"></a>                    <span class="ss">f"loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>                )</span>
<span id="cb7-62"><a href="#cb7-62"></a></span>
<span id="cb7-63"><a href="#cb7-63"></a>    <span class="cf">return</span> policy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="training"><span class="header-section-number">5.5</span> Training</h2>
<div class="sourceCode" id="cb8" data-code-line-numbers=""><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="at">@dataclass</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="kw">class</span> GRPOCfg:</span>
<span id="cb8-3"><a href="#cb8-3"></a>    V: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>  <span class="co"># vocab size (token values 0..V-1)</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>    L: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span>  <span class="co"># sequence length</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>    B: <span class="bu">int</span> <span class="op">=</span> <span class="dv">128</span>  <span class="co"># batch prompts per outer iter</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>    K: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>  <span class="co"># responses per prompt (group size)</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>    outer_iters: <span class="bu">int</span> <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-8"><a href="#cb8-8"></a>    inner_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb8-9"><a href="#cb8-9"></a></span>
<span id="cb8-10"><a href="#cb8-10"></a>    lr: <span class="bu">float</span> <span class="op">=</span> <span class="fl">2e-2</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a>    delta_mode: Literal[<span class="st">"raw"</span>, <span class="st">"centered_rewards"</span>, <span class="st">"normalized_rewards"</span>, <span class="st">"max_rewards"</span>] <span class="op">=</span> <span class="st">"centered_rewards"</span></span>
<span id="cb8-14"><a href="#cb8-14"></a>    loss_mode: Literal[<span class="st">"naive"</span>, <span class="st">"unclipped"</span>, <span class="st">"clipped"</span>] <span class="op">=</span> <span class="st">"clipped"</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>    clip_eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb8-16"><a href="#cb8-16"></a></span>
<span id="cb8-17"><a href="#cb8-17"></a>    use_kl: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>    kl_beta: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb8-19"><a href="#cb8-19"></a>    ref_update_every: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb8-20"><a href="#cb8-20"></a></span>
<span id="cb8-21"><a href="#cb8-21"></a>    seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-22"><a href="#cb8-22"></a>    print_every: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-23"><a href="#cb8-23"></a></span>
<span id="cb8-24"><a href="#cb8-24"></a>    reward_fn: Literal[<span class="st">"distance"</span>, <span class="st">"inclusion_ordering"</span>] <span class="op">=</span> <span class="st">"inclusion_ordering"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="results" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="results"><span class="header-section-number">5.6</span> Results</h2>
<pre class="text" data-code-line-numbers=""><code>iter 0000 | reward mean 3.070 (min 0.0, max 7.0) | loss 0.0138
iter 0020 | reward mean 3.250 (min 0.0, max 6.0) | loss 0.0034
iter 0040 | reward mean 3.396 (min 0.0, max 7.0) | loss 0.0008
iter 0060 | reward mean 3.625 (min 0.0, max 7.0) | loss 0.0016
iter 0080 | reward mean 3.813 (min 1.0, max 7.0) | loss 0.0059
iter 0100 | reward mean 4.071 (min 1.0, max 7.0) | loss 0.0043
iter 0120 | reward mean 4.278 (min 1.0, max 7.0) | loss 0.0030
iter 0140 | reward mean 4.497 (min 1.0, max 7.0) | loss 0.0058
iter 0160 | reward mean 4.710 (min 1.0, max 7.0) | loss 0.0058
iter 0180 | reward mean 4.859 (min 2.0, max 7.0) | loss 0.0033
iter 0199 | reward mean 4.964 (min 2.0, max 7.0) | loss 0.0067


=== sample check ===
prompt: [4, 2, 4, 4] | pred: [0, 2, 4, 6] | gt: [2, 4, 4, 4] | sort reward: 1 | inclusion+ordering reward: 7
prompt: [6, 8, 1, 9] | pred: [8, 9, 1, 1] | gt: [1, 6, 8, 9] | sort reward: 0 | inclusion+ordering reward: 5
prompt: [4, 3, 8, 7] | pred: [0, 3, 6, 7] | gt: [3, 4, 7, 8] | sort reward: 0 | inclusion+ordering reward: 5
prompt: [6, 0, 5, 6] | pred: [0, 5, 7, 9] | gt: [0, 5, 6, 6] | sort reward: 2 | inclusion+ordering reward: 5
prompt: [3, 1, 4, 6] | pred: [1, 1, 4, 7] | gt: [1, 3, 4, 6] | sort reward: 2 | inclusion+ordering reward: 5
prompt: [2, 7, 8, 0] | pred: [3, 7, 7, 8] | gt: [0, 2, 7, 8] | sort reward: 2 | inclusion+ordering reward: 5
prompt: [7, 9, 1, 2] | pred: [9, 9, 1, 9] | gt: [1, 2, 7, 9] | sort reward: 1 | inclusion+ordering reward: 4
prompt: [3, 6, 0, 2] | pred: [1, 1, 3, 7] | gt: [0, 2, 3, 6] | sort reward: 1 | inclusion+ordering reward: 4</code></pre>
<p>全部的代码可以在这个<a href="https://github.com/YYZhang2025/Stanford-CS336/blob/main/assignment5-alignment/simple_grpo.ipynb">Notebook</a>中看到</p>
</section>
</section>
<section id="summary" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Summary</h1>
<p>在这篇笔记中，我将 Lecture 16 与 Lecture 17 合并整理，围绕一个核心问题展开：为什么传统的 PPO 与 DPO 已不足以支撑推理型 LLM 的强化学习，以及 RLVR（Reinforcement Learning from Verified Rewards）如何成为新的突破口。我首先回顾了 PPO、DPO 在推理场景中的工程复杂度与数据形态局限，并由此引出 GRPO 这一代表性算法，系统梳理了其目标函数、基于 group 的相对 advantage 计算、clip ratio、KL 约束以及长度归一化等关键设计。在此基础上，我进一步分析了 GRPO 中两个重要但容易被忽略的偏置来源：一是由组内奖励标准差引入的 biased gradient，二是按回答长度归一化导致的 length bias，并结合 Dr.GRPO 展示了通过去除 1/与 length normalization 来缓解这些问题的思路。最后，我从课程视角总结了 DeepSeek-R1、Kimi 1.5 与 Qwen3 在 RLVR 实践中的不同取向，并通过一个排序任务的 toy 示例，完整实现了 GRPO 的数据生成、奖励设计、模型结构与训练流程，将算法从公式推导真正落到可运行的代码层面。 # Further 在完成了这三节课程之后，我们可以开始Assignment05的实现，在Assignment05中，我们会实现3中算法：</p>
<ul>
<li>SFT</li>
<li>Expert Iteration</li>
<li>GRPO</li>
</ul>
<p>并且比较它们的不同，具体的代码实现可以参考<a href="https://github.com/YYZhang2025/Stanford-CS336/tree/main/assignment5-alignment">GitHub</a>， 以及对应的<a href="https://yyzhang2025.github.io/posts/LearningNotes/CS336/Assignments/Ass05/ass05.html">笔记</a>。</p>
</section>
<section id="in-the-end" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> In the End</h1>
<p>最后，感谢你一路坚持到这里！创作不易，如果你觉得内容对你有帮助，欢迎请我<strong>喝杯咖啡/支付宝红包</strong>，支持我继续创作！你们的支持是我最大的动力！ :) <br></p>
<p><img src="../../../style/AliPay.jpg" class="img-fluid" width="300"></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-DeepSeekR1IncentivizingReasoning2025deepseek-ai" class="csl-entry" role="listitem">
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, et al. 2025. <span>“<span>DeepSeek-R1</span>: <span>Incentivizing Reasoning Capability</span> in <span>LLMs</span> via <span>Reinforcement Learning</span>.”</span> <em>Nature</em> 645 (8081): 633–38. <a href="https://doi.org/10.1038/s41586-025-09422-z">https://doi.org/10.1038/s41586-025-09422-z</a>.
</div>
<div id="ref-UnderstandingR1ZeroLikeTraining2025liu" class="csl-entry" role="listitem">
Liu, Zichen, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. <span>“Understanding <span>R1-Zero-Like Training</span>: <span>A Critical Perspective</span>.”</span> October 6, 2025. <a href="https://doi.org/10.48550/arXiv.2503.20783">https://doi.org/10.48550/arXiv.2503.20783</a>.
</div>
<div id="ref-DeepSeekMathPushingLimits2024shao" class="csl-entry" role="listitem">
Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, et al. 2024. <span>“<span>DeepSeekMath</span>: <span>Pushing</span> the <span>Limits</span> of <span>Mathematical Reasoning</span> in <span>Open Language Models</span>.”</span> April 27, 2024. <a href="https://doi.org/10.48550/arXiv.2402.03300">https://doi.org/10.48550/arXiv.2402.03300</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/sta210-s22\.github\.io\/website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark_dimmed">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "YYZhang2025/YYZhang2025.github.io";
    script.dataset.repoId = "R_kgDOQlDTcQ";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOQlDTcc4C2MRz";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../posts/CS336/Lecture15/lec15.html" class="pagination-link" aria-label="Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Lecture 15: LLM Alignment SFT &amp; RLHF(PPO, DPO)</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../posts/100-AI-Papers/01-transformer/Transformer.html" class="pagination-link" aria-label="Transformer">
        <span class="nav-page-text">Transformer</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize,
          commentDelimiter: el.dataset.commentDelimiter,
          lineNumber: el.dataset.lineNumber.toLowerCase() === "true",
          lineNumberPunc: el.dataset.lineNumberPunc,
          noEnd: el.dataset.noEnd.toLowerCase() === "true",
          titlePrefix: el.dataset.captionPrefix
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




<script src="../../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>