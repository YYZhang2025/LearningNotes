[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "posts/CS336/Ass04/ass04.html",
    "href": "posts/CS336/Ass04/ass04.html",
    "title": "Assignment 04: Data Collection & Processing",
    "section": "",
    "text": "Some text here."
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html",
    "href": "posts/CS336/Ass05/ass05.html",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "",
    "text": "TL;DR: 快速版\n\n\n只需将运行以下命令，We are ready to GO！！！\n下载代码\ngit clone https://github.com/YYZhang2025/Stanford-CS336.git\ncd Stanford-CS336/assignment5-alignment\n安装依赖，下载数据集和模型\npip install uv \nuv sync --no-install-package flash-attn\nuv sync\nsource .venv/bin/activate\n\nhf download YuYangZhang/Reasoning-Dataset  --repo-type dataset --local-dir data\n\npython download_model.py \\\n  --repo-id Qwen/Qwen2.5-Math-1.5B \\\n  --save-dir models/Qwen2.5-Math-1.5B \\\n  --method snapshot --no-symlinks --verify\n\n\n我们先来下载模型权重，只需一个命令：\npython download_model.py \\\n  --repo-id Qwen/Qwen2.5-Math-1.5B \\\n  --save-dir models/Qwen2.5-Math-1.5B \\\n  --method snapshot --no-symlinks --verify\n通过上面的命令，我们就可以把Qwen2.5-Math-1.5B模型下载到models/Qwen2.5-Math-1.5B目录下。\n\n在这个Assignment中，我们将会用到Math Dataset，不过Assignment中，由于版权问题，并没有提供完整的数据，因此我们需要自行下载数据集，在这个Assignment中，我们主要会用到以下两个数据集：\n\nGSM8K Dataset：一个包含8,500多个高中水平数学问题的数据集，专注于逐步推理和解决方案生成。（这个数据集在Assignment中已经提供 data/gsm8k）\nMATH Dataset: 这个数据集包含12,500多个高中和大学水平的数学问题，涵盖多个主题和难度级别 Link。\n\n我们现在下载MATH:\nhf download nlile/hendrycks-MATH-benchmark \\\n  --repo-type dataset \\\n  --local-dir ./data/hendrycks-MATH-benchmark\n\nmv ./data/hendrycks-MATH-benchmark  ./data/math\n接下来，我们来预处理一下这些数据集，因为不同的数据集格式不一样，我们需要把他们处理成统一的格式， 以便我们后续的训练。先来看GSM8K数据集的格式：\n{\n  \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", \n  \"answer\": \"Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.\\nNatalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.\\n#### 72\"}\n每个样本包含 question 和 answer 两个字段，我们需要把他们处理成 prompt 和 cot 的格式，并且提取出里面的答案：\n\n\nassignment5-alignment/cs336_alignment/dataset_utils/gsm8k.py\n\ndef extract_gsm8k_answer(answer: str) -&gt; str:\n    ANS_RE = re.compile(r\"####\\s*([\\-0-9\\.\\,]+)\")\n    match = ANS_RE.search(answer)\n    if match:\n        return match.group(1).strip().replace(\",\", \"\")\n    return \"[invalid]\"\n\n\ndef process_row(row: Dict[str, Any]):\n    problem = row[\"question\"]\n    cot = row[\"answer\"]\n    clean_cot = re.sub(r\"\\s*\\n####\\s*-?\\d+(?:\\.\\d+)?\\s*$\", \"\", cot)\n    answer = extract_gsm8k_answer(row[\"answer\"])\n\n    clean_cot = wrap_cot_with_answer(clean_cot, answer)\n\n    return problem, str(clean_cot), str(answer).lower() if answer is not None else None\n\n在预处理之后，我们会把数据集处理成下面的格式：\n{\n  \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", \n  \"cot\": \"Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.\\nNatalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.\\n&lt;/think&gt; &lt;answer&gt;72&lt;/answer&gt;\", \n  \"answer\": \"72\"\n}\n类似地，我们也需要对MATH数据集进行预处理，MATH数据集的格式如下：\n{\n  \"problem\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\", \n  \"solution\": \"The denominator of the rational function factors into $x^2+x-6=(x-2)(x+3)$. Since the numerator is always nonzero, there is a vertical asymptote whenever the denominator is $0$, which occurs for $x = 2$ and $x = -3$.  Therefore, the graph has $\\\\boxed{2}$ vertical asymptotes.\", \n  \"answer\": \"2\"\n}\n\n\nassignment5-alignment/cs336_alignment/dataset_utils/math.py\n\ndef process_row(row: Dict[str, Any]):\n    problem = row[\"problem\"]\n    cot = row[\"solution\"]\n\n    if row[\"answer\"] is None:\n        answer = extract_final_answer_from_text(cot)\n    else:\n        answer = row[\"answer\"]\n        cot = wrap_cot_with_answer(cot, answer)\n\n    return problem, str(cot), str(answer).lower() if answer is not None else None\n\n在处理之后，我们会把MATH数据集处理成下面的格式：\n{\n  \"question\": \"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\", \n  \"cot\": \"The denominator of the rational function factors into $x^2+x-6=(x-2)(x+3)$. Since the numerator is always nonzero, there is a vertical asymptote whenever the denominator is $0$, which occurs for $x = 2$ and $x = -3$.  Therefore, the graph has $\\\\boxed{2}$ vertical asymptotes.\\n&lt;/think&gt; &lt;answer&gt;2&lt;/answer&gt;\", \n  \"answer\": \"2\"\n}\n具体的细节，看assignment5-alignment/cs336_alignment/dataset_utils 文件夹下的代码。以及 assignment5-alignment/preprocess.py 这个脚本。在这里就不赘述了。处理完之后，我们会有：\ndata/\n├── alpaca_eval/\n├── gsm8k/\n├── math/\n├── mmlu/\n├── pre-processed/\n│   ├── gsm8k/\n│   │   ├── test.jsonl\n│   │   └── train.jsonl\n│   └── math/\n│       ├── test.jsonl\n│       └── train.jsonl\n当然，大家也可以选择直接使用我已经处理好的数据集， 直接从这里下载即可。或者使用下面的命令：\nhf download YuYangZhang/Reasoning-Dataset  --repo-type dataset --local-dir data"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#vllm",
    "href": "posts/CS336/Ass05/ass05.html#vllm",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "2.1 vLLM",
    "text": "2.1 vLLM\n在这个Assignment中，我们会使用vLLM 来进行模型的推理， 以便我们可以更快的进行评估和训练。 以下几个方法：\n\n\nassignment5-alignment/cs336_alignment/vllm_utils.py\n\ndef init_vllm(model_id: str, device: str, seed: int, gpu_memory_utilization: float = 0.85):\n    vllm_set_random_seed(seed)\n    world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n    profiling_patch = patch(\n        \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\", return_value=None\n    )\n    with world_size_patch, profiling_patch:\n        return LLM(\n            model=model_id,\n            device=device,\n            dtype=torch.bfloat16,\n            enable_prefix_caching=True,\n            gpu_memory_utilization=gpu_memory_utilization,\n        )\n\ndef load_policy_into_vllm_instance(policy: PreTrainedModel, llm: LLM):\n    state_dict = policy.state_dict()\n    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model\n    llm_model.load_weights(state_dict.items())\n  \ndef generate_responses(vllm: LLM, prompts: list[str], sampling_params) -&gt; list[str]:\n    outputs = vllm.generate(\n        prompts,\n        sampling_params=sampling_params,\n    )\n    responses = [output.outputs[0].text for output in outputs]\n    return responses\n\n当我们需要使用vLLM进行推理时，我们只需要先初始化vLLM实例， 之后把模型权重load进去， 最后调用generate_responses函数即可完成推理：\nvllm = init_vllm(\n    model_id=MODEL_NAME,\n    device=str(get_device(rank=1)),\n    seed=42,\n    gpu_memory_utilization=0.85,\n)\nsampling_params = SamplingParams(\n    max_tokens=1024, temperature=1, top_p=1, stop=[\"&lt;/answer&gt;\"], include_stop_str_in_output=True\n)\nload_policy_into_vllm_instance(policy, vllm)\nresponses = generate_responses(\n    vllm,\n    prompts,\n    sampling_params=sampling_params,\n)\n接下来，我们先来评估一下Qwen2.5-Math-1.5B在MATH和GSM8K数据集上的表现， 具体的评估代码在 assignment5-alignment/eval.py 以及 assignment5-alignment/cs336_alignment/eval.py， 运行下面的命令即可：\npython eval.py  \n来看一下评估结果：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndataset_path\ntotal\nanswer_correct\nformat_correct\nreward_1\nformatted_but_answer_wrong\nanswer_accuracy\n\n\n\n\nmath/train.jsonl\n12000\n359\n2038\n359\n1679\n0.029\n\n\nmath/test.jsonl\n500\n13\n77\n13\n64\n0.026\n\n\ngsm8k/train.jsonl\n7473\n232\n1433\n232\n1201\n0.031\n\n\ngsm8k/test.jsonl\n1319\n41\n258\n41\n217\n0.031\n\n\n\n\n\nTable 1: Qwen2.5-Math-1.5B Zero-Shot Evaluation Results on MATH and GSM8K Datasets\n\n\n\n\n可以看到，在Zero-Shot的情况下，Qwen2.5-Math-1.5B在MATH和GSM8K数据集上的表现都非常差，只有大约2.6%到3.1%的准确率。这也符合预期，因为Qwen2.5-Math-1.5B虽然是一个强大的语言模型，但在没有经过专门微调的情况下，其在复杂数学问题上的表现仍然有限。"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#tokenize-prompt-and-output",
    "href": "posts/CS336/Ass05/ass05.html#tokenize-prompt-and-output",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.1 Tokenize Prompt and Output",
    "text": "3.1 Tokenize Prompt and Output\n首先我们要定义的第一个函数就是 tokenize_prompt_and_output(), 它的作用，顾名思义，就是接收一系列的prompts，和 Response，并且tokenize他们，并且返回他们的ids。不过，需要注意的一点，也是很重要的一点就是，我们要同时返回Response Mask。 我们先来看看这个Response Mask的作用是什么：\n\n假如我们有一个 \\(q\\) 和 \\(o\\), 我们将它并在一起，得到了我们的函数 \\([q, o ]\\), 我们将整段传入Model，在没有Response Mask的情况下，模型就是对所有的token计算loss，这样模型就会被迫去预测：\n\nprompt 里的下一个 token（本质是在“复述/重建 prompt”）\noutput 里的 token（这才是我们真正关心的）\n\n但是在SFT训练的阶段， prompt 是输入条件，我们并不希望优化模型去“背 prompt 的分布”，只希望它在给定 prompt 后生成正确输出。所以用 response_mask 把 loss 限定在 output token 上： 训练信号只来自回答部分。当然，除此之外，Response Mask还对应着pad tokens 举个直观的例子：\n假设输入是：\n\nq: “2+2=?”\no: “4”\n\n拼接后 token 序列是：[q_tokens][o_tokens][pad...]\nresponse_mask 会像这样：\n\nq_tokens → False False False …\no_tokens → True True …\npad → False False …\n\nTokens:        [ 2 , + , 2 , = , ? , 4 , &lt;pad&gt; , &lt;pad&gt; ]\nResponse_mask: [ F , F , F , F , F , T ,   F   ,   F   ] \n结果：loss 只在 “4” 的 token 上算，prompt 部分完全不参与。\n我们来看看代码是怎么实现的：\n首先第一步自然是Tokenize Prompts和Response：\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\nprompt_tokens = tokenizer(\n    prompt_strs,\n    add_special_tokens=False,\n    padding=False,\n    truncation=False,\n    return_attention_mask=False,\n)\n\noutput_tokens = tokenizer(\n    output_strs,\n    add_special_tokens=False,\n    padding=False,\n    truncation=False,\n    return_attention_mask=False,\n)\n\n注意! 在这里，我们并不会返回Tensor 返回的是List， 并且List里面每个元素的长度是不一样的，\n接下来， 我们把这两个List中的内容 concat 在一起，得到 [q, o]， 并且计算出我们的Response Mask\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\ninput_ids = []\nresponse_mask = []\n\nfor p_ids, o_ids in zip(prompt_tokens[\"input_ids\"], output_tokens[\"input_ids\"]):\n    combined_ids = p_ids + o_ids\n    input_ids.append(combined_ids)\n\n    mask = ([False] * len(p_ids)) + ([True] * len(o_ids))\n    response_mask.append(mask)\n\n到目前为止，input_ids 和 response_mask 里面的内容长度不一样长，所以我们要将它Pad 到同样的长度，这样才可以传入模型：\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\nMAX_LEN = max(len(ids) for ids in input_ids)\n# 151643 for Qwen/Qwen2.5-Math-1.5B\npad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n\ndef pad_to(x, value):\n    return x + [value] * (MAX_LEN - len(x))\n\nfull = torch.tensor([pad_to(x, pad_id) for x in input_ids], dtype=torch.long)\nresponse_mask = torch.tensor([pad_to(x, False) for x in response_mask], dtype=torch.bool)\n\n接下来就构建我们的input和labels， Labels中记录的是inputs中的下一个：\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\ninput_ids = full[:, :-1].contiguous()\nlabels = full[:, 1:].contiguous()\nresponse_mask = response_mask[:, 1:].contiguous()\n\n\nreturn {\n        \"input_ids\": input_ids,\n        \"labels\": labels,\n        \"response_mask\": response_mask,\n    }\n\n需要注意的是， Response Mask中是Labels中的mask，而不是inputs_ids中的mask。\n\n\nTL;DR: Tokenization & Prompts\n\n\n在这里函数中，我们主要做两件事情：\n\nTokenize Prompt 和 Output，这里的Prompt是我们添加了Template之后\nConcat Tokenized Prompt， Output一起，并且生成Response Mask\nPadding到相同的长度\n将Concat之后的内容移一位，得到inputs和labels"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#per-token-entropy",
    "href": "posts/CS336/Ass05/ass05.html#per-token-entropy",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.2 Per Token Entropy",
    "text": "3.2 Per Token Entropy\n对于每一个位置\\(t\\), 模型会给出一下个token的分布，也就是 \\(p_{t}(x) = \\text{softmax}(\\text{logits}_{t})\\), Entropy定义为：\n\\[\nH(p) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log p(x)\n\\tag{2}\\]\n\nEntropy 高： 分布更“平”，模型不那么确定（探索更强）（对于Category Distribution，\\(p(x) = \\frac{1}{| x| }\\) 有最高的Entropy\nEntropy 低：分布更“尖”，模型更确定（可能变得过度自信、模式坍缩（model collapse）\n\n在 RL 里如果你看到 entropy 很快掉到很低，常见含义是：\n\n策略变得太确定（exploration 变差）\n训练可能开始“钻 reward 漏洞”或输出单一模板\n学习可能不稳定（尤其和 KL/clip 配合不当时）\n\n计算entropy也很简单的，\n\\[\n\\begin{split}\n\\ell &= \\text{logits} \\\\\n\\log p &= \\log\\text{softmax}(\\ell) \\\\\np &= \\exp(\\log p) \\\\\nH(p) &= - \\sum_{x \\in \\mathcal{X}} p(x) \\log p(x)\n\\end{split}\n\\tag{3}\\]\n用上面的公式，我们可以定义一个函数 compute_entropy 来计算entropy：\ndef compute_entropy(logits: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the entropy of the probability distribution defined by the logits.\n    \"\"\"\n    log_probs = F.log_softmax(logits, dim=-1)\n    probs = torch.exp(log_probs)\n    entropy = -torch.sum(probs * log_probs, dim=-1)\n\n    return entropy"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#getting-log-probs-from-model",
    "href": "posts/CS336/Ass05/ass05.html#getting-log-probs-from-model",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.3 Getting Log Probs from Model",
    "text": "3.3 Getting Log Probs from Model\n接下来，我们来定义另一个Helper Function get_response_log_probs 它的作用是：把“模型对每个位置真实 token 的条件概率”算出来（以 log 形式），并按 token 粒度返回 可能现在理解这个有点困难，在之后SFT 和 RL 的算法中，我们会具体讲解一下的。在这里，我们先定义一下这个函数 我们知道，SFT 的Loss Equation 1 中需要用到 \\(\\log p_\\theta(R_t \\mid P, R_{&lt;t})\\)， 也就是模型在位置\\(t\\)上， 对真实token \\(R_t\\)的log prob。 因此，我们需要定义一个函数来计算这个值：\ndef get_response_log_probs(\n    model, input_ids: torch.Tensor, labels: torch.Tensor, return_token_entropy: bool = False\n) -&gt; dict[str, torch.Tensor]:\n对于模型输出 \\(f_{\\theta}(x)\\), 其输出的是Logits，也就是没有normalized的distribution，我们第一步自然是利用softmax来将其变为分布， 之后我们再根据我们需要的label，作为索引，来提取出我们需要的log-probs\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\ndef get_response_log_probs(\n    model, \n    input_ids: torch.Tensor, # （B, T）\n    labels: torch.Tensor, # （B, T）\n    return_token_entropy: bool = False\n) -&gt; dict[str, torch.Tensor]:\n    logits = model(input_ids=input_ids).logits # (B, T, V)\n\n    logp = F.log_softmax(logits, dim=-1)\n    log_probs = logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1) \n\n    res = {\n        \"log_probs\": log_probs,\n    }\n    if return_token_entropy:\n        entropy = compute_entropy(logits)\n        res[\"token_entropy\"] = entropy\n    return res\n\n这个函数最关键的部分就是第10行， 通过 gather 函数，我们可以根据labels，来提取出我们需要的log_probs。logp的shape是(B, T, V)， 其中B是Batch Size， T是Sequence Length， V是Vocabulary Size，而labels的shape是(B, T)， 因此通过 gather 函数，我们可以得到每个位置上，真实token的log_probs， 最终得到的log_probs shape是 (B, T)， 也就是我们想要的结果。\n\n\nWARNING: About Response Mask\n\n\n我们在这一步，并没用用到Response Mask，也就是说，这个函数会返回所有位置的Log Probs， 包括Prompt部分和Pad部分。我们需要在之后的Loss计算中 masked_normalize，使用Response Mask来Mask掉Prompt和Pad部分。\n\n\n因此，在SFT 中，Log Probs 我们通过 log_probs.sum(dim=-1) 可以计算出Loss，当然，在做这个计算之前，我们还需要Mask掉Prompts，我们接下去来实现它："
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#masked-normalize",
    "href": "posts/CS336/Ass05/ass05.html#masked-normalize",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.4 Masked Normalize",
    "text": "3.4 Masked Normalize\n接下来，我们来实现 masked_normalize 函数， 这个函数的作用是：根据Mask，对Tensor进行Mask，并且进行Normalize\n\n\nassignment5-alignment/cs336_alignment/algs/utils.py\n\ndef masked_normalize(\n    tensor: torch.Tensor, # (B, T)\n    mask: torch.Tensor,  # (B, T)\n    normalize_constant: float = 1.0, \n    dim: int | None = None\n) -&gt; torch.Tensor:\n    assert tensor.shape == mask.shape, \"Tensor and mask must have the same shape\"\n\n    masked_f = mask.type_as(tensor)\n    masked_tensor = tensor * masked_f\n    masked_sum = torch.sum(masked_tensor, dim=dim) if dim is not None else torch.sum(masked_tensor)\n\n    return masked_sum / normalize_constant\n\n这个函数很简单，首先我们会根据Mask来Mask掉Tensor中不需要的部分， 之后我们会对剩下的部分进行Sum， 最后我们会根据normalize_constant来进行Normalize。 传入masked_normalize的tensor，一般是Log Probs， 这样我们就可以计算出Masked Log Probs的和, 也就是我们想要的Loss。其中我认为一个很巧妙的设计是 normalize_constant 和 dim 参数， 通过这两个参数，我们可以灵活的控制我们想要的Normalize方式， 以及想要Sum的维度， 比如：\n\n如果我们想要计算Batch中所有Token的Loss，我们可以传入 dim=None， 并且 normalize_constant = mask.sum()， 这样我们就可以得到Batch中所有Token的平均Loss。 (Token Level Loss)\n如果我们想要计算Batch中每个Sequence的Loss，我们可以传入 dim=-1， 并且 normalize_constant = mask.sum(dim = -1)， 这样我们就可以得到Batch中每个Sequence的Loss。 (Sequence Level Loss)"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#sft-micro-batch-training-step",
    "href": "posts/CS336/Ass05/ass05.html#sft-micro-batch-training-step",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.5 SFT Micro-batch Training Step",
    "text": "3.5 SFT Micro-batch Training Step\n有了这些Helper Functions，我们可以来实现SFT的训练, 由于Qwen2.5-Math-1.5B的模型比较大， 我们不能完全实现Batch Size较大的，因此，我们需要通过Gradient Accumulation的技术，来使得训练变得可能，我们先来定义一小步：\n\n\nassignment5-alignment/cs336_alignment/algs/sft.py\n\ndef sft_microbatch_train_step(\n    policy_log_probs: torch.Tensor,\n    response_mask: torch.Tensor,\n    gradient_accumulation_steps: int,\n    normalize_constant: float = 1.0,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n\n其实很简单，这个函数就做两件事情：\n\n计算Loss\nBackward 计算Gradient\n\n\\[\n\\mathcal{L}_{\\text{SFT}}^{\\text{batch-sum}}\n=\n-\\sum_{i=1}^{B}\\sum_{t=1}^{T}\nm^{(i)}_t\\;\n\\log p_\\theta\\!\\big(y^{(i)}_t \\mid x^{(i)}_{&lt;t}\\big)\n\\tag{4}\\]\n其中 \\(m_{t}^{(i)}\\) 表示的是Mask值\n\n\nassignment5-alignment/cs336_alignment/algs/sft.py\n\nloss_unscaled = masked_normalize(\n    policy_log_probs,\n    response_mask,\n    normalize_constant=normalize_constant,\n    dim=-1,\n) # 我们可以看到， dim=-1， 而且 normalize_constant = 1.0， 也就是Sequence Level Loss\n\nloss_unscaled = -loss_unscaled.mean()\nloss_scaled = loss_unscaled / gradient_accumulation_steps \nloss_scaled.backward()\n\nmetadata = {\n    \"loss_unscaled\": loss_unscaled.detach(),\n}\nreturn loss_scaled.detach(), metadata\n\nGradient Accumulation的实现也很简单， 我们只需要在计算Loss之后， 除以 gradient_accumulation_steps 即可。 这样我们就可以在之后的SFT Trainer中， 实现Gradient Accumulation的功能了。"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#sft-trainer",
    "href": "posts/CS336/Ass05/ass05.html#sft-trainer",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.6 SFT Trainer",
    "text": "3.6 SFT Trainer\n有了这些Helper Functions，我们就可以定义我们的SFT Trainer了。相当的直观\nclass SFTTrainer:\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        train_config: SFTTrainingConfig,\n        device: torch.device,\n        dataset_dir_base: str = \"./data/pre-processed\",\n    ): \n        ...\n    \n    def train_step(\n        self,\n    ) -&gt; tuple[float, float]: \n        ... \n        \n    def train(self, vllm=None):\n        ...\n在这里就不过多的赘述了，有需要的同学请自行查看代码 assignment5-alignment/cs336_alignment/algs/sft.py 以及它的训练代码 assignment5-alignment/train_sft.py"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#sft-experiment",
    "href": "posts/CS336/Ass05/ass05.html#sft-experiment",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "3.7 SFT Experiment",
    "text": "3.7 SFT Experiment\n接下来我们来看看SFT 的训练结果：\n\n\n\n\n\n\n\n\n\n\n\n(a) Accuracy\n\n\n\n\n\n\n\n\n\n\n\n(b) Format Reward\n\n\n\n\n\n\n\nFigure 1: Result of SFT on Qwen2.5-Math-1.5B\n\n\n\n可以看到，经过SFT训练之后，模型在MATH, GSM8K数据集上的表现都有了显著的提升，尤其是在Format Reward上，提升非常明显，这也符合我们之前提到的SFT的作用， 让模型变得更像助理，更会按指令做事。并且Accuracy也有了显著的提升， 这也说明SFT在提升模型能力方面是有效的。"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#ei-experiment",
    "href": "posts/CS336/Ass05/ass05.html#ei-experiment",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "4.1 EI Experiment",
    "text": "4.1 EI Experiment\n接下来我们来看看EI 的训练结果：\n\n\n\n\n\n\n\n\n\n\n\n(a) GSM8K Accuracy\n\n\n\n\n\n\n\n\n\n\n\n(b) Format Reward\n\n\n\n\n\n\n\nFigure 2: Result of EI on Qwen2.5-Math-1.5B on MATH and GSM8K Datasets\n\n\n\n可以看到，经过EI训练之后，模型在MATH, GSM8K数据集上的表现都有了显著的提升，尤其是在Accuracy上，提升非常明显，这也符合我们之前提到的EI的作用，通过采样更多的Prompt-Response Pairs，来提升模型的能力。并且Format Reward也有了显著的提升， 这也说明EI在提升模型能力方面是有效的。"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#grpo-experiment",
    "href": "posts/CS336/Ass05/ass05.html#grpo-experiment",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "5.1 GRPO Experiment",
    "text": "5.1 GRPO Experiment\n接下来我们来看看GRPO 的训练结果：\n\n\n\n\n\n\n\n\n\n\n\n(a) GRPO Accuracy\n\n\n\n\n\n\n\n\n\n\n\n(b) GSM8K Token Entropy\n\n\n\n\n\n\n\nFigure 3: Result of GRPO on Qwen2.5-Math-1.5B on GSM8K Dataset"
  },
  {
    "objectID": "posts/CS336/Ass05/ass05.html#other-experiments",
    "href": "posts/CS336/Ass05/ass05.html#other-experiments",
    "title": "Assignment 05: LLM Alignment: SFT, Expert Iteration, GRPO & DPO",
    "section": "5.2 Other Experiments",
    "text": "5.2 Other Experiments\n在Assignment中， 我还实现了一些其他的实验， 比如不同的Reward Function， 不同的Loss Type， 以及不同的Group Size。由于时间的关系，我并没有做这些实验，不过在代码中，可以很容易的实现这些实验。\n\n5.2.1 Learning Rate Tuning\n首先第一个实验就是学习率的调节， 由于GRPO的训练比较不稳定， 因此学习率的选择非常重要。 我尝试了不同的学习率， 发现0.0001是一个比较合适的选择， 过高的学习率会导致训练不稳定， 过低的学习率会导致训练收敛慢。\n\n\n5.2.2 Effect of Baseline\nBaseline的作用是减少训练的方差， 因此我尝试了不同的Baseline， 发现使用Baseline可以显著提升训练的稳定性， 并且可以提升模型的性能。\n\n\n5.2.3 Length Normalization\n在计算Log Probs的时候，根据不同的Length Normalization方式， 也会对训练产生影响。 我尝试了不同的Normalization方式， 发现使用Token Level Normalization可以提升模型的性能。\n\n\n5.2.4 Normalization with group std\n在计算Advantage的时候， 我尝试了是否使用Group Std来进行Normalization，\n\n\n5.2.5 Off-Policy vs. On-Policy\n要想把GRPO 训练好， 还有一个很重要的点就是Off-Policy 和 On-Policy的选择。 在GRPO中， 我们使用的是On-Policy的方式， 也就是使用Old Policy来采样数据， 然后使用New Policy来进行优化。 这种方式可以提升训练的稳定性， 并且可以提升模型的性能。要想把它改成Off-Policy也是可以的， 但是需要注意一些细节， 比如Importance Sampling等。\n\n\n5.2.6 Off Policy Clipping\n\n\n5.2.7 Different Prompts\n当然，我们还可以尝试不同的Prompts， 以便提升模型的性能。 不同的Prompt会对模型的性能产生影响， 因此选择合适的Prompt也是很重要的。当Prompts改变时，对应的Reward Function也需要相应的调整， 以便更好的适应新的Prompt。"
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html",
    "href": "posts/CS336/Lecture15/lec15.html",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "",
    "text": "在之前的课程中，我们通过训练（Pre-Training）获得了一个可以自动补全的LLM。但是，这个显然和我们现在使用的ChatGPT，Gemini有很大的区别，如何从这个GPT变成ChatGPT，将是我们接下去要学习的内容。也就是所谓的Post-Training，通过Post-Training，模型可以输出制定的内容，并且变得更加安全。在这节Lecture中，我们首先会学习：",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#dataset",
    "href": "posts/CS336/Lecture15/lec15.html#dataset",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "1.1 Dataset",
    "text": "1.1 Dataset\nSFT（监督微调）阶段用的数据量通常 远小于预训练，但它对模型行为的影响却极大，所以：\n\n数据里的“细节”会被模型强烈放大：风格、长度、格式、口吻、是否爱列点、是否爱加引用、是否爱 emoji……都会被学成“默认行为”。\nSFT 更擅长教会模型输出的“类型签名”（type signature）：像不像聊天、是不是有结构、有没有礼貌、会不会拒绝。\n但 SFT 不一定可靠地教会“新知识”，甚至会引入捷径行为（比如为了符合“专家答案的形式”，去编造引用/事实）。\n\n接下来我们来看看几个SFT数据的例子：\n\n1.1.1 FLAN\nFLAN(Longpre et al. 2023) 数据是把很多 NLP 任务用“自然语言指令模板”表达出来，然后把模型在这些任务上做 instruction tuning（指令微调），从而提升零样本泛化 FLAN 系列的关键不是原始任务，而是： - 把每个任务写成若干种 自然语言模板（instruction + input + output） - 模型训练时看到的是“像聊天指令一样的文本”，但背后很多是分类/抽取/QA/生成等传统任务\n论文把它称为 “tasks formatted with instructions” 的 instruction tuning\n\n\n\n\n\n\nFigure 2: FLAN 数据示例：把分类任务写成“指令 + 输入 + 输出”的形式\n\n\n\n\n\n1.1.2 Alpaca\nAlpaca 是斯坦福 CRFM / Tatsu-lab 在 2023 年提出的一个可复现路线：\n用 LLaMA-7B 做基座，拿一份由更强模型生成的指令跟随数据（52K）做 SFT，从而得到一个“像 ChatGPT 一样更会听指令”的模型。\n它的数据类似于：\n\n\n\n\n\n\nFigure 3: Alpaca 数据示例：给定指令，生成对应回答\n\n\n\n下面是Alpaca的Prompt Template\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n通过这种方式，模型学会了“看到 instruction + input 后，应该生成什么样的 response”。\nAlpaca 的数据生成基本沿着 Self-Instruct 的思路走：\n\n起点：175 条人工写的 seed instruction-output pairs（来自 Self-Instruct 的 seed set）\n\n用 text-davinci-003（当时非常强的 teacher）：\n\n生成更多指令（用 seed 做 in-context 示例，让 teacher 扩写/变换出新 instruction）\n再让 teacher 为这些指令生成回答，得到“instruction-following demonstrations”\n最终形成大约 52K 条数据。\n\n\n所以 Alpaca 的本质是：用强模型当“数据工厂”，低成本造出大批 instruction→response 的 SFT 样本。\n\n\n\n\n\n\nFigure 4: Alpaca 数据生成流程示意：用强模型（text-davinci-003）基于少量人工示范，自动生成大规模的指令-回答对，用于微调基础模型（LLaMA-7B）。这种方法显著降低了高质量 SFT 数据的获取成本。Image source Self-Instruct: Aligning LM with Self Generated Instructions\n\n\n\n\n\n1.1.3 OpenAssistant\nOpenAssistant Conversations (OASST1)(Köpf et al. 2023) 是 LAION 组织的全球众包项目产出的一个 “助手风格（assistant-style）对话语料”，目标是把对齐（SFT / RLHF）研究“民主化”：把原本经常被大厂私有化的高质量偏好/对话数据开源出来。它包含 161,443 条消息、35 种语言、超过 10,000 棵完整标注的对话树，并附带大量质量评分。\n简要来说，整个数据集由一系列对话树（Conversation Tree, CT）组成。每一棵树的根节点表示一个初始提示（prompt），由“prompter”角色给出；在对话中只区分两种角色：prompter（提问方）和 assistant（回答方），而“user”这个词仅用来指参与数据标注或贡献内容的人类，以避免角色概念混淆。需要注意的是，这两种角色在原则上既可以由人类完成，也可以由模型生成。\n在对话树中： - 每个节点代表一条书面消息，并明确标注其角色（prompter 或 assistant）。 - 每个节点可以有多个子节点，且子节点的角色一定与父节点相反，表示同一轮对话下的不同可能回复。 - 从根节点到树中任意节点的一条路径称为一个 thread，它对应一段合法的完整对话，体现提问方与助手轮流发言的过程。 - 每个节点都会附带额外标注信息，例如人工标签、元数据（采集时间、语言等）。 - assistant 节点还包含排序信息（rank），用于表示在同一父 prompt 下，多条候选回复之间的人类偏好顺序，这是后续偏好学习和奖励建模的重要信号。\n整体上，这种对话树结构不仅能表示多轮对话，还能自然地支持一问多答 + 人类偏好排序，非常适合用于指令微调、奖励模型训练以及对齐研究。\n下图是OpenAssistant数据集的一个对话树示例：\n\n\n\n\n\n\nFigure 5: OpenAssistant 数据集中的一个对话树示例，展示了从初始提示（prompt）到多轮交互的完整对话结构。每个节点代表一条消息，并标注了角色（prompter 或 assistant）及其对应的回复选项和偏好排序信息。\n\n\n\n\n\n1.1.4 Self-Annotated Dataset\n在课堂上，还一起Label了几个Prompts， 但是从这些Prompts的例子中，明显可以看出有几个问题：\n\n质量方差极大（high variance）: 同一个 prompt，有人认真写长文、有人一句话、有人直接套 ChatGPT 模板。SFT 会把这种风格差异当成“都对”的示范学进去，导致模型输出风格不稳定。\n“写长、写好”很难 → 数据会偏短或偏模板: 大多数人写不出持续高质量长回答；要么很短，要么用套话填充。模型学到的往往是“模板化结构”，不一定是更有用的内容。\n容易产生“风格&gt;正确性”的偏置（length/list bias: 人类写作天然倾向于列点、写得更长显得更“像答案”。模型学到的可能是“多写、列点、客气”这种类型签名，而不是“简洁且准确”。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#algorithm",
    "href": "posts/CS336/Lecture15/lec15.html#algorithm",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "1.2 Algorithm",
    "text": "1.2 Algorithm\n在了解了SFT的Dataset之后，我们可以训练模型了。其实SFT的算法很简单，与Pre-Training的Object一样，都是Next-Token-Prediction，其基本的代码框架是：token-level NLL）\n\\[\n\\underset{\\theta}{\\max} \\log p_{\\theta}(y | x)\n\\tag{1}\\]\n从代码来看，就是简单的几步：\n\n\nShow the code\nfor step in range(train_steps):\n    batch = next(train_dataloader)\n    \n    input_ids = batch['input_ids']\n    labels = batch['labels']\n    response_mask = batch['response_mask']\n    \n    output = model(input_ids) \n    \n    loss = loss_fn(output, input_ids, response_mask)\n    loss.backward()\n    \n    optimizer.step()\n\n\n我们可以看到，基本上与Pre-Training的Loss 类似，只不过就是多了一个Response Mask.\n\n\nQuestion：为什么要 mask prompt？\n\n\n因为我们希望模型学的是：“看到 prompt 后，应该怎么答”, 而不是：“把 prompt 也背下来复现一遍”。 通过mask掉 prompt 部分的 loss，我们只让模型在 response 部分学习预测， 并且避免模型过拟合 prompt 内容。\n\n\n\n1.2.1 Mid-Training\n既然SFT和Pre-Training的训练目标一致，那么我们可不可以将SFT的训练混合到Pre-Training当中呢？答案是可以的，这也就是所谓的Mid-Training/Two-Phase Training\n在这个训练过程中，主要做3件事：\n\n先正常做预训练（Pre-train on web/pretraining data） 在Common Crawl / books / code / papers 等大规模语料中训练，目标是 next-token prediction。\n在预训练的后半段，把 instruction-tuning 数据混进去（Mix in instruction-tuning data into pre-training）关键点是：\n\n不是等预训练结束再单独 SFT\n当模型已经有一定能力、学习率开始下降（进入 decay / anneal 阶段）时， 继续用“预训练数据”保持通用能力\n同时加大“高质量/指令/对话/推理”数据的比例，让模型在还处在“预训练优化状态”时就逐渐学会指令跟随的分布\n这一步本质上：还是 next-token loss，只是数据分布变了。\n\n最后再做一个很短的真正 instruction tuning：由于第二步已经把“指令分布”深度融进模型了，最后的纯 SFT 往往可以更短、更像“校准/收尾”。\n\n通过这个做法的好处就是：让模型能在不严重灾难性遗忘（catastrophic forgetting）的情况下，把 instruction tuning 扩大规模.\n我们来对比一下传统 SFT 和 Mid-Training 的区别：\n\n传统做法：先预训练完，再 SFT ：SFT 数据量虽然小，但梯度信号很集中、风格强，会把模型“拉”到很窄的分布上。\n如果你 SFT 过拟合（学习率大/步数多/数据分布太偏），就容易：\n\n通用能力下降（遗忘预训练里学到的广泛知识/语言能力）\n过拟合某种风格（更啰嗦、更爱列点、更爱模板化）\n\nMid-Training：预训练后期逐步加指令数据 ：因为预训练数据还在、学习率也在 decay，模型被“温和地”引导到指令分布，\n\n不会一下子被 SFT 的强分布冲刷。\n\n同时可以把 instruction 数据规模做大（甚至到“像预训练一样大”），而不用担心彻底把模型训偏。\n\n\n通过这种方法，我们只需要在训练的时候，修改不同阶段的数据比例即可，比如：\n\n训练进度前 70%：几乎全是预训练数据\n后 30%（学习率开始衰减）：逐步提高 instruction/高质量数据占比：例如从 0% → 10% → 30% → 50%\n训练末尾：再做少量纯 SFT（更像“对齐收尾”）\n\n\n\n\n\n\n\nFigure 6: 这张图说明很多模型会把“指令微调数据”提前混进预训练的后期（decay/mid-training），让数据配方从“纯网页预训练”（左图）逐步变成“预训练语料 + 各类SFT/高质量指令数据的混合”（右图），从而更规模化地获得指令跟随能力并减少灾难性遗忘。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#rlhf-data",
    "href": "posts/CS336/Lecture15/lec15.html#rlhf-data",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.1 RLHF Data",
    "text": "2.1 RLHF Data\n课上提到 InstructGPT 的标注准则很经典：helpful、truthful、harmless。\n实际标注界面通常就是：\n\nA vs B 哪个更好？（或 4 选 1 / ties 等）\n有时还会分别打分：事实性、遵循指令、安全性、写作质量等\n\n不过需要注意的是：这不是“对错题”，很多任务是开放式偏好。\n有了这些数据之后，我们要训练一个Reward Model \\(r_{\\phi}(x, y)\\). 每个回答都有一个隐藏分数，标注者更常选分高的。\n用一个 logistic/softmax 形式拟合：\n\\[\nP(y^+ \\succ y^- \\mid x) = \\sigma\\big(r_\\phi(x,y^+) - r_\\phi(x,y^-)\\big)\n\\tag{3}\\]\n于是你的 RLHF 数据就变成 reward model 的监督数据:\n\\[\n\\underset{\\phi}{\\max} \\sum_{(x, y^+, y^-) \\in D} \\log \\sigma\\big(r_\\phi(x,y^+) - r_\\phi(x,y^-)\\big)\n\\tag{4}\\]\n训练完成后，就有了一个 reward model，可以给任意 (x, y) 对打分： \\(r_{\\phi}(x, y)\\)。\n当然，这个流程看似简单，实际上还是有很多考量的：\n\n数据质量：标注者培训、审核、分布覆盖、偏见控制等\n数据多样性：prompt 类型、回答风格、难度等\n模型架构：reward model 通常是一个小型 LM，或者在 LM 上加个头",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#rlhf-algorithms",
    "href": "posts/CS336/Lecture15/lec15.html#rlhf-algorithms",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.2 RLHF Algorithms",
    "text": "2.2 RLHF Algorithms\n有了Pair-Wise 的Dataset和Reward Model之后，我们可以开始训练的我们的模型了。在InstructGPT(Ouyang et al. 2022) 中，主要用的是PPO的算法。 接下来看看PPO的具体内容。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#ppo",
    "href": "posts/CS336/Lecture15/lec15.html#ppo",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.3 PPO",
    "text": "2.3 PPO\n回顾一下，看一下我们现在手头上有些什么东西：\n\n一个初始化的策略模型（通常是 SFT 模型）\\(\\pi_{\\text{ref}}(y|x)\\)（作为参考策略/基线）\n一个奖励函数/奖励模型 \\(r_{\\phi}(x,y)\\)（由偏好数据训练出来）\n要训练的策略 \\(\\pi_\\theta(y|x)\\), (由LLM初始化)\n\nRLHF-PPO 的核心目标就是：\n\\[\n\\underset{\\theta}{\\max}  \\mathcal{J}(\\theta) = \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\big[r_\\phi(x,y)\\big] \\ -\\ \\beta \\, \\mathrm{KL}\\big(\\pi_\\theta(\\cdot|x)\\ \\|\\ \\pi_{\\text{ref}}(\\cdot|x)\\big)\n\\tag{5}\\]\n通过这个目标函数，我们希望模型： 回答更“高奖励”，但别偏离 SFT 太远（KL 约束防止跑飞、学会作弊或变得怪异/不安全）。\n\n2.3.1 REINFORCE\n在Neural Network中，我们优化目标通常用梯度下降法，因此我们需要计算上面目标的梯度。对于Deep RL也不例外，我们需要计算出这个Object Function (Equation 5) 的梯度:\n\\[\n\\nabla_\\theta \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\big[r_\\phi(x,y)\\big] = \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y) \\, \\nabla_\\theta \\log \\pi_\\theta(y|x)\\right]\n\\tag{6}\\]\n通过这个方法，我们可以计算出梯度，然后用SGD来更新模型参数，这也就是REINFORCE算法。 在实际操作中，我们可以通过Sampling的方式来估计上面的期望：\n\\[\n\\nabla_\\theta \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\big[r_\\phi(x,y)\\big] \\approx \\frac{1}{N} \\sum_{i=1}^N r_\\phi(x,y_i) \\, \\nabla_\\theta \\log \\pi_\\theta(y_i|x), \\quad y_i \\sim \\pi_\\theta(\\cdot|x)\n\\tag{7}\\]\n但是REINFORCE有两个主要问题：\n\nHigh variance：奖励信号往往很稀疏且噪声大，导致梯度估计方差很高，训练不稳定。\n单步更新：REINFORCE 每次更新都基于当前策略采样的数据，不能多步利用旧数据，效率低。\n\n接下来，我们看看如何解决这两个问题，并且逐步引出PPO算法。\n\n\n2.3.2 Variance Reduction with Advantage Function\n我们先来看一下为什么会有High Variance的问题。\n假设我们把回答 \\(y\\) 看成一个序列的动作 \\((a_1, a_2, \\ldots, a_T)\\)，每个动作对应生成一个 token。 那么根据链式法则，回答的概率可以写成： \\[\n\\pi_\\theta(y|x) = \\prod_{t=1}^T \\pi_\\theta(a_t | s_t)\n\\tag{8}\\]\n其中 \\(s_t\\) 是生成第 \\(t\\) 个 token 时的状态（包括 prompt 和前面生成的 tokens）。 根据 REINFORCE 的梯度公式 Equation 6，我们可以把梯度展开成对每个时间步的贡献求和： \\[\n\\nabla_\\theta \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\big[r_\\phi(x,y)\\big] = \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\left[r_\\phi(x,y) \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right]\n\\tag{9}\\]\n这里的关键问题是：奖励 \\(r_\\phi(x,y)\\) 是对整个序列 \\(y\\) 的评价，但我们把它直接用在每个时间步的梯度上，导致每个时间步的梯度估计都包含了整个序列的噪声，方差很大。\n为了降低梯度估计的方差，我们引入优势函数（Advantage Function） \\(A_t\\)，它衡量在状态 \\(s_t\\) 下采取动作 \\(a_t\\) 相对于平均水平的好坏：\n\\[\nA_t = Q(s_t, a_t) - V(s_t)\n\\tag{10}\\]\n其中 \\(Q(s_t, a_t)\\) 是在状态 \\(s_t\\) 下采取动作 \\(a_t\\) 后的预期回报，\\(V(s_t)\\) 是状态 \\(s_t\\) 的平均回报。 通过使用优势函数，我们可以把梯度公式改写为： \\[\n\\nabla_\\theta \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\big[r_\\phi(x,y)\\big] = \\mathbb{E}_{y\\sim \\pi_\\theta(\\cdot|x)}\\left[\\sum_{t=1}^T A_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right]\n\\tag{11}\\]\n这样，每个时间步的梯度只受到该时间步优势 \\(A_t\\) 的影响，减少了整个序列奖励带来的噪声，从而降低了方差。\n\n\n2.3.3 Off-Policy Updates\nREINFORCE 的另一个问题是它是on-policy的：每次更新都需要用当前策略采样新数据，不能多次利用旧数据，效率低。 为了解决这个问题，我们可以采用离线数据重用（off-policy updates）的思想。具体来说，我们可以保存之前采样的数据（prompts 和生成的回答），并在多次迭代中重复使用这些数据进行更新。\n但是直接使用旧数据会引入偏差，因为这些数据是根据旧策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 采样的，而我们现在要更新的是新策略 \\(\\pi_\\theta\\)。 为了纠正这种偏差，我们可以使用重要性采样（importance sampling），通过计算每个回答在新旧策略下的概率比来调整梯度估计：\n\\[\n\\rho(y) = \\frac{\\pi_\\theta(y|x)}{\\pi_{\\theta_{\\text{old}}}(y|x)}\n\\tag{12}\\]\n然后，我们可以把梯度公式改写为： \\[\n\\nabla_\\theta \\mathbb{E}_{y\\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|x)}\\big[r_\\phi(x,y)\\big] = \\mathbb{E}_{y\\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|x)}\\left[\\rho(y) \\sum_{t=1}^T A_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right]\n\\tag{13}\\]\n这样，我们就可以多次利用旧数据进行更新，提高数据效率。\n\n\n2.3.4 Proximal Policy Optimization (PPO)\n结合上面的两个改进，我们就引出了PPO（Proximal Policy Optimization）算法。PPO 通过限制新旧策略的变化幅度，进一步稳定训练过程。具体来说，PPO 使用一个裁剪目标（clipped objective），防止策略更新过大：\n\\[\nL^{\\text{clip}}(\\theta) = \\mathbb{E}_{y\\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|x)}\\left[\\min\\left(\\rho(y) A, \\text{clip}(\\rho(y), 1-\\epsilon, 1+\\epsilon) A\\right)\\right]\n\\tag{14}\\] 其中 \\(\\epsilon\\) 是一个小的超参数，控制裁剪范围(通常是0.1到0.3)。 通过这个裁剪目标，PPO 保证了新策略不会偏离旧策略太远，从而避免了训练不稳定的问题。\n我们来看一下PPO的整体训练流程。\n\nRollout（采样回答）:对一批 prompts \\(x\\)，用当前策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 生成回答 \\(y\\)。同时保存每个生成 token 的：\n\nlogprob：\\(\\log \\pi_{\\theta_{\\text{old}}}(a_t|s_t)\\)\n\n算奖励（reward）:用奖励模型 \\(r_{\\phi}(x,y)\\) 给整段回答一个标量分数。再加上 KL 惩罚，得到最终奖励信号:\n\nKL 惩罚通常有两种做法：\n\n显式 KL penalty：把 \\(-\\beta \\, \\mathrm{KL}(\\pi_\\theta(\\cdot|x) \\| \\pi_{\\text{ref}}(\\cdot|x))\\) 加进 reward\n或在 loss 里单独加 KL 项（类似 InstructGPT）\n\n很多实现把 token-level 的 KL 变成一个 shaping reward:\n\\[\nr_t^{\\text{KL}} = -\\beta\\left(\\log \\pi_\\theta(a_t|s_t)-\\log \\pi_{\\text{ref}}(a_t|s_t)\\right)\n\\]\n然后把最终奖励分配到序列末端或做一些分摊。\n\n估计 Value + Advantage\n\n训练一个 value head \\(V_\\psi(s_t)\\) 预测“从当前前缀往后能拿到的回报”。\n用（GAE）等方法得到 \\(A_t\\)\n\\[\n\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) = \\sum_{l\\ge 0}(\\gamma\\lambda)^l \\delta_{t+l}\n\\tag{15}\\]\n4：PPO update（多 epoch、小步更新）\n对同一批 rollout 数据，做 K 个 epoch 的 minibatch 更新：\n\npolicy loss：−Lclip-L^{clip}−Lclip\nvalue loss：∥Vψ−R∥2|V_- R|^2∥Vψ​−R∥2\nentropy bonus：鼓励探索 (+αH)(+H)(+αH)\n（可选）KL 控制项\n\n总体 loss（常见形式）：\nL=Lpolicy+cvLvalue−ceH+cklKLL = L_{} + c_v L_{} - c_e H + c_{kl}L=Lpolicy​+cv​Lvalue​−ce​H+ckl​KL\nPPO 工程上复杂，主要因为：\n\non-policy：每轮都要采样新数据（rollouts 成本高）\n需要 value function：要训 value head，容易不稳\n需要 careful 的 KL 控制：不然要么跑飞、要么学不动\nsequence credit assignment：奖励常是序列级，怎么分到 token 上很敏感\n长度偏置/奖励 hacking：reward model 可能偏好长回答 → 策略学会“写长骗分”\n\n# PPO RLHF: one training iteration (one \"outer step\")\n# Assumes:\n#   policy: trainable LM πθ\n#   ref_policy: frozen LM πref (often SFT checkpoint)\n#   reward_model: rφ(x, y) -&gt; scalar reward per sequence\n#   value_head: Vψ(s_t) -&gt; scalar value per token/state (often a head on top of policy)\n#\n# Notation:\n#   B = batch size (number of prompts)\n#   T = max total tokens (prompt + generated)\n#   Tp = prompt length (varies per sample)\n#   Tr = response length (varies per sample)\n#\n# Key masks:\n#   response_mask[b,t] = 1 if token t is a generated response token (NOT prompt), else 0\n#   valid_mask[b,t] = 1 if token t exists (not padding), else 0\n#\n# IMPORTANT alignment:\n#   For causal LM, token-level logprob at position t corresponds to predicting token_ids[t]\n#   from prefix token_ids[:t]. Commonly computed with a 1-step shift.\n\ndef ppo_train_step(prompts):\n    # ------------------------------------------------------------\n    # 1) Rollout: sample responses from current policy (old policy snapshot)\n    # ------------------------------------------------------------\n    with no_grad():\n        policy.eval()\n\n        # Generate tokens (can be via vLLM or your sampler)\n        # returns:\n        #   token_ids: (B, T) padded\n        #   response_mask: (B, T) 1 for response tokens\n        #   valid_mask: (B, T) 1 for non-pad tokens\n        token_ids, response_mask, valid_mask = generate(policy, prompts)\n\n        # (Optional) store prompt lengths, response lengths, etc.\n        # prompt_mask = valid_mask & (~response_mask)\n\n    # Freeze a copy of current params as \"old\" logically.\n    # In practice, we keep old_logp computed here as constants.\n    # ------------------------------------------------------------\n    # 2) Compute old_logp and ref_logp for the generated response tokens\n    # ------------------------------------------------------------\n    with no_grad():\n        # old policy logprobs on the sampled trajectory\n        # logp_old: (B, T) where positions not scored can be 0\n        logp_old = token_logprobs(policy, token_ids)     # aligned to token_ids\n        logp_ref = token_logprobs(ref_policy, token_ids) # aligned to token_ids\n\n        # Only optimize on response tokens (typical RLHF)\n        # Keep only response positions; everything else masked out.\n        logp_old = logp_old * response_mask\n        logp_ref = logp_ref * response_mask\n\n    # ------------------------------------------------------------\n    # 3) Reward + KL shaping\n    # ------------------------------------------------------------\n    with no_grad():\n        # Sequence-level reward from reward model (scalar per sample)\n        # r_seq: (B,)\n        r_seq = reward_model(prompts, token_ids)  # evaluates (x, y)\n\n        # Token-level KL term (per token):\n        #   kl_t = logπθ(a_t|s_t) - logπref(a_t|s_t)\n        # For shaping, we usually use old policy logp here because rollout came from old policy.\n        # kl_tok: (B, T)\n        kl_tok = (logp_old - logp_ref)  # already masked to response tokens\n\n        # KL penalty as \"negative reward\" per token\n        # r_kl_tok: (B, T)\n        r_kl_tok = -beta * kl_tok\n\n        # Combine rewards into a token-level reward signal.\n        # Common simple choice: put the sequence reward at the final response token,\n        # plus KL penalty at each response token.\n        # r_tok: (B, T)\n        r_tok = zeros_like(kl_tok)                # (B, T)\n        last_resp_index = last_index(response_mask)  # (B,) gives t_end per sample\n        r_tok[range(B), last_resp_index] += r_seq  # terminal reward\n        r_tok += r_kl_tok                          # dense KL shaping\n\n        # Ensure padding doesn't contribute\n        r_tok = r_tok * valid_mask\n\n    # ------------------------------------------------------------\n    # 4) GAE: compute advantages A_t and returns R_t for response tokens\n    # ------------------------------------------------------------\n    with no_grad():\n        # Value predictions for each token/state\n        # v: (B, T)\n        v = value_head(policy, token_ids)  # or separate critic network\n        v = v * valid_mask\n\n        # Compute next-state values v_next (shifted)\n        v_next = shift_left(v)            # v_next[:, t] = v[:, t+1], last = 0\n        v_next = v_next * valid_mask\n\n        # TD residuals δ_t = r_t + γ v_{t+1} - v_t\n        # delta: (B, T)\n        delta = r_tok + gamma * v_next - v\n        delta = delta * response_mask     # only response tokens matter\n\n        # GAE recursion backwards over time for each sample\n        # adv: (B, T)\n        adv = zeros_like(delta)\n        gae = zeros(B)\n        for t in reversed(range(T)):\n            mask_t = response_mask[:, t]  # (B,)\n            # if mask_t=0, reset gae to 0 so prompt/pad doesn't leak\n            gae = delta[:, t] + gamma * lam * gae\n            gae = gae * mask_t\n            adv[:, t] = gae\n\n        # Returns (target for value): R_t = A_t + V_t\n        ret = adv + v\n        ret = ret * response_mask\n\n        # Normalize advantages over all response tokens in the batch (stabilizes PPO)\n        adv = masked_normalize(adv, response_mask)  # zero-mean, unit-std over masked positions\n\n    # ------------------------------------------------------------\n    # 5) PPO clipped loss (policy + value + entropy)\n    # ------------------------------------------------------------\n    policy.train()\n\n    # Recompute current policy logprobs for the same token_ids (now θ is trainable)\n    # logp_new: (B, T)\n    logp_new = token_logprobs(policy, token_ids)\n    logp_new = logp_new * response_mask\n\n    # Probability ratio ρ_t = exp(logp_new - logp_old)\n    # ratio: (B, T)\n    ratio = exp(logp_new - logp_old) * response_mask\n\n    # Clipped surrogate objective\n    # unclipped = ratio * adv\n    # clipped   = clip(ratio, 1-eps, 1+eps) * adv\n    unclipped = ratio * adv\n    clipped = clip(ratio, 1 - eps, 1 + eps) * adv\n\n    # Policy loss: negative because we maximize objective\n    # Take masked mean over response tokens\n    policy_loss = -masked_mean(min(unclipped, clipped), response_mask)\n\n    # Value loss: regress to ret (returns)\n    v_pred = value_head(policy, token_ids) * response_mask\n    value_loss = masked_mean((v_pred - ret) ** 2, response_mask)\n\n    # Entropy bonus (encourage exploration) on response tokens\n    # entropy_tok: (B, T)\n    entropy_tok = token_entropy(policy, token_ids) * response_mask\n    entropy_bonus = masked_mean(entropy_tok, response_mask)\n\n    # (Optional) explicit KL term vs ref using current logp_new\n    # Helps keep policy close even if clip isn't enough\n    kl_new = (logp_new - logp_ref) * response_mask\n    kl_mean = masked_mean(kl_new, response_mask)\n\n    total_loss = policy_loss + c_v * value_loss - c_ent * entropy_bonus + c_kl * kl_mean\n\n    optimizer.zero_grad()\n    total_loss.backward()\n    clip_grad_norm_(policy.parameters(), max_grad_norm)\n    optimizer.step()\n\n    # Return logs\n    return {\n        \"loss_total\": total_loss,\n        \"loss_policy\": policy_loss,\n        \"loss_value\": value_loss,\n        \"entropy\": entropy_bonus,\n        \"kl\": kl_mean,\n        \"reward_seq_mean\": mean(r_seq),\n    }",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#dpo",
    "href": "posts/CS336/Lecture15/lec15.html#dpo",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.4 DPO",
    "text": "2.4 DPO\n显然，PPO的算法，存在的主要一个缺陷就是所需的内存过多： 我们需要保存:\n\nPolicy: 和LM一样大的模型\nReference Policy: 和LM一样大的模型\nValue Model： 和LM一样大的模型\nReward Model： 和LM差不多大的模型\n\n并且，在训练过程中，还需要保存大量的中间激活（activations）用于反向传播（backpropagation）。\n这对于动辄几个B的LM模型来说，消耗是巨大的，因此，提出了DPO的算法。 DPO（Direct Preference Optimization）(Rafailov et al. 2024) 可以把“RLHF + PPO”那套 采样→训练reward→RL更新，简化成一个纯监督式的偏好学习：直接用 \\((x,y+,y^-)\\) 更新策略模型。一句话总结就是： 让模型对 preferred 回答的概率比 rejected 更大，同时用参考模型 πref_{}πref​ 约束别偏太远。\n\n\n\n\n\n\nFigure 8: 图中展示了 DPO 和 PPO 的对比，DPO 直接用偏好数据（chosen vs rejected）来训练策略 \\(\\pi_\\theta\\), 对“人类更喜欢的回答”给更高概率，对“不喜欢的回答”给更低概率.\n\n\n\n接下来，我们来具体看看DPO算法： 假设 policy 不是神经网络，而是任意分布（nonparametric）。 在这个假设下，这个优化问题有解析解：\nπr(y∣x)=1Z(x) πref(y∣x) exp⁡ ⁣(1βr(x,y))r(y|x)= {}(y|x) !(r(x,y))πr​(y∣x)=Z(x)1​ πref​(y∣x) exp(β1​r(x,y))\n这其实就是一个 Boltzmann / energy-based reweighting：\n\n参考分布 \\(\\pi_{\\text{ref}}\\) 提供“先验”\nreward 越高，exp⁡(r/β)(r/)exp(r/β) 越把概率往上推\nZ(x)Z(x)Z(x) 是归一化常数（partition function）",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#反解得到-implied-rewardreward-log-ratio差一个常数",
    "href": "posts/CS336/Lecture15/lec15.html#反解得到-implied-rewardreward-log-ratio差一个常数",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.5 反解”得到 implied reward：reward ≈ log-ratio（差一个常数）",
    "text": "2.5 反解”得到 implied reward：reward ≈ log-ratio（差一个常数）\n把上式取 log 并整理，得到图里最后一行：\nr(x,y)=βlog⁡πr(y∣x)πref(y∣x)+βlog⁡Z(x)r(x,y)= + Z(x)r(x,y)=βlogπref​(y∣x)πr​(y∣x)​+βlogZ(x)\n关键点：\n\nβlog⁡Z(x)Z(x)βlogZ(x) 只依赖 x，不依赖 y → 在“比较 y+y^+y+ vs y−y^-y−”时会相消\n所以在偏好学习里，你可以把 reward 的差写成：\n\nr(x,y+)−r(x,y−)=β(log⁡π(y+∣x)πref(y+∣x)−log⁡π(y−∣x)πref(y−∣x))r(x,y+)-r(x,y-) =( - )r(x,y+)−r(x,y−)=β(logπref​(y+∣x)π(y+∣x)​−logπref​(y−∣x)π(y−∣x)​)\n这一步就是 DPO 的核心：不显式训练 reward model，而是用 policy 的 logprob（相对 ref 的差）来“隐式表示 reward”。\nr(x,y+)−r(x,y−)=β[logπref​(y+∣x)π(y+∣x)​−logπref​(y−∣x)π(y−∣x)​]\n把上面差值写得更紧凑一点：\nΔθ(x)=(log⁡πθ(y+∣x)−log⁡πθ(y−∣x))−(log⁡πref(y+∣x)−log⁡πref(y−∣x))(x) =((y+|x)-(y^-|x)) -({}(y+|x)-_{}(y^-|x))Δθ​(x)=(logπθ​(y+∣x)−logπθ​(y−∣x))−(logπref​(y+∣x)−logπref​(y−∣x))\n于是\n\\[\nr(x,y^+) - r(x,y^-) = \\beta \\, \\Delta_\\theta(x)\n\\tag{16}\\]\n代回偏好似然：\nLDPO(θ)=−E(x,y+,y−)[log⁡σ(β Δθ(x))]{}() = -{(x,y+,y-)}LDPO​(θ)=−E(x,y+,y−)​[logσ(βΔθ​(x))]\n这就是 DPO。\n直觉解释：\n\n如果你的新策略 πθ​ 相比 ref 更偏向 chosen（Δθ​ 大），loss 小\n如果反而更偏向 rejected（Δθ&lt;0_&lt;0Δθ​&lt;0），loss 大，会被梯度推回去\n\n在 LLM 里 log⁡πθ(y∣x)_(y|x)logπθ​(y∣x) 通常是 response tokens 的 logprob 之和：\nlog⁡πθ(y∣x)=∑t∈responselog⁡πθ(yt∣x,y&lt;t)(y|x)={t } (y_t x, y{&lt;t})logπθ​(y∣x)=t∈response∑​logπθ​(yt​∣x,y&lt;t​)\n所以 DPO 训练一次 step 就是：\n\n对 batch 中每个样本，分别算：\n\nlogp_pos = sum_logp(policy, x, y_pos)\nlogp_neg = sum_logp(policy, x, y_neg)\nlogp_ref_pos = sum_logp(ref, x, y_pos)（no grad）\nlogp_ref_neg = sum_logp(ref, x, y_neg)（no grad）\n\ndelta = (logp_pos - logp_neg) - (logp_ref_pos - logp_ref_neg)\nloss = -log_sigmoid(beta * delta).mean()\n\nimport torch\nimport torch.nn.functional as F\n\ndef dpo_train_step(\n    policy,                 # trainable LM πθ\n    ref_policy,             # frozen LM πref (e.g., SFT checkpoint)\n    optimizer,\n    batch_pos_input_ids,    # (B, T) prompt+chosen padded\n    batch_pos_attn_mask,    # (B, T) bool/int\n    batch_pos_resp_mask,    # (B, T) bool: 1 only on response tokens\n    batch_neg_input_ids,    # (B, T) prompt+rejected padded\n    batch_neg_attn_mask,    # (B, T)\n    batch_neg_resp_mask,    # (B, T)\n    beta: float = 0.1,\n    max_grad_norm: float | None = 1.0,\n):\n    \"\"\"\n    DPO core update step (ONLY training part).\n    Assumes inputs are already tokenized + padded and include response masks.\n\n    DPO:\n      delta = (logπθ(y+|x)-logπθ(y-|x)) - (logπref(y+|x)-logπref(y-|x))\n      loss  = -E[ log σ(beta * delta) ]\n    \"\"\"\n\n    def seq_logprob(model, input_ids, attn_mask, resp_mask):\n        # logits: (B, T, V)\n        logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n\n        # causal shift: logits[:, t] predicts input_ids[:, t+1]\n        logits = logits[:, :-1, :]          # (B, T-1, V)\n        labels = input_ids[:, 1:]           # (B, T-1)\n\n        logp = F.log_softmax(logits, dim=-1)\n        tok_logp = logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # (B, T-1)\n\n        # align masks with shift\n        mask = (resp_mask[:, 1:] & attn_mask[:, 1:]).to(tok_logp.dtype)  # (B, T-1)\n\n        return (tok_logp * mask).sum(dim=-1)  # (B,)\n\n    # ----- policy logprobs -----\n    logp_pos = seq_logprob(policy, batch_pos_input_ids, batch_pos_attn_mask, batch_pos_resp_mask)\n    logp_neg = seq_logprob(policy, batch_neg_input_ids, batch_neg_attn_mask, batch_neg_resp_mask)\n\n    # ----- reference logprobs (no grad) -----\n    with torch.no_grad():\n        logp_ref_pos = seq_logprob(ref_policy, batch_pos_input_ids, batch_pos_attn_mask, batch_pos_resp_mask)\n        logp_ref_neg = seq_logprob(ref_policy, batch_neg_input_ids, batch_neg_attn_mask, batch_neg_resp_mask)\n\n    # ----- DPO loss -----\n    delta = (logp_pos - logp_neg) - (logp_ref_pos - logp_ref_neg)  # (B,)\n    loss = -F.logsigmoid(beta * delta).mean()\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n\n    if max_grad_norm is not None:\n        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)\n\n    optimizer.step()\n\n    return {\n        \"loss\": float(loss.detach().cpu()),\n        \"delta_mean\": float(delta.detach().mean().cpu()),\n        \"pref_acc\": float((delta.detach() &gt; 0).float().mean().cpu()),\n    }",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#others",
    "href": "posts/CS336/Lecture15/lec15.html#others",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.6 Others",
    "text": "2.6 Others\n在DPO提出之后，后续也有许多算法对其提出了改进，在这里介绍两种 ### SimPO DPO 的“参考模型项（ref）”可以不要 → 得到 SimPO (no ref) DPO/偏好学习很容易出现：长回答更容易赢\n因为 sequence logprob 是 token logprob 的“和”，长度不同会导致比较不公平。\n所以把\nlog⁡πθ(y∣x)=∑t∈ylog⁡pθ(yt∣⋅)(y|x)={ty}p_(y_t|)logπθ​(y∣x)=t∈y∑​logpθ​(yt​∣⋅)\n改成平均每 token 的 logprob：\n1∣y∣log⁡πθ(y∣x)_(y|x)∣y∣1​logπθ​(y∣x)\n图里蓝框就是这个：β/∣yw∣⋅log⁡πθ(yw∣x)/|y_w|(y_w|x)β/∣yw​∣⋅logπθ​(yw​∣x) 和 β/∣yl∣⋅log⁡πθ(yl∣x)/|y_l|(y_l|x)β/∣yl​∣⋅logπθ​(yl​∣x)。\nimPO 的 logit 里减了一个 γ：\nΔSimPO=β∣yw∣log⁡πθ(yw∣x)−β∣yl∣log⁡πθ(yl∣x)−γ{} = (y_w|x) - _(y_l|x) -​=∣yw​∣β​logπθ​(yw​∣x)−∣yl​∣β​logπθ​(yl​∣x)−γ\n直觉：你不是只要 ywy_wyw​ 比 yly_lyl​ 好一点点就行，而是希望它至少好过一个幅度（margin）。\nγ越大，训练越“严格”。\nimport torch\nimport torch.nn.functional as F\n\ndef simpo_step(\n    policy, optimizer,\n    pos_input_ids, pos_attn, pos_rmask,\n    neg_input_ids, neg_attn, neg_rmask,\n    beta: float = 0.1,\n    gamma: float = 0.0,\n):\n    def seq_logprob_and_len(model, input_ids, attn_mask, resp_mask):\n        logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n        logits = logits[:, :-1, :]\n        labels = input_ids[:, 1:]\n\n        logp = F.log_softmax(logits, dim=-1)\n        tok_logp = logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n\n        mask = (resp_mask[:, 1:] & attn_mask[:, 1:]).to(tok_logp.dtype)\n        seq_lp = (tok_logp * mask).sum(dim=-1)            # (B,)\n        resp_len = mask.sum(dim=-1).clamp_min(1.0)        # (B,)\n        return seq_lp, resp_len\n\n    lp_pos, len_pos = seq_logprob_and_len(policy, pos_input_ids, pos_attn, pos_rmask)\n    lp_neg, len_neg = seq_logprob_and_len(policy, neg_input_ids, neg_attn, neg_rmask)\n\n    # SimPO logit (no ref) + length normalization + margin gamma\n    delta = (beta * (lp_pos / len_pos) - beta * (lp_neg / len_neg) - gamma)\n\n    loss = -F.logsigmoid(delta).mean()\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    return loss\n\n2.6.1 Length Normalized DPO\nimport torch\nimport torch.nn.functional as F\n\ndef dpo_len_norm_step(\n    policy, ref_policy, optimizer,\n    pos_input_ids, pos_attn, pos_rmask,\n    neg_input_ids, neg_attn, neg_rmask,\n    beta: float = 0.1,\n):\n    def seq_logprob_and_len(model, input_ids, attn_mask, resp_mask):\n        logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n        logits = logits[:, :-1, :]\n        labels = input_ids[:, 1:]\n\n        logp = F.log_softmax(logits, dim=-1)\n        tok_logp = logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n\n        mask = (resp_mask[:, 1:] & attn_mask[:, 1:]).to(tok_logp.dtype)\n        seq_lp = (tok_logp * mask).sum(dim=-1)            # (B,)\n        resp_len = mask.sum(dim=-1).clamp_min(1.0)        # (B,)\n        return seq_lp, resp_len\n\n    # policy\n    lp_pos, len_pos = seq_logprob_and_len(policy, pos_input_ids, pos_attn, pos_rmask)\n    lp_neg, len_neg = seq_logprob_and_len(policy, neg_input_ids, neg_attn, neg_rmask)\n\n    # ref (no grad)\n    with torch.no_grad():\n        lp_ref_pos, len_ref_pos = seq_logprob_and_len(ref_policy, pos_input_ids, pos_attn, pos_rmask)\n        lp_ref_neg, len_ref_neg = seq_logprob_and_len(ref_policy, neg_input_ids, neg_attn, neg_rmask)\n\n    # length-normalized log-ratio\n    pos_term = (lp_pos / len_pos) - (lp_ref_pos / len_ref_pos)\n    neg_term = (lp_neg / len_neg) - (lp_ref_neg / len_ref_neg)\n    delta = beta * (pos_term - neg_term)\n\n    loss = -F.logsigmoid(delta).mean()\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    return loss",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#ppo-vs.-dpo",
    "href": "posts/CS336/Lecture15/lec15.html#ppo-vs.-dpo",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "2.7 PPO vs. DPO",
    "text": "2.7 PPO vs. DPO\n\nPPO 的优势来自“更灵活的优化信号”\nPPO 显式做 on-policy rollout + advantage（GAE）+ clip + KL 约束，能更直接地把“奖励模型/偏好信号”转成梯度更新；\nDPO/SimPO 更像“把 RL 变成监督学习”，简单、稳定、便宜，但表达能力/可控性有时不如 PPO（尤其当你需要更细的控制或 reward 很复杂时）。\n右侧 Tülu 3 的表：同一个 benchmark 上，结论也会跟着超参和变体跑\n你会看到 SimPO、DPO、PPO、DPO-norm（长度归一化）分数差异不大，而且对 β、γ、学习率、batch size、epoch 很敏感。\n⇒ 这页想让你记住：在 RLHF 里，工程细节（数据 + 超参 + 训练recipe）往往比“算法名字”更决定结果。\n\nDPO/SimPO 把 RLHF 简化成“好实现的监督学习”，但 PPO 仍可能在某些数据/奖励/超参组合下更强；因此 RLHF 的实验结论必须连同 setup 一起看。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#over-optimization",
    "href": "posts/CS336/Lecture15/lec15.html#over-optimization",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "3.1 Over-optimization",
    "text": "3.1 Over-optimization\n\n横轴是 KL distance（RL 后的策略跟初始/参考策略差多远）。\n纵轴是 RM score（reward model 给的分）。\n曲线先升后“变坏”：一开始往 RM 喜欢的方向走，分数上升；但当 KL 越来越大时，模型会学到 奖励模型的漏洞/捷径，导致：\n\nRM 分数可能还很高，但真实质量（人类偏好/事实性/有用性）开始下降；\n这就是典型的 “对代理目标（proxy reward）过拟合”。\n\n\n一句话：你优化的是 RM，不是人类真实偏好；走太远会开始“刷分”。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture15/lec15.html#model-collapse",
    "href": "posts/CS336/Lecture15/lec15.html#model-collapse",
    "title": "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)",
    "section": "3.2 Model Collapse",
    "text": "3.2 Model Collapse\nRLHF 会把模型从“按概率拟合数据”的语言模型，推成“为拿高奖励而输出”的策略模型，从而降低输出分布的熵、压缩多样性，并让模型的置信度不再可信（calibration 变差）。\nThis is the updated content.",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 15: LLM Alignment SFT & RLHF(PPO, DPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture01/lec01.html#character-level-tokenization",
    "href": "posts/CS336/Lecture01/lec01.html#character-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.1 Character-level Tokenization",
    "text": "2.1 Character-level Tokenization\nCharacter-level Tokenization是将文本拆分为单个字符。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n需要知道的就是，每个Character就是一个token，利用Python，的ord()函数，我们可以将其转换为对应的整数ID：\ntext = \"Hello, world!\"\ntokens = [char for char in text]\ntoken_ids = [ord(char) for char in tokens]\nprint(tokens)\nprint(token_ids)\n这种方式很简单，也很直观，但是它有一些明显的缺点：\n\nVocabulary Size：对于所有可能的字符（包括字母、数字、标点符号和特殊字符），Vocabulary Size会非常大，导致模型参数量增加。（大约有150K 个不同的字符）\n150K个字符中，有很多字符是非常少见的，导致模型难以学习到这些字符的表示。\n语义信息缺失：单个字符无法捕捉到词语的语义信息，导致模型难以理解上下文。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 01: Introduction & BPE"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture01/lec01.html#word-level-tokenization",
    "href": "posts/CS336/Lecture01/lec01.html#word-level-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.2 Word-level Tokenization",
    "text": "2.2 Word-level Tokenization\n与Character-level Tokenization不同，Word-level Tokenization是将文本拆分为单词。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['Hello,', 'world!']\n每个单词就是一个token，同样地，我们可以通过一个简单的Python代码将单词转换为对应的整数ID：\nimport regex\n\ntext= \"Hello, world!\"\ntokens = text.split()  # 简单的空格拆分\nvocab = {word: idx for idx, word in enumerate(set(tokens))}\ntoken_ids = [vocab[word] for word in tokens]\nprint(tokens)\nprint(token_ids)\n除了简单的根据空格拆分单词的方法，我们还可以有稍微复杂一点的方法，比如使用正则表达式来处理标点符号等。举个例子，GPT-2的tokenizer就是使用了一种基于正则表达式的方法来进行Word-level Tokenization。\nGPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n当然，这种方法也有比较明显的缺点：\n\nVocabulary Size 依然很大：对于所有可能的单词，Vocabulary Size会非常大，导致模型参数量增加。\n未登录词问题（Out-of-Vocabulary, OOV）：对于训练集中未出现的单词，模型无法处理，导致性能下降。 尽管我们可以通过一些方法（如使用特殊的&lt;UNK&gt; token）来缓解这个问题，但仍然无法完全解决。\nVocabulary Size 的大小不是固定的，随着训练数据的增加，Vocabulary Size会不断增加，导致模型难以扩展。\n很多单词是非常少见的，导致模型难以学习到这些单词的表示。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 01: Introduction & BPE"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture01/lec01.html#byte-based-tokenization",
    "href": "posts/CS336/Lecture01/lec01.html#byte-based-tokenization",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.3 Byte-Based Tokenization",
    "text": "2.3 Byte-Based Tokenization\n在学习Byte Pair Encoding (BPE)之前，我们先介绍一下Byte-Based Tokenization。 Byte-Based Tokenization是将文本拆分为字节单元（byte-level tokens）。 例如，句子 “Hello, world!” 会被拆分为以下tokens：\n['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n每个字节就是一个token，同样地，我们可以通过一个简单的Python代码将字节转换为对应的整数ID：\ntext = \"Hello, world!\"\ntokens = [char.encode('utf-8') for char in text]\ntoken_ids = [byte[0] for byte in tokens]\nprint(tokens)\nprint(token_ids)\n这种方法的优点是：\n\nVocabulary Size 固定且较小：由于字节的范围是0-255，Vocabulary Size固定为256，模型参数量较小。\n无OOV问题：由于所有文本都可以表示为字节序列，不存在OOV的问题。\n适用于多语言文本：字节级别的表示可以处理各种语言的文本。\n简单高效：字节级别的表示简单且高效，适合大规模文本处理。\n\n不过，这种方法有个明显的缺点就是Compression Ratio较低。 由于字节级别的表示过于细粒度，导致文本长度增加，影响模型的训练效率和性能。因为Transformer模型的计算复杂度与输入长度的平方成正比，输入长度增加会显著增加计算资源的消耗。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 01: Introduction & BPE"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "href": "posts/CS336/Lecture01/lec01.html#byte-pair-encoding-bpe",
    "title": "Lecture 01: Introduction & Tokenization",
    "section": "2.4 Byte Pair Encoding (BPE)",
    "text": "2.4 Byte Pair Encoding (BPE)\n为了克服Character-level和Word-level Tokenization的缺点，同时提高Byte-Based Tokenization的Compression Ratio，我们引入了Byte Pair Encoding (BPE)算法 (Sennrich, Haddow, and Birch 2016) 。 BPE是一种基于频率的子词单元（subword unit）分词方法。 它的基本思想是通过迭代地合并最频繁出现的字符对（byte pairs）来构建一个更紧凑的词汇表。\n\n\n\nBPE算法的步骤如下：\n\n初始化Vocabulary：将文本中的所有唯一字符作为初始的词汇表（vocabulary）。\n统计频率 get_stats：计算文本中所有相邻字符对的出现频率。\n合并字符对并且更新Vocabulary：选择出现频率最高的字符对，将其合并为一个新的token，并更新文本中的所有出现该字符对的地方，并且将新token添加到词汇表中。\n重复步骤2-4：重复上述步骤，直到达到预定的词汇表大小或满足其他停止条件。\n\n\n\n\n\n\n\n\n\nFigure 4: BPE算法的示意图，展示了字符对的合并过程。\n\n\n\n\n\n\n下面是一个简单的BPE算法的Python实现示例：\ndef train_bpe(string: str, num_merges: int):\n    indices = list(map(int, string.encode(\"utf-8\"))) \n    merges: dict[tuple[int, int], int] = {}  \n    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  \n\n    for i in range(num_merges):\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]): \n            counts[(index1, index2)] += 1\n        \n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        new_index = 256 + i\n        merges[pair] = new_index\n        vocab[new_index] = vocab[index1] + vocab[index2]\n        indices = merge(indices, pair, new_index)\n\n    return merges, vocab\n以上是最简单的BPE算法的实现，显然有很多的不足，我们在Assignment 01的第一部分中，会优化这个实现：\n\n优化 merge 函数的效率。\n利用pre-tokenization来加速BPE的训练过程。\n\n等\n对于那些想深入理解BPE算法的同学，可以参考以下资源： \n关于BPE算法，还有很多可以优化的地方，比如：\n\n在寻找最频繁字符对时，可以使用更高效的数据结构（如优先队列）来加速查找过程。\n在合并字符并且更新文本时，可以使用更高效的字符串处理方法来减少时间复杂度。这些优化可以显著提高BPE算法的训练速度，特别是在处理大规模文本数据\n\n这些方法我们将在 Assignment 01中进行更加详细的介绍。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 01: Introduction & BPE"
    ]
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html",
    "href": "posts/CS336/Ass01/ass01.html",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "",
    "text": "Assignment 01 要求我们从0实现一个简单的语言模型训练流程，涵盖：\n通过这一个Assignment，我们可以了解到创建一个完整的LM模型的全部流程，后续的课程以及Assignment都会基于这个流程进行扩展和优化。\n在完成这个Assignment之前，我们首先需要学习Lecture 01,02,03 的内容，主要包括：\n当然，我们还需要对Transformer有一定的了解，如果你不了解Transformer，我个人推荐阅读这篇 100-PaperwithCode系列的第一篇：01 Attention is all you Need.\n对于不熟悉LM的同学们来说，这个Assignment可能有一定的难度，毕竟光任务的描述就50多页。不过，只要我们一步一步来，还是可以完成这个Assignment的。加油，别放弃😃😃！！\n预计需要的时间10hours."
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#download-start-code-and-dataset",
    "href": "posts/CS336/Ass01/ass01.html#download-start-code-and-dataset",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "1.1 Download Start Code and Dataset",
    "text": "1.1 Download Start Code and Dataset\n首先我们需要下载Start Code：\ngit clone https://github.com/stanford-cs336/assignment1-basics\n下载完代码之后，我们再下载数据集：\nmkdir -p data\ncd data\n\nwget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\nwget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n\nwget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\ngunzip owt_train.txt.gz\nwget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\ngunzip owt_valid.txt.gz\n\ncd ..\n上面这个代码会下载两个数据集：\n\nTinyStories：一个非常小的故事数据集(1GB) ，适合快速测试和调试代码。\nOpenWebText (OWT) Sample：一个较大的文本数据集(4.7GB)，适合进行更深入的训练和评估。\n\n除此之外，我们还需要安装 uv:\n\n\nuv是什么？\n\n\nuv是一个轻量级的Python项目管理和运行工具，可以帮助我们更方便地运行和测试代码。在这个Assignment中，我们将使用uv来运行测试和管理项目依赖。\n\n\npip install uv\n安装完uv之后，我们就可以通过一下的代码来运行测试代码：\nuv run pytest \nuv run python train.py"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#sec-bpe-recap",
    "href": "posts/CS336/Ass01/ass01.html#sec-bpe-recap",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.1 BPE Algorithm Recap",
    "text": "2.1 BPE Algorithm Recap\n回顾一下BPE算法的基本步骤：\n\nInitialization: 将输入文本视为字节序列，每个字节作为一个token。初始化词汇表包含所有可能的字节（0-255）。以及Special Tokens，比如 &lt;|endoftext|&gt;\nCount Pairs: 统计文本中所有相邻字节对的出现频率。\nMerge Pairs: 将频率最高的字节对其合并为一个新的token，更新文本和词汇表:\n\nGet the most frequent pair: 找到出现频率最高的字节对。\nAdd the new pair: 将这个新的字节对加入词汇表。\nUpdate the word counter: 更新文本中所有出现该字节对的地方。\nUpdate Pairs Counts: 重新统计文本中所有相邻字节对的出现频率。\n\nRepeat: 重复步骤2,3，直到达到预定的合并次数\n\nBPE 的伪代码如下所示：\n\n\n\\begin{algorithm} \\caption{BPE Training} \\begin{algorithmic} \\Require Corpus text $\\mathcal{D}$ \\Require Number of merges $M$ \\Require Special tokens $\\mathcal{S}$ (e.g., \\texttt{&lt;|endoftext|&gt;}) \\Ensure Vocabulary $V$, merge rules list $\\Pi$ \\State Convert each document $x \\in \\mathcal{D}$ into bytes $b(x)$ \\State Initialize tokenized corpus as sequences of single-byte tokens \\State $V \\gets \\{0,1,\\dots,255\\} \\cup \\mathcal{S}$ \\State $\\Pi \\gets [\\,]$ \\For{$t = 1$ \\textbf{to} $M$} \\State $C \\gets$ empty map from pair $\\to$ count \\For{\\textbf{each} token sequence $s$ in the corpus} \\Comment{Count adjacent byte-token pairs} \\For{\\textbf{each} index $i$ from $1$ to $|s|-1$} \\State $C[(s_i, s_{i+1})] \\gets C[(s_i, s_{i+1})] + 1$ \\EndFor \\EndFor \\State Choose $(a,b)$ with the largest count in $C$ \\Comment{Select the most frequent pair} \\State Define a new token $new$ as the merge of $(a,b)$ \\State $V \\gets V \\cup \\{new\\}$ \\Comment{Create a new merged token and record the merge rule} \\State Append $(a,b)\\rightarrow new$ to $\\Pi$ \\For{\\textbf{each} token sequence $s$ in the corpus} \\Comment{Replace all occurrences of $(a,b)$ in the corpus} \\State Replace adjacent $(a,b)$ with $new$ left-to-right (non-overlapping) \\EndFor \\EndFor \\State \\Return $V, \\Pi$ \\end{algorithmic} \\end{algorithm}\n\n\n首先，我们来实现一下最简单的BPE算法:"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#sec-bpe-v0",
    "href": "posts/CS336/Ass01/ass01.html#sec-bpe-v0",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.2 BPE Version 0",
    "text": "2.2 BPE Version 0\n假如我们要Tokenized以下的文本：\nstring = \"\"\" \nlow low low low low &lt;|endoftext|&gt;\nlower lower widest widest widest &lt;|endoftext|&gt;\nnewest newest newest newest newest newest \n\"\"\"\nStep1 要做的就是初始化我们的词汇表：\ndef init_vocab(special_tokens: list[str] | None = None) -&gt; dict[int, bytes]:\n    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # idx -&gt; byte representation\n    current_index = 256\n\n    if special_tokens:\n        for token in special_tokens:\n            token_bytes = token.encode(\"utf-8\")\n            vocab[current_index] = token_bytes\n            current_index += 1\n\n    return vocab\n初始化时，我们会先为所有 byte 值 0–255 建立基础词表（文本先用 UTF-8 编码成字节序列来处理），并额外加入 special tokens；在编码过程中这些 special tokens 会被 优先匹配并作为整体保留，不参与普通的切分与 BPE 合并。\n接下来我们来实现Step2: 统计文本中所有相邻字节对的出现频率。\ndef pair_counts(word_counter: dict[tuple[int, ...], int]) -&gt; dict[tuple[int, int], int]:\n    pairs: dict[tuple[int, int], int] = {}\n\n    for word, count in word_counter.items():\n        for a, b in zip(word, word[1:]):\n            pairs[(a, b)] = pairs.get((a, b), 0) + count\n\n    return pairs\n我们先统计每个词（token 序列）出现的次数 count，再在遍历该词的相邻 token 对时，把每个 pair 的出现次数累加 count，从而得到全语料的 pair 频次。\n接下来，我们需要实现 Step3.1: 找到出现频率最高的字节对。在这里，我们遵循的规则是：\n\n频率最高的pair\n若多个 pair 频率相同，我们按 pair 的字典序（先比左 token，再比右 token）选择更大的那个。\n\ndef get_most_frequent_pair(\n    pair_counter: dict[tuple[int, int], int],\n) -&gt; tuple[int, int]:\n    max_freq = max(pair_counter.values())\n    candidates = [pair for pair, freq in pair_counter.items() if freq == max_freq]\n    res = max(candidates)\n\n    return res\n它(res)是本轮要 merge 的 pair（将它替换为一个新 token）\n接下来，我们需要实现Step3.2: 将这个新的字节对加入词汇表：\ndef add_pair_to_vocab(\n    vocab: dict[int, bytes],\n    pair: tuple[int, int],\n) -&gt; int:\n    index1, index2 = pair\n    vocab[len(vocab)] = vocab[index1] + vocab[index2]\n    return len(vocab) - 1\n将这个新的pair加入词汇表后，我们需要实现 Step3.3 和 Step3.4: 更新文本中所有出现该字节对的地方，以及重新统计文本中所有相邻字节对的出现频率。 在这里，我们需要遍历所有的word，来看是不是有这个pair出现，若出现了，就将其合并成一个新的token。 同时，我们还需要重新统计所有的pair的频率。\ndef merge_pair_ids(\n    word_counter: dict[tuple[bytes] | tuple[int], int],\n    pair: tuple[int, int],\n    new_id: int,\n) -&gt; tuple[dict[tuple[int], int], dict[tuple[int, int], int]]:\n    new_word_counter: defaultdict[tuple[int], int] = defaultdict(int)\n    updated_pair_counts: defaultdict[tuple[int, int], int] = defaultdict(int)\n\n    for token, freq in word_counter.items():\n        new_token = []\n        i = 0\n        L = len(token)\n\n        while i &lt; L:\n            if i + 1 &lt; L and (token[i], token[i + 1]) == pair:\n                new_token.append(new_id)\n                i += 2\n            else:\n                new_token.append(token[i])\n                i += 1\n\n        new_word_counter[tuple(new_token)] += freq\n\n        for index1, index2 in zip(new_token[:-1], new_token[1:]):\n            updated_pair_counts[(index1, index2)] += freq\n\n    return dict(new_word_counter), dict(updated_pair_counts)\n至此，我们已经完成了一轮，重复以上的步骤，直到我们达到目标的轮数，放在一起代码就是：\ndef train_bpe(\n    string: str = string,\n    vocab_size: int = 263,\n    special_tokens: list[str] = special_tokens,\n    save_path: str | None = None,\n):\n    vocab = init_vocab(special_tokens)\n    num_merges = vocab_size - len(vocab)\n\n    merges: dict[tuple[int, int], int] = {}\n\n    word_counter = pre_tokenize(string, special_tokens, including_special=False)\n\n    pairs_freqs = pair_counts(word_counter)\n\n    for _ in range(num_merges):\n        most_common_pair = get_most_frequent_pair(pairs_freqs)\n        new_index = add_pair_to_vocab(vocab, most_common_pair)\n        merges[most_common_pair] = new_index\n        word_counter, pairs_freqs = merge_pair_ids(word_counter, most_common_pair, new_index)\n    \n    return vocab, merges\n这也就是我们最简单的BPE的算法，我们称其为BPE Version0, 当我们运行这个代码，并且把vocab_size设置为263时，我们可以得到以下merges的顺序。\ntrain_bpe(\n    string=string,\n    vocab_size=256 + 1 + 6, # 256 bytes + 1 special token + 6 merges\n    special_tokens=special_tokens,\n)\nMost common pair: (b's', b't') -&gt; 9\nMost common pair: (b'e', b'st') -&gt; 9\nMost common pair: (b'o', b'w') -&gt; 7\nMost common pair: (b'l', b'ow') -&gt; 7\nMost common pair: (b'w', b'est') -&gt; 6\nMost common pair: (b'n', b'e') -&gt; 6\n我们可以看到，尽管这个版本的BPE算法是正确的，但是它的效率非常低，因为每次我们都需要遍历所有的pair，来找到出现频率最高的pair，这样的时间复杂度是 \\(\\mathcal{O}(N \\cdot P)\\)，其中N是合并的次数，P是pair的数量。如果只是用这种简单的算法，我们是通不过测试的。因此我们需要优化这个算法，不过在优化之前，我们先来了解一下Pre-Processing的步骤。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#pre-processing",
    "href": "posts/CS336/Ass01/ass01.html#pre-processing",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.3 Pre-Processing",
    "text": "2.3 Pre-Processing\n在实现BPE算法之前，我们需要对文本进行预处理（Pre-Processing），主要包括两个步骤：\n\n根据Special Tokens来分文本\n根据正则表达来分文本\n\n我们先来看一下根据Special Tokens来分文本的情况\n\n2.3.1 Special Tokens Based Splitting\n在这一节(Section 2.1) ，我们已经了解过了，在初始化vocab 时，我们也需要初始化special tokens，其中一个常见的special tokens就是 &lt;|endoftext|&gt;. 这个token意味着一段文本的结束。给出一段很长的文本，我们要做的第一件事情就是把这个文本分成许多段，代码的实现如下：\ndef split_by_special_tokens(text: str, special_tokens: list[str], include_special: bool = False) -&gt; list[str]:\n    if not special_tokens:\n        return [text]\n\n    special_tokens_sorted = sorted(special_tokens, key=len, reverse=True)\n    pattern = \"|\".join(re.escape(t) for t in special_tokens_sorted)\n\n    if include_special:\n        special_chunks = re.split(f\"({pattern})\", text)\n    else:\n        # Split without capturing the special tokens\n        special_chunks = re.split(pattern, text)\n\n    return special_chunks\n至此，我们就完成了 Special Token-aware 的切分：\n\n通过把所有 special tokens 先按长度降序排序，并用正则构造匹配 pattern，我们可以把原始长文本拆成一系列 普通文本片段（以及可选的 special token 片段）。\n当 include_special=True 时，re.split(f\"({pattern})\", text) 会把匹配到的 special token 也保留下来，从而在后续编码时我们可以把它们当作“原子 token”直接映射到对应的 id；\n当 include_special=False 时，special token 会作为分隔符被丢弃，仅返回普通文本片段，适合训练阶段不想让 special tokens 参与 pair 统计 / merges 的场景。\n\n接下来，我们就可以对每个普通片段执行Regular-based的切分了，在这个过程中，我们会把文本切成更小的片段，比如词、子词片段、标点分隔片段等。\n\n\n2.3.2 Regex-based Splitting (Pre-Tokenization)\nPre-Tokenization（预分词） 就是在真正训练 BPE 合并规则之前，先对整份语料做一次粗粒度的切分，把文本切成一段段“更大的片段”（pre-token），然后在这些片段内部去统计相邻字节（byte pair）的出现频率。、 那么，为什么需要Pre-Tokenization呢，主要有两个原因：\n\n原因一：避免“每合并一次就全语料扫一遍”  我们知道，merge一次，我们就要重新扫描一次，以获得更新后的新语料，如果这个语料特别大，或者我们merge的次数特别多，那么就会导致我们算法特别的慢。  这个时候我们就需要Pre-Tokenization，它的作用是：\n\n先把语料切成很多“pre-token”（比如词、子词片段、标点分隔片段等）\n\n统计时不再对整个语料逐字符/逐字节扫描，而是利用重复出现的 pre-token 的次数来加速。\n\n举个例子：\n\n‘text’ 这个 pre-token 出现了 10 次\n当我们要统计 ‘t’ 和 ‘e’ 相邻出现次数\n只要在 ‘text’ 里看到一次 “t”+“e” 相邻，就可以一次性把计数加 10 而不是把语料里每个 ‘text’ 都逐字节再看一遍。\n\n\n原因二：避免学出“只有标点不同”的重复 token  比如有两个词 dog! 和 dog. 如果我们那不Pre-tokenization，那么这个很容易被当成不同的序列，从而对于这个类似的词，有两个完全不同的IDs。而 Pre-tokenization 通常会用一些规则（比如按空白、标点边界等）先切开，让 BPE 更多在“词内部”学习合并规律，而不是把词和各种标点粘在一起乱合并。\n在这个 Assignment 里，我们采用 regex-based pre-tokenizer（GPT-2 使用的那条正则），先把原始文本切成一串“预分词片段”（pre-tokens），再对每个片段做 byte-level BPE。\nPAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\n\nRegex 详解\n\n\n我们来详细解释一下这个正则表达式： 它的分块规则（从左到右匹配）：\n\n英语缩写/词尾：’s, ’t, ’re, ’ve, ’ll, ’m, ’d 等（第一段）\n字母串： ?+ —— 一段字母（允许前面带一个可选空格，把空格“粘”到后面的 token 上）\n数字串： ?+ —— 一段数字（同样允许前置空格）\n标点/其它符号串： ?[^\\s\\p{L}\\p{N}]+ —— 非空白、非字母、非数字的一串符号（也允许前置空格）\n空白：+(?!)（末尾空白）或 +（一般空白）\n\n+：匹配一段空白（空格、换行、tab 等）。\n(?!)：负向前瞻，确保这段空白后面没有非空白字符（即这是行尾或文本末尾的空白）。\n\n\n这种设计的关键点是：很多 token 会把前导空格包含进去（例如 ” hello” 会被当成一个整体的 pre-token），这能更好地匹配英语里“词边界=空格”的统计特性，也更接近 GPT-2 的实际 tokenizer 行为。如果不理解，也没有关系，直接用这个正则就行。\n\n\n有了这个正则表达式，我们就可以实现 Pre-Tokenization 了，代码如下：\ndef pre_tokenize(string: str, special_tokens: list[str], including_special: bool = False) -&gt; Counter:\n    word_counter = Counter()\n\n    chunks = split_by_special_tokens(string, special_tokens, include_special=including_special)\n\n    for chunk in chunks:\n        if including_special and chunk in special_tokens:\n            word_counter[tuple(string_to_bytes(chunk))] += 1\n        else:\n            for match in re.finditer(PAT, chunk):\n                word = match.group(0)\n                word_encoded = tuple(string_to_bytes(word, return_int=True))\n                word_counter[word_encoded] += 1\n\n    return word_counter\n通过 pre-tokenization，我们把原始文本转换成许多“预分词片段”的 byte/id 序列，并用 Counter 统计每种片段出现的次数。后续在统计 pair 频率时，每个片段的相邻 token 对出现次数都会按其 count 加权累加，从而得到全语料的 pair 频次。\n\n\n2.3.3 Multi-Processing\n以上这两步（Special Token-aware Splitting 和 Regex-based Pre-Tokenization），我们可以通过一个 Multi-Processing\n\n\nMulti-Processing Review\n\n\nPython 的MultiProcessing 是一个- 多进程 更适合 CPU 密集型任务（比如预分词、统计）。，我们只需要了解以下的内容：\nfrom multiprocessing import Process, Queue  \nimport queue  \nfrom collections import Counter \n\ndef task(*args):  # 定义实际要并行执行的任务函数\n    # ... do something ...  # 这里写你的真实任务逻辑\n    return Counter()  # 返回一个 Counter（示例），便于主进程聚合\n\ndef task_worker(out_queue: Queue, *args):  # worker：接收输出队列和任务参数\n\n    output = task(*args)  # 执行任务，得到部分结果\n    \n    out_queue.put(output)  # 把结果放进队列，交给主进程汇总\n\nnum_process = 4  # 进程数示例（你需要自己设置）\ntask_args_list = [(\"a\",), (\"b\",), (\"c\",), (\"d\",)]  # 每个进程的参数示例（你需要替换成真实参数）\n\nout_queue: Queue = manager.Queue() # 创建进程间通信队列\nprocesses: list[Process] = []  # 保存所有进程对象，方便后面 join\n\nfor args in task_args_list:  # 遍历每个任务的参数\n    p = Process(target=task_worker, args=(out_queue, *args))  # 创建进程，并把队列+参数传给 worker\n    processes.append(p)  # 记录进程对象\n    p.start()  # 启动进程开始执行\n\nall_out = Counter()  # 主进程的总 Counter，用于累加所有部分结果\n\nfor _ in range(len(processes)):  # 预期每个进程都会 put 一次结果，所以收 len(processes) 次\n    try:\n        partial_out = out_queue.get(timeout=10)  # 从队列取一个结果，最多等待 10 秒\n        all_out.update(partial_out)  # 把这个进程的 Counter 合并到总 Counter\n    except queue.Empty:  # 如果超时没取到，就跳过\n        continue  # 继续尝试下一个\n\nfor p in processes:  # 遍历所有进程\n    p.join()  # 等待进程结束\n\n\n有了这些前置知识之后，实现这个Pre-Processing的步骤就很容易了，以下是Pre-process的代码\n\n\ncs336_basics/tokenizer/tokenizer.py\n\ndef pre_tokenize_string_worker(*args):\n    input_path, special_tokens, queue, start, end, include_special = args\n\n    # Read the chunk from the file\n    with open(input_path, \"rb\") as f:\n        f.seek(start)\n        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n\n    word_counter = pre_tokenize(chunk, special_tokens, include_special)\n\n    # Put the result in the queue\n    queue.put(word_counter)\n\ndef train_bpe():\n    with open(input_path, \"rb\") as f:\n        chunk_boundaries = find_chunk_boundaries(\n            f, desired_num_chunks=kwargs.get(\"desired_num_chunks\", NUM_PROCESSES), split_special_token=b\"\\n\"\n        )\n    \n    manager = Manager()\n    queue = manager.Queue()\n    processes: list[Process] = []\n    \n    for start, end in zip(chunk_boundaries[:-1], chunk_boundaries[1:]):\n        p = Process(\n            target=pre_tokenize_string_worker,\n            args=(input_path, special_tokens, queue, start, end, False),\n        )\n        processes.append(p)\n        p.start()\n\n    word_counter = Counter() \n    for _ in range(len(processes)):\n        try:\n            partial_counter = queue.get(timeout=10)\n            word_counter.update(partial_counter)\n        except Empty:\n            continue\n    for p in processes:\n        p.join()\n\n通过这个合集，我们的得到了 word_counter 这个变量. 它记录了每个 pre-token（byte/id 序列）在整个语料中出现的次数，接下来我们就可以基于这个 word_counter 来统计 pair 频次，并进行 BPE 合并了.\n\n\nMulti-Processing 注意事项\n\n\n需要注意的一点是，进程数（NUM_PROCESSOR）并不是越多越好。在实际实现中，当进程数继续增大时，整体速度反而可能变慢，主要原因有三点：\n\n创建与调度开销：启动多个进程本身就有成本（fork/spawn、初始化、调度），任务越细碎，这部分开销占比越高。\n跨进程通信成本：多进程之间需要传递数据（例如把 chunk 分发给 worker、再把统计结果汇总回来），会引入序列化/反序列化（pickle）以及 IPC 的额外耗时。\n内存与缓存压力：进程越多，往往会带来更高的内存占用与 cache/memory bandwidth 竞争，反而拖慢吞吐。\n\n因此，多进程的最佳数量通常取决于：任务粒度、数据规模、CPU 核数、以及 IPC 的比例。在本次 Assignment 的语料规模与实现方式下，一个经验上更稳的选择是 NUM_PROCESSOR=4：既能获得明显的并行加速，又能避免过多进程带来的额外开销与拥塞。\n\n\n\n\n2.3.4 Others\n除了 word_counter（记录每个 word/token 序列出现次数）之外，我们还会额外构建两个辅助结构，来支持后续 更高效的 pair 统计与更新：\n\n\ncs336_basics/tokenizer/tokenizer.py\n\npairs_counter = Counter()\npair_to_words: dict[tuple[int, int], set[tuple[int, ...]]] = defaultdict(set)\nfor word in word_counter:\n    for i in range(len(word) - 1):\n        pair = (word[i], word[i + 1])\n        pair_to_words[pair].add(word)\n        pairs_counter[pair] += word_counter[word]\n\n\npairs_counter[pair]：记录该相邻 pair 在全语料中的总出现次数。 因为每个 word 在语料中出现了 word_counter[word] 次，所以 word 内部每出现一次 pair，就为全局频次贡献 word_counter[word]。\npair_to_words[pair]：记录该 pair 出现在哪些 word（token 序列）里, 这个映射非常关键：当我们选择某个 pair 进行 merge 时，只有包含该 pair 的 word 会发生变化。借助 pair_to_words，我们可以只遍历这些“受影响的 words”，并对 pairs_counter 做局部增量更新，而不是每轮都重新扫描全部 word_counter。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#bpe-version-1-using-heap",
    "href": "posts/CS336/Ass01/ass01.html#bpe-version-1-using-heap",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.4 BPE Version 1: Using Heap",
    "text": "2.4 BPE Version 1: Using Heap\n一个很明显的优化点是：每一轮都要找当前频率最高的 pair。在 Version 0 (Section 2.2) 里，我们每轮都通过遍历 pairs_counter 来取最大值，这一步是 \\(\\mathcal{O}(n)\\)（\\(n\\) 是 pair 的数量）。而这个操作正好符合堆（heap）的使用场景：用堆维护“当前最大的元素”，就能把“取最大”降到 \\(\\mathcal{O}(\\log n)\\)（严格来说是：取堆顶是 \\(\\mathcal{O}(1)\\)，但如果包含 pop/push 更新则是 \\(\\mathcal{O}(\\log n)\\)）。\n具体做法是把每个 pair 作为堆元素，并把“排序依据”设计成：\n\n频次越大优先级越高\n频次相同则按 pair 的字典序更大者优先\n\n在 Python 的 heapq 是最小堆，因此我们可以用负号把它变成“最大堆”，例如存成：\n\nkey = (-freq, -a, -b)\n\n这样每一轮我们都能快速拿到候选的“最常见 pair”。\n不过要注意一点：频次在 merge 之后会发生变化，因此堆里旧的条目可能变“过期”。在 pop 堆顶时，我们需要检查该 pair 的当前频次是否和堆里存的频次一致；如果不一致，说明堆顶是过期的，就继续 pop 直到找到一个有效的 pair。\n\n\ncs336_basics/tokenizer/merge_fn.py\n\nclass HeapItem:\n    def __init__(self, neg_freq: int, pair_bytes: tuple[bytes, bytes], pair: tuple[int, int]):\n        self.neg_freq = neg_freq\n        self.pair_bytes = pair_bytes\n        self.pair = pair\n\n    def __lt__(self, other: \"HeapItem\") -&gt; bool:\n        if self.neg_freq != other.neg_freq:\n            return self.neg_freq &lt; other.neg_freq\n        return self.pair_bytes &gt; other.pair_bytes  # reverse order for max-heap behavior\n\n\ndef build_pair_heap(pairs_freqs: Counter, vocab: dict[int, bytes]):\n    heap = []\n    for (a, b), f in pairs_freqs.items():\n        if f &gt; 0:\n            item = HeapItem(-f, (vocab[a], vocab[b]), (a, b))\n            heapq.heappush(heap, item)\n    return heap\n\n\ndef pop_most_frequent_pair(heap, pairs_counter: Counter) -&gt; tuple[int, int]:\n    while heap:\n        item = heap[0]  # Peek at the top item\n        neg_f = item.neg_freq\n        pair = item.pair\n        cur_f = pairs_counter.get(pair, 0)\n        if cur_f &lt;= 0 or -neg_f != cur_f:  # frequency changed, which means the pair we store in heap is stale\n            heapq.heappop(heap)\n            continue\n        return pair\n\n    raise ValueError(\"No positive-frequency pairs remain\")"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#bpe-version-2-using-heap-indexing",
    "href": "posts/CS336/Ass01/ass01.html#bpe-version-2-using-heap-indexing",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.5 BPE Version 2: Using Heap + Indexing",
    "text": "2.5 BPE Version 2: Using Heap + Indexing\n除了用 Heap 加速“选出频率最高的 pair”，另一个更关键的瓶颈在于 merge 更新阶段：在 Version 0{Section 2.2} 里，我们每一轮都会遍历 word_counter 里的所有 word，检查这个 word 里是否出现了目标 pair；这一步的代价通常非常高，因为绝大多数 word 根本不包含 当前要 merge 的 pair，但我们还是把它们都扫了一遍。\n因此我们可以用一个“倒排索引”来做 空间换时间：提前维护一个映射 pair -&gt; {words…}，记录每个 pair 出现在哪些 word 中。这样当我们决定 merge 某个 pair 时，就只需要遍历 pair_to_words[pair] 里的那一小部分 word，而不必全量扫描所有 word。\n这也正是我们搭建 pair_to_words 的原因：\n\n没有索引：每轮 merge 都是 全量扫描所有 words（慢，\\(\\mathcal{O}(\\#words)\\) 级别）。\n有索引：每轮只处理 包含该 pair 的 words 子集（快，复杂度取决于该 pair 的覆盖范围，通常远小于全量）。\n\n接下来，我们还需要在 merge 之后，更新这个索引：当某个 pair 被 merge 成一个新 token 后，所有包含该 pair 的 word 都会发生变化，因此我们需要把这些 word 从旧 pair 的索引里移除，并把它们添加到新 pair 的索引里。具体实现如下：\n\n\ncs336_basics/tokenizer/merge_fn.py\n\ndef merge_pairs_with_heap_index(\n    word_counter: dict[tuple[int, ...], int],\n    pair_counter: Counter,\n    target_pair: tuple[int, int],\n    new_id: int,\n    vocab: dict[int, bytes],\n    pair_heap,\n    pair_to_words: dict[tuple[int, int], set[tuple[int, ...]]],\n) -&gt; tuple[\n    dict[tuple[int, ...], int],\n    Counter,\n    list,\n    dict[tuple[int, int], set[tuple[int, ...]]],\n]:\n    # Start from full counters so unaffected words remain.\n    new_word_counter: Counter = Counter(word_counter)\n    updated_pair_counter: Counter = pair_counter.copy()\n    changed_pairs: set[tuple[int, int]] = set()\n\n    # Get all words that contain the target pair.\n    affected_words = list(pair_to_words.get(target_pair, set()))\n\n    for w in affected_words:\n        freq = word_counter.get(w, 0)\n        if freq &lt;= 0 or len(w) &lt; 2:\n            continue\n\n        # 1. Remove the old word from the corpus counts.\n        new_word_counter[w] -= freq\n        if new_word_counter[w] &lt;= 0:\n            del new_word_counter[w]\n\n        # 2. Subtract ALL old adjacent pairs for this word + remove old word from index.\n        for i in range(len(w) - 1):\n            pair = (w[i], w[i + 1])\n            updated_pair_counter[pair] -= freq\n            changed_pairs.add(pair)\n\n            s = pair_to_words.get(pair)\n            if s is not None:\n                s.discard(w)\n                if not s:\n                    del pair_to_words[pair]\n\n        # 3. Build merged word (greedy left-to-right, same as standard BPE).\n        new_word = get_new_word(w, target_pair, new_id)\n        new_word_counter[new_word] += freq\n\n        # 4. Add ALL new adjacent pairs for merged word + add merged word into index.\n        if len(new_word) &gt;= 2:\n            for i in range(len(new_word) - 1):\n                pair = (new_word[i], new_word[i + 1])\n                updated_pair_counter[pair] += freq\n                changed_pairs.add(pair)\n                pair_to_words.setdefault(pair, set()).add(new_word)\n\n    # 5. Push updated frequencies for changed pairs into heap (skip non-positive).\n    if pair_heap is not None:\n        for p in changed_pairs:\n            f = updated_pair_counter.get(p, 0)\n            if f &gt; 0:\n                heapq.heappush(pair_heap, HeapItem(-f, (vocab[p[0]], vocab[p[1]]), p))\n\n    return dict(new_word_counter), updated_pair_counter, pair_heap, pair_to_words\n\n有了这两个优化的点，BPE的训练速度可以大大的提升，\n\n\nComparison of BPE Versions\n\n\n\n\n\n\n\n\n版本\n找最频繁 pair\n更新计数\n训练主循环瓶颈\n\n\n\n\nv0\n每轮扫一遍 pairs (\\(\\mathcal{O}(\\#pairs)\\))\n每轮重算\n很慢\n\n\nheap\npop \\(\\mathcal{O}(\\log \\#pairs)\\)\n仍可能扫很多\n更快\n\n\nheap + pair_to_words\npop \\(\\mathcal{O}(\\log \\#pairs)\\)\n只更新受影响的 words/pairs\n明显更快"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#train-bpe",
    "href": "posts/CS336/Ass01/ass01.html#train-bpe",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.6 Train BPE",
    "text": "2.6 Train BPE\n将上面的实现，替换成我们最新的实现后，我们就可以实现BPE的算法：\n\n\ncs336_basics/tokenizer/tokenizer.py\n\ndef train_bpe(\n    input_path: str | os.PathLike,\n    vocab_size: int,\n    special_tokens: list[str] | None = None,\n    verbose: bool = False,\n    **kwargs,\n) -&gt; tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n    num_merges = vocab_size - 256 - (len(special_tokens) if special_tokens else 0)\n    vocab: dict[int, bytes] = init_vocab(special_tokens)\n    merges: list[tuple[bytes, bytes]] = []\n\n    # 1. Pre-tokenization\n    # 1.1 Find chunk boundaries\n    with open(input_path, \"rb\") as f:\n        chunk_boundaries = find_chunk_boundaries(\n            f, desired_num_chunks=kwargs.get(\"desired_num_chunks\", NUM_PROCESSES), split_special_token=b\"\\n\"\n        )\n\n    if verbose:\n        print_color(f\"Identified {len(chunk_boundaries) - 1} chunks for pre-tokenization.\")\n\n    # 1.2 Count word frequencies across chunks using multiprocessing\n    manager = Manager()\n    queue = manager.Queue()\n    processes: list[Process] = []\n\n    for start, end in zip(chunk_boundaries[:-1], chunk_boundaries[1:]):\n        p = Process(\n            target=pre_tokenize_string_worker,\n            args=(input_path, special_tokens, queue, start, end, False),\n        )\n        processes.append(p)\n        p.start()\n\n    if verbose:\n        print_color(\"Pre-tokenization processes completed. Aggregating results...\")\n\n    word_counter = Counter()\n    for _ in range(len(processes)):\n        try:\n            partial_counter = queue.get(timeout=10)\n            word_counter.update(partial_counter)\n        except Empty:\n            continue\n    for p in processes:\n        p.join()\n\n    if verbose:\n        print_color(f\"Completed pre-tokenization. Vocabulary size: {len(word_counter)} unique tokens.\")\n\n    pairs_counter = Counter()\n    pair_to_words: dict[tuple[int, int], set[tuple[int, ...]]] = defaultdict(set)\n    for word in word_counter:\n        for i in range(len(word) - 1):\n            pair = (word[i], word[i + 1])\n            pair_to_words[pair].add(word)\n            pairs_counter[pair] += word_counter[word]\n\n    # 2. BPE Core Loop\n    pair_heap = build_pair_heap(pairs_counter, vocab)\n\n    for i in trange(num_merges):\n        most_frequent_pair = pop_most_frequent_pair(pair_heap, pairs_counter)\n        new_id = update_vocab(vocab, most_frequent_pair)\n\n        word_counter, pairs_counter, pair_heap, pair_to_words = merge_pairs_with_heap_index(\n            word_counter, pairs_counter, most_frequent_pair, new_id, vocab, pair_heap, pair_to_words\n        )\n\n        merges.append((vocab[most_frequent_pair[0]], vocab[most_frequent_pair[1]]))\n\n    if kwargs.get(\"save_path\"):\n        save_vocab_and_merges(vocab, merges, kwargs[\"save_path\"])\n        with open(os.path.join(kwargs[\"save_path\"], \"special_tokens.txt\"), \"w\", encoding=\"utf-8\") as f:\n            if special_tokens:\n                for token in special_tokens:\n                    f.write(f\"{token}\\n\")\n\n    return vocab, merges\n\n运行一下测试代码\n\n\ntests/adapters.py\n\nfrom cs336_basics.tokenizer.tokenizer import train_bpe\n\nreturn train_bpe(\n    input_path=input_path,\n    vocab_size=vocab_size,\n    special_tokens=special_tokens,\n    **kwargs,\n)\n\nuv run pytest tests/test_train_bpe.py\n我们看到，所有的测试都通过了！"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#bpe-on-tinystory",
    "href": "posts/CS336/Ass01/ass01.html#bpe-on-tinystory",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.7 BPE on TinyStory",
    "text": "2.7 BPE on TinyStory\n在TinyStory上训练BPE，\nuv run python ./train_bpe.py\n只需要不到2mins。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#bpe-tokenizer",
    "href": "posts/CS336/Ass01/ass01.html#bpe-tokenizer",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.8 BPE Tokenizer",
    "text": "2.8 BPE Tokenizer\n有了vocab merges 我们可以实现一个BPE Tokenizer\n\n\ncs336_basics/tokenizer/tokenizer.py\n\n\nclass BPETokenizer:\n    def __init__(\n        self,\n        vocab: dict[int, bytes],\n        merges: list[tuple[bytes, bytes]],\n        special_tokens: list[str] | None = None,\n    ):\n        self.vocab = vocab\n        self.merges = merges\n        self.special_tokens = special_tokens if special_tokens else []\n        self.special_tokens_bytes = [t.encode(\"utf-8\") for t in self.special_tokens]\n        self.special_set = set(self.special_tokens_bytes)\n\n        self.vocab_inv = {v: k for k, v in self.vocab.items()}\n\n        rank: dict[tuple[int, int], int] = {}\n        merge_to_new_id: dict[tuple[int, int], int] = {}\n\n        for r, (a_bytes, b_bytes) in enumerate(self.merges):\n            a_id = self.vocab_inv.get(a_bytes)\n            b_id = self.vocab_inv.get(b_bytes)\n            # The merged token should be present in vocab; if not, skip this merge rule.\n            new_id = self.vocab_inv.get(a_bytes + b_bytes)\n            if a_id is None or b_id is None or new_id is None:\n                continue\n            pair = (a_id, b_id)\n            rank[pair] = r\n            merge_to_new_id[pair] = new_id\n\n        self.rank = rank\n        self.merge_to_new_id = merge_to_new_id\n\n        self.eos_token_id = self.vocab_inv.get(b\"&lt;|endoftext|&gt;\", None)\n    def encode(self):\n        pass \n    \n    def encode_iterable(self):\n        pass\n    \n    def decode(self):\n        tokens = b\"\".join(self.vocab.get(i, b\"\\xef\\xbf\\xbd\") for i in ids)\n        return tokens.decode(\"utf-8\", errors=\"replace\")\n    @classmethod\n    def from_files(\n        cls, vocab_filepath: str, merges_filepath: str, special_tokens: list[str] | str | None = None\n    ) -&gt; \"BPETokenizer\":\n        with open(vocab_filepath) as vf:\n            vocab_data = json.load(vf)\n            vocab = {int(i): bytes(v, \"latin1\") for v, i in vocab_data.items()}\n\n        merges = []\n        with open(merges_filepath) as mf:\n            # Skip the first line (header)\n            next(mf)\n            for line in mf:\n                if line.strip() and not line.startswith(\"#\"):\n                    parts = line.strip().split()\n                    if len(parts) == 2:\n                        merges.append((bytes(parts[0], \"latin1\"), bytes(parts[1], \"latin1\")))\n\n        if isinstance(special_tokens, str):\n            with open(special_tokens, encoding=\"utf-8\") as stf:\n                special_tokens_list = [line.strip() for line in stf if line.strip()]\n        elif isinstance(special_tokens, list):\n            special_tokens_list = special_tokens\n        else:\n            special_tokens_list = []\n\n        return cls(vocab, merges, special_tokens_list)\n\nBPE Tokenizer 主要实现三个功能：\n\nencode：把字符串编码成 token IDs 列表\nencode_iterable：把字符串编码成 token IDs 生成器\ndecode：把 token IDs 列表解码成字符串\n\n在这里，我们主要介绍 encode 的实现；相比之下，decode 的逻辑更直接：把 token ids 依次映射回对应的 bytes，拼接成完整的字节序列，再用 UTF-8 解码得到字符串。需要特别注意的是 b\"\\xef\\xbf\\xbd\" 的处理——它是 Unicode U+FFFD（replacement character，“�”）在 UTF-8 下的字节表示。我们在 decode 时会对每个 i（token id）执行一次查表 self.vocab.get(i, ...)：\n\n如果 i 能在词表中找到，就取出对应的 bytes；\n如果找不到（例如遇到非法/越界的 id，或词表不完整），就用 b”” 作为兜底。 这样做的好处是：即使输入 ids 中混入了未知 token，decode 也不会崩溃，而是用“�”显式标记无法还原的部分，保证整个解码过程始终可运行、输出始终是一个合法字符串。\n\n\n2.8.1 Encode in BPETokenizer\ndef encode(self, text: str) -&gt; list[int]:\n    def merge_one_pretoken(ids: list[int]) -&gt; list[int]:\n        pass \n    \n    # step 1\n    byte_tokens = self._pre_tokenize(text)\n    \n    # step 2\n    token_ids: list[int] = []\n    for btok in byte_tokens:\n        if btok in self.special_set:\n            token_ids.append(self.vocab_inv[btok])\n        else:\n            ids = [self.vocab_inv[bytes([b])] for b in btok]\n            token_ids.extend(merge_one_pretoken(ids))\n\n    return token_ids\n这个encode主要做两个事情：\n\nPre-tokenization：先粗粒度切分文本\n对每个 pre-token 做 BPE merge\n\n第一步和我们之前实现的一样，对于第二步，主要的实现方法在 merge_one_pretoken 中实现。 在这个函数中，我们通过Heap和Double Linked List 来高效实现这个Encode。\n首先，我们用数组来模拟双向链表：\nprev = [-1] * n\nnxt  = [-1] * n\nalive = [True] * n\n合并时并不真的 del 掉元素，而是：\n\n标记被吞掉的节点 alive[j] = False\n调整指针 nxt[i] = nxt[j]、prev[nxt[j]] = i\n\n这样每次合并都是 \\(\\mathcal{O}(1)\\) 的时间复杂度。\n接下来，我们用一个min heap，来获取我们最先要实现merge的pair，也就是在训练阶段，出现频率最高的pair。\n堆里存 (rank, i)，表示当前位置 i 与其右邻居 nxt[i] 的 pair 在 merge 规则中的优先级（rank 越小越先合并）。 每次取出最小 rank 的候选，做一次合并，然后只需要重新检查局部的两个 pair：\n\n(prev[i], i)\n(i, nxt[i])\n\nheap: list[tuple[int, int]] = []\n\ndef push_if_valid(i: int):\n    cur_r = None\n    j = nxt[i]\n    if j == -1 or not alive[i] or not alive[j]:\n        cur_r = None\n    else:\n        cur_r = self.rank.get((ids[i], ids[j]))\n\n    if cur_r is not None:\n        heapq.heappush(heap, (cur_r, i))\n\nfor i in range(n):\n    push_if_valid(i)\n与之前的heap一样，heap里面的内容会 “过期”： 因为合并会改变邻接关系，堆中旧条目会过期，所以每次 pop 出来都要验证,\n接下来就是遍历这个heap，如果这个heap不是空的，我们就弹出，并且验证：\n这段 while heap: 是整个 merge_one_pretoken 的核心：堆里维护“当前可合并的相邻 pair”，每次取出 rank 最小（最优先） 的候选进行合并，并只更新合并点附近的候选。\nwhile heap:  # 只要还有候选 pair，就继续尝试合并\n    r, i = heapq.heappop(heap)  # 取出当前 rank 最小的候选：(rank, 左端点位置 i)\n    j = nxt[i]  # 右端点位置 j 是 i 在链表中的后继\n    if j == -1 or not alive[i] or not alive[j]:  # i/j 无效或 i 已到尾部：这是过期候选\n        continue  # 跳过，继续处理下一个堆元素\n\n    # stale check：堆里的记录可能已过期（邻居关系/ids 已改变），需要重新验证\n    pair = (ids[i], ids[j])  # 当前时刻 i 和 j 对应的 token id 组成的相邻 pair\n    cur_r = self.rank.get(pair)  # 查询这个 pair 在 merge 规则中的 rank（不可合并则为 None）\n    if cur_r is None or cur_r != r:  # 现在不可合并，或 rank 已不匹配：说明堆元素过期\n        continue  # 跳过该候选\n\n    # 执行合并：把 (ids[i], ids[j]) 合成一个新 token，并写回到位置 i\n    new_id = self.merge_to_new_id.get(pair)  # 查找该 pair 合并后的 token id\n    if new_id is None:  # 理论上不该发生（rank 有但映射没建好），当作过期/异常处理\n        continue  # 跳过\n    ids[i] = new_id  # 用新 token id 覆盖左端点 i（i 成为合并后的节点）\n\n    # 从链表中删除 j：j 被 i 吞掉了\n    alive[j] = False  # 标记 j 节点被删除\n    nj = nxt[j]  # 记住 j 的后继节点\n    nxt[i] = nj  # 让 i 直接指向 nj（跳过 j）\n    if nj != -1:  # 如果 nj 存在\n        prev[nj] = i  # 更新 nj 的前驱为 i，保持链表一致\n\n    # 局部更新：合并只会影响 i 附近的两个相邻 pair\n    pi = prev[i]  # i 的前驱位置\n    if pi != -1:  # 如果前驱存在\n        push_if_valid(pi)  # (pi, i) 这个 pair 可能变得可合并或 rank 改变\n    push_if_valid(i)  # (i, nxt[i]) 这个 pair 也可能变得可合并或 rank 改变\n最后我们只需要把链表结构还原成最终的token序列即可：\n在 BPE 合并阶段，我们用 prev / nxt / alive 维护了一个“数组模拟的双向链表”。合并时并不会真的删除 ids 里的元素，而是把被吞掉的位置标记为 alive=False，并通过 nxt 跳过它们。\n因此在所有合并完成后，需要把“还活着的节点”按顺序重新收集成一个紧凑的输出序列：\nout: list[int] = []          # 最终合并后的 token id 序列\nk = 0                        # 从链表头（位置 0）开始遍历\nwhile k != -1:               # -1 表示到达链表末尾\n    if alive[k]:             # 如果该位置还没有被合并删除\n        out.append(ids[k])   # 把当前位置的 token id 加入输出\n    k = nxt[k]               # 跳到下一个“仍在链表中的”位置\n至此，我们以及完成了BPE阶段的所有的内容，接下来就是要训练，并存储我们预先Token好的内容\nuv run pytest tests/test_tokenizer.py"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#tokenize-and-save-file",
    "href": "posts/CS336/Ass01/ass01.html#tokenize-and-save-file",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.9 Tokenize and Save File",
    "text": "2.9 Tokenize and Save File\n有了 tokenizer.encode() 之后，我们通常会希望把一整个文本文件编码成 紧凑的二进制（.bin），方便后续训练时用 np.memmap 之类的方式高效加载，而不是每次都重新分词。\n下面这段函数做的事情很简单：按行读取文本 → 把每行编码成 token ids → 用固定 dtype 写入二进制文件。\ndef encode_file_to_bin(tokenizer, text_path, out_bin_path, dtype=np.uint16):\n    total_bytes = os.path.getsize(text_path)\n\n    with open(text_path, encoding=\"utf-8\") as f_in, open(out_bin_path, \"wb\") as f_out:\n        p_bar = tqdm(total=total_bytes, desc=\"Encoding to binary\", unit=\"B\", unit_scale=True)\n\n        for line in f_in:\n            token_ids = tokenizer.encode(line)          # 1) 把一行文本编码成 token ids\n            arr = np.array(token_ids, dtype=dtype)      # 2) 转成 numpy 数组（更适合写二进制）\n            arr.tofile(f_out)                           # 3) 直接以二进制写入 .bin 文件\n\n            p_bar.update(len(line.encode(\"utf-8\")))     \n根据我们的实现，只需要不到30mins就可以训练完BPE。\n在这里 .bin 里不保存行边界/样本边界 训练时把它当作一个长序列做 next-token prediction（GPT 风格），用 block sampling；\n\n\nQuestion 1: 为什么用uint16就可以了呢？\n\n\n应为在BPE的训练阶段，我们将vocab size设置为 10,000 或者 32,000 远远小于 uint16的最大值 65,535因此用uint16是安全的。\n\n\n我们通过运行以下代码来完成TinyStory的Tokenization与保存：\nuv run python ./train_bpe.py\n在训练完之后，我们可以到的一下的directory\ndatasets/\n└── tiny_stories/\n    ├── eval.bin\n    ├── merges.txt\n    ├── special_tokens.txt\n    ├── train.bin\n    └── vocab.json\n\n\n\n\n\n\nNote\n\n\n\n如果大家不想训练Tokenizer，可以直接下载我训练完的文件，只需在 asssignment1-basics 的目录下运行以下两行：\npip install -U huggingface_hub\nhf download YuYangZhang/TinyStory-Tokenized  --repo-type dataset --local-dir datasets/tiny_stories\n这段代码会自动从hugging face上下载数据，并且保存至以上的directory。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#part-01-summary",
    "href": "posts/CS336/Ass01/ass01.html#part-01-summary",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "2.10 Part 01 Summary",
    "text": "2.10 Part 01 Summary\n总的来说，Part 01 相比大家更期待的「LLM 训练与模型结构」部分，更偏向工程实现与性能优化：通过合适的数据结构与算法设计（例如 heap、索引表、双向链表、并行统计等），我们可以在不改变算法(Algorithm 1) 的前提下，把 BPE 的训练与推理速度提升一个数量级。\n很多读者（包括我自己）会觉得这一部分“又长又绕”，主要原因往往不是内容本身有多难，而是对这些工程细节还不够熟悉：一旦把数据结构的作用、更新范围、以及 stale check 的逻辑串起来，整体会清晰很多。所以如果你第一次读完仍然觉得有点乱，这是非常正常的—— 请不要气馁 , 精彩的部分还正要开始！\nTokenization 是训练 LLM 的第一步。真正理解这部分，会直接帮助你在后续更顺畅地掌握：\n\n如何进行数据加载与采样（例如 .bin + memmap）\n如何高效地 encode / decode\n以及在更进阶的话题里，如何围绕 tokenizer 与序列表示去扩展模型的 context length\n\n下一部分我们将把 tokenizer 生成的二进制数据接入训练 pipeline，进入真正的 model training 环节。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#linear-module",
    "href": "posts/CS336/Ass01/ass01.html#linear-module",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.1 Linear Module",
    "text": "3.1 Linear Module\nLinear Module 基本是所有神经网络的起始点，它的定义如下:\n\\[\ny = Wx\n\\tag{1}\\]\n其中 \\(W \\in \\mathbb{R}^{d_{\\text{out}}  \\times d_{\\text{in}}}\\) , \\(x \\in \\mathbb{R}^{d_{\\text{in}} \\times 1}\\) , \\(y \\in \\mathbb{R}^{d_{\\text{out}} \\times 1}\\)\n\n\nNOTE\n\n\n在这里，我们实现的 Linear Module 与任务中要求的略有不同， 主要体现在以下两点：\n\n我们将 weight 的 shape 设为 (in_features, out_features)， 这样在 forward 的时候，可以直接使用 @ 运算符进行矩阵乘法，代码更简洁。\n我们将 bias 设为可选项，默认不使用 bias，这样可以更好地模拟 Transformer 中的 Linear Layer。\n\n\n\nclass Linear(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n        bias: bool = False,\n    ):\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.weight = nn.Parameter(torch.empty((in_features, out_features), device=device, dtype=dtype))\n        self.bias = nn.Parameter(torch.empty(out_features, device=device, dtype=dtype)) if bias else None\n        self._init_weight()\n\n    def forward(self, x):\n        o = x @ self.weight\n\n        if self.bias is not None:\n            o = o + self.bias\n\n        return o\n    \n    def _init_weight(self):\n        mean = 0.0\n        std = 1.0 / (2 * (self.in_features + self.out_features) ** 0.5)\n        torch.nn.init.trunc_normal_(self.weight, mean=mean, std=std, a=-3 * std, b=3 * std)\n其中 _init_weight() 是初始化的方法， 在Assignment 1 中为：\n\\[\n\\mathcal{N}\\left( \\mu = 0, \\sigma^{2}=\\frac{2}{d_{\\text{in}} + d_{\\text{out}}} \\right)\n\\quad  \\text{truncated at}  [-3\\sigma, 3\\sigma ]\n\\]\n这种初始化的方式是最常见的，也是比较robust的，当然，大家还可以尝试不同的初始化的方式, 例如Xavier-initialization， Kaiming-initialization等。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#embedding-model",
    "href": "posts/CS336/Ass01/ass01.html#embedding-model",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.2 Embedding Model",
    "text": "3.2 Embedding Model\n记得我们在前面第一章节，实现了BPE的Tokenization，回顾一下，\n\n\nTL;DR: BPE Tokenization\n\n\nTokenization的步骤就是把文字，转化成一个个的IDs。 但是这个IDs是不能被模型处理的，我们需要将其转化成一个个的Dense Vector，这个就是所谓的 Embedding。\n\n\nEmbedding 的数学定义如下： \\[\n\\text{Embedding}(x) = W_{e}[x]\n\\tag{2}\\]\n其中 \\(W_{e} \\in \\mathbb{R}^{V \\times d_{\\text{model}}}\\) 是 Embedding 矩阵，\\(V\\) 是词汇表的大小，\\(d_{\\text{model}}\\) 是模型的隐藏维度，\\(x \\in \\mathbb{N}^{B \\times L}\\) 是输入的token IDs， \\(B\\) 是batch size，\\(L\\)是序列长度。\n代码实现如下：\nclass Embedding(nn.Module):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n\n        self.weight = nn.Parameter(torch.empty((num_embeddings, embedding_dim), device=device, dtype=dtype))\n\n        self._init_weight()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        B, L = x.shape  # x: (B, L)\n        out = x.reshape(-1)  # (B*L,)\n        out = self.weight.index_select(0, out)  # (B*L, D)\n        out = out.reshape(B, L, self.embedding_dim)  # (B, L, D)\n\n        return out\n很简单的也很直观，它的权重初始化的方式为：\n\\[\n\\mathcal{N}\\left( \\mu = 0, \\sigma^{2}=1 \\right)\n\\quad  \\text{truncated at}  [-3, 3]\n\\]\n\n\nNOTE\n\n\n其实在forward中，我们只需要使用 self.weight[x] 这一行代码，就可以实现 Embedding 的功能， 但是为了更清晰地展示 Embedding 的工作原理，我们使用了 index_select() 来实现。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#rms-norm",
    "href": "posts/CS336/Ass01/ass01.html#rms-norm",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.3 RMS-Norm",
    "text": "3.3 RMS-Norm\n在现代的Language Model中，常见的Normalization的方法是 RMS-Norm(Zhang and Sennrich 2019)， 其数学定义如下：\n\\[\n\\begin{split}\n\\text{RMSNorm}(a_{i}) &= \\frac{a_{i}}{\\text{RMS}(a)} g_{i} \\\\\n\\text{where} \\quad  \\text{RMS}(a) &= \\sqrt{ \\frac{1}{d_{\\text{model}}} \\sum_{i=1}^{d_{\\text{model}}}a_{i}^{2}  + \\epsilon}\n\\end{split}\n\\]\n其中 \\(g\\) 是可学习的缩放参数，它的维度与输入 \\(a\\) 相同，\\(\\epsilon\\) 是一个很小的数值，防止除以0。\n实现RMS-Norm的方式也很简单，不过有一个需要注意的点就是：如果我们用了Mixed Precision Training，当用 sqrt() 时， 可能会导致Underflow，为了避免这一点，在训练的时候，我们需要先将 activation upcast到 float32， 结束的时候再返回原来的数据类型。具体的请看代码：\n\n\ncs336_basics/modules/norm.py\n\nclass RMSNorm(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        eps: float = 1e-5,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n\n        self.d_model = d_model\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.ones(d_model, device=device, dtype=dtype))\n\n    def _rms(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        input_dtype = x.dtype\n        x = x.to(torch.float32)\n\n        rms = self._rms(x)\n        x_normed = x / rms\n\n        return (x_normed * self.weight).to(input_dtype)\n\nNormalization的位置也是很有讲究的，在现代的LM中，通常用Pre-Norm，这一部分，等我们介绍完了所有的模块之后再来介绍。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#pointwise-feed-forward-network",
    "href": "posts/CS336/Ass01/ass01.html#pointwise-feed-forward-network",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.4 PointWise Feed Forward Network",
    "text": "3.4 PointWise Feed Forward Network\n在原始 Transformer (Vaswani et al. 2023)里，FFN 是一个非常经典的两层结构：Linear → ReLU → Linear，并且中间隐层维度通常取 d_ff = 4 * d_model。但到了现代大语言模型（例如 Llama 3、Qwen 2.5），FFN 的设计出现了两个几乎“标配”的变化：\n\n换激活函数\n引入门控（gating）机制。\n\n一个典型代表就是 SwiGLU：它把 SiLU/Swish 的平滑激活和 GLU 的门控相乘结合起来，并且很多实现会像 PaLM、LLaMA 一样去掉线性层 bias（更简洁、也更贴近主流训练配方）。\n先看 SiLU（也常叫 Swish），定义很简单：\n\\[\n\\mathrm{SiLU}(x)=x\\cdot\\sigma(x)=\\frac{x}{1+e^{-x}}\n\\tag{3}\\]\n它和 ReLU 一样能提供非线性，但在 0 附近是平滑的，梯度行为更连续。再看 GLU，它用一个 sigmoid 分支充当“门”，去调节另一条线性分支：\n\\[\n\\mathrm{GLU}(x, W_1, W_2)=\\sigma(W_1x)\\odot (W_2x)\n\\tag{4}\\]\n直觉上，这种门控能给梯度提供一条更“线性”的通路，同时保留非线性表达能力。把两者拼起来就是 SwiGLU在 FFN 中的写法： \\[\n\\mathrm{FFN}(x)=W_2\\big(\\mathrm{SiLU}(W_1x)\\odot (W_3x)\\big)\n\\tag{5}\\]\n其中\\(x\\in\\mathbb{R}^{d_\\text{model}}\\)，\\(W_1,W_3\\in\\mathbb{R}^{d_\\text{ff}\\times d_\\text{model}}\\), \\(W_2\\in\\mathbb{R}^{d_\\text{model}\\times d_\\text{ff}}\\)。实践里常见的经验设定是 \\(d_\\text{ff}=\\frac{8}{3}d_\\text{model}\\)\n也就是说，相比早期的 4x，现代 LLM 经常用一个更“性价比”更好的宽度配合门控结构。Shazeer (Shazeer 2020) 的实验也表明，SwiGLU 往往能在语言建模任务上优于 ReLU 或仅 SiLU（无门控）的基线——当然，最终还是要回到实验：在后续对比不同 FFN 变体时，你会更直观地看到这些设计在 loss、收敛速度与最终指标上的差异。\n\n\n\n\n\n\nFigure 3: 不同激活函数样子\n\n\n\n代码的实现还是很简单的：\n\n\ncs336_basics/modules/ffn.py\n\ndef silu(x: torch.Tensor) -&gt; torch.Tensor:\n    return x * torch.sigmoid(x)\n\n\nclass FFN(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        d_ff: int,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n\n        from cs336_basics.modules.linear import Linear\n\n        self.up = Linear(d_model, d_ff, device=device, dtype=dtype)\n        self.down = Linear(d_ff, d_model, device=device, dtype=dtype)\n        self.gate = Linear(d_model, d_ff, device=device, dtype=dtype)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.down(silu(self.up(x)) * self.gate(x))"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#rope",
    "href": "posts/CS336/Ass01/ass01.html#rope",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.5 RoPE",
    "text": "3.5 RoPE\nTransformer 本身对序列的顺序并不敏感，因此需要把位置信息注入到注意力机制里。除了常见的绝对位置编码（absolute PE），现代 LLM 更常用的一类方法是 Rotary Position Embeddings（RoPE) (Su et al. 2023)：它不是把位置向量“加到 embedding 上”，而是对 Q/K 向量做按维度成对的旋转，从而让注意力天然具备相对位置信息。\nRoPE的思想也很简单：对第 \\(i\\) 个 token 的 query： \\[\nq^{(i)} = W_q x^{(i)} \\in \\mathbb{R}^d\n\\tag{6}\\]\nRoPE 会乘上一个位置相关的旋转矩阵 \\(R_{i}\\): \\[\nq'^{(i)} = R_i q^{(i)} = R_i W_q x^{(i)}\n\\tag{7}\\]\n其中 \\(R_i\\) 会把向量按维度两两分组：\\((q_{1},q_{2}), (q_{3},q_{4}), \\dots\\)，把每一对看作一个 2D 向量，在平面里旋转一个角度 \\(\\theta_{i,k}\\)。\n\n\n\n\n\n\nFigure 4: RoPE 旋转示意图：对每一对维度 \\((q_{2k-1}, q_{2k})\\)，按位置 \\(i\\) 旋转角度 \\(\\theta_{i,k}\\)。\n\n\n\n接下来，我们来看如何定义旋转角度 \\(\\theta_{i,k}\\)，以及如何构建旋转矩阵 \\(R_i\\)。\n\n定义旋转角度 \\(\\theta_{i,k}\\) \n根据RoPE(Su et al. 2023)的设定， 对第 \\(k\\) 对维度 \\(k \\in \\{1,\\dots, d/2\\}\\)，旋转角度定义为： \\[\n\\theta_{i,k} = i \\cdot \\Theta^{-\\frac{2k-2}{d}}\n\\tag{8}\\]\n这里 \\(\\Theta\\) 是一个常数我们通常把 \\(\\Theta\\) 设为 10,000，这样第 \\(k\\) 对维度的频率是 \\(\\frac{1}{10000^{(2k-2)/d}}\\)，和 Transformer 绝对位置编码里的频率设计是一致的。\ninv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n直觉上：\n\n不同维度对应不同“旋转频率”（像一组不同波长的正弦/余弦）\n位置越靠后，旋转角度越大, 用于编码更长距离的相对位置关系\n最终让注意力可以通过 Q/K 的相对旋转，编码相对位置信息\n\n\n定义旋转块 \\(R^i_k\\) \n其中，每一对维度 \\((q_{2k-1}, q_{2k})\\) 对应一个 \\(2\\times 2\\) 旋转块： \\[\nR^i_k =\n\\begin{bmatrix}\n\\cos(\\theta_{i,k}) & -\\sin(\\theta_{i,k}) \\\\\n\\sin(\\theta_{i,k}) & \\cos(\\theta_{i,k})\n\\end{bmatrix}\n\\tag{9}\\]\n\n整体旋转矩阵 \\(R_i\\) \n整体 \\(R_i\\) 是一个 \\(d\\times d\\) 的块对角矩阵，由 \\(d/2\\) 个\\(2\\times 2\\) 块组成（其它位置为 0）。数学上写成：\n\\[\nR_i=\n\\begin{bmatrix}\nR^i_1 & 0      & 0      & \\cdots & 0 \\\\\n0     & R^i_2  & 0      & \\cdots & 0 \\\\\n0     & 0      & R^i_3  & \\cdots & 0 \\\\\n\\vdots& \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0     & 0      & 0      & \\cdots & R^i_{d/2}\n\\end{bmatrix}\n\\tag{10}\\]\n\n这样，对于第 \\(j\\) 个 token 的 key 向量 \\(k^{(j)}\\)，RoPE 也会做类似的旋转：\n\\[\nk'^{(j)} = R_j k^{(j)}\n\\tag{11}\\]\n这样在计算注意力分数 \\(q'^{(i)} \\cdot k'^{(j)}\\) 时，位置差异会以“相对旋转”的形式体现出来，这也是 RoPE 在长上下文建模中非常常用的原因之一。\n\\[\nq'^{(i)} \\cdot k'^{(j)} = (R_i q^{(i)}) \\cdot (R_j k^{(j)}) = q^{(i)} \\cdot (R_i^T R_j k^{(j)})\n\\tag{12}\\]\n\n3.5.1 RoPE 的实现细节\nRoPE 层没有可学习参数。为了效率，通常会：\n\n预计算所有 \\(\\cos(\\theta_{i,k})\\) 与 \\(\\sin(\\theta_{i,k})\\)\n作为 buffer 缓存在模块里，而不是 nn.Parameter（因为它们是固定的）\n甚至可以让所有 Transformer 层共享同一个 RoPE 模块（跨层复用缓存）\n\n实现上常用：\n\nself.register_buffer(..., persistent=False) 来保存预计算好的 sin/cos（不进 state_dict 或不作为可训练参数）\n只要序列长度/维度不变，这些值可以在不同 batch、不同 layer 间复用\n\n不过，在实际实现 RoPE 旋转时，我们并不需要显式构建大块对角矩阵 \\(R_i\\)，而是把向量按 2 维一组配对 \\((x_{2k-1}, x_{2k})\\) ，对每一组做一个平面旋转：\n\\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1 \\cos(m\\theta_{1}) - x_2 \\sin(m\\theta_{1})\\\\\nx_2 \\cos(m\\theta_{1}) + x_1 \\sin(m\\theta_{1})\\\\\nx_3 \\cos(m\\theta_{2}) - x_4 \\sin(m\\theta_{2})\\\\\nx_4 \\cos(m\\theta_{2}) + x_3 \\sin(m\\theta_{2})\\\\\n\\vdots\\\\\nx_{d-1} \\cos(m\\theta_{d/2}) - x_d \\sin(m\\theta_{d/2})\\\\\nx_d \\cos(m\\theta_{d/2}) + x_{d-1} \\sin(m\\theta_{d/2})\n\\end{pmatrix}\n\\]\n代码的实现也很简单：\n\n\ncs336_basics/modules/rope.py\n\nclass RoPEEmbedding(nn.Module):\n    def __init__(\n        self,\n        theta: float,\n        d_k: int,\n        max_seq_len: int,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n\n        self.theta = theta\n        self.d_k = d_k\n        self.max_seq_len = max_seq_len\n\n        inv_freq = 1.0 / (theta ** (torch.arange(0, d_k, 2, device=device, dtype=torch.float32) / d_k)) \n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n    def _rotate_half(self, x):\n        x = einops.rearrange(x, \"... (d j) -&gt; ... d j\", j=2) \n        x1, x2 = x.unbind(dim=-1) \n        return einops.rearrange(torch.stack((-x2, x1), dim=-1), \"... d j-&gt; ... (d j)\") \n\n    def forward(self, x: torch.Tensor, token_positions: int | None = None) -&gt; torch.Tensor:\n        if token_positions is None:\n            seq_len = x.shape[-2]\n            token_positions = torch.arange(seq_len, device=x.device)\n            token_positions = token_positions.unsqueeze(0)\n\n        theta = torch.einsum(\"...i , j -&gt; ... i j\", token_positions, self.inv_freq)\n        cos = torch.cos(theta).repeat_interleave(2, dim=-1) \n        sin = torch.sin(theta).repeat_interleave(2, dim=-1) \n\n        x_rotated = (x * cos) + (self._rotate_half(x) * sin)\n        return x_rotated\n\n\n\nNOTE: RoPE in Open Source Project\n\n\n在阅读其他LLM的源代码时，我们可能会碰到以下形式的实现：\ndef rotate_half(x: torch.Tensor) -&gt; torch.Tensor:\n    x1, x2 = x.chunk(2, dim=-1) \n    return torch.cat((-x2, x1), dim=-1) \n\n\ndef forward(self, x: torch.Tensor, token_positions: int | None = None) -&gt; torch.Tensor:\n    if token_positions is None:\n        seq_len = x.shape[-2]\n        token_positions = torch.arange(seq_len, device=x.device)\n        token_positions = token_positions.unsqueeze(0)\n\n    theta = torch.einsum(\"...i , j -&gt; ... i j\", token_positions, self.inv_freq)\n    theta = torch.cat([theta, theta], dim=-1) \n    cos = torch.cos(theta)\n    sin = torch.sin(theta)\n    x_rotated = (x * cos) + (self._rotate_half(x) * sin)\n    return x_rotated\n这段实现看起来和论文里的 \\(R_i\\) 块对角矩阵公式不一样：论文写的是“每两维一组做 2D 旋转”，而这里把向量拆成两半 (x1, x2)，再用 rotate_half 做拼接，像是在“整体换位”。\n\\[\n\\operatorname{rotate\\_half}(x) = (-x_{\\text{second half}},\\ x_{\\text{first half}})\n\\]\n这对应的就是“二维旋转里那个把 \\((a,b)\\) 变成 \\((-b,a)\\)”的操作，只不过它把配对方式从论文常见的“(1,2)(3,4)…邻接配对”，换成了“(前半, 后半) 的配对”。 只要 cos/sin 的重复方式 和 rotate 的配对方式 一致，配对是邻接还是前后半，本质都在做同一个 block-rotation。\n这两种只是坐标重排（permutation）不同：存在一个置换矩阵 P，使得\n\\[\nR_{\\text{half-split}} = P^\\top R_{\\text{adjacent}} P\n\\]\n几何上仍然是对每个 2D 子空间做旋转，所以一样可行。\n\n\n当然，除了上面的这种形式，我们还可以通过构造Complex Number的形式来完成Vector的旋转，在这里就不展开了。有兴趣的同学可以参考LLaMA的Inference Code。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#multi-headed-attention",
    "href": "posts/CS336/Ass01/ass01.html#multi-headed-attention",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.6 Multi-Headed Attention",
    "text": "3.6 Multi-Headed Attention\n接下来，我们来实现Transformer中，最重要也是相对比较复杂的部分，Attention，我们先来看看什么是Scaled Dot Product Attention\n\n3.6.1 Scaled Dot-Product Attention\n在 Transformer(Vaswani et al. 2023)中，最核心的计算之一就是 scaled dot-product attention。它可以看作：\n\n计算 query 和 key 的相似度（打分），\n把这些分数(logits)归一化成概率分布，\n最后用这个分布对 value 做加权求和。\n\n首先我们来看看如何将分数(logits)归一化为概率分布，在这里我们需要用到的就是 Softmax 函数\n\n3.6.1.1 Softmax Function\nSoftmax 的定义是：\n\\[\n\\mathrm{softmax}(v)_i=\\frac{\\exp(v_i)}{\\sum_{j=1}^{n}\\exp(v_j)}\n\\tag{13}\\]\n直觉上，softmax 会把任意实数向量变成一个非负、和为 1 的分布，因此常用于注意力里的“权重归一化”。 然而，直接算 softmax 有一个 常见数值问题：当 \\(v_i\\) 很大时，\\(\\exp(v_i)\\) 可能溢出变成 inf，从而导致 inf/inf = NaN。 仔细观察我们可以发现softmax 对所有输入同时加同一个常数不变。也就是说，对任意常数 \\(c\\)：\n\\[\n\\mathrm{softmax}(v)=\\mathrm{softmax}(v+c).\n\\tag{14}\\]\n证明很简单：分子分母都会多乘一个 \\(\\exp(c)\\)，会抵消掉。因此工程实现里通常取：\n\\[\nc=-\\max_i v_i,\n\\tag{15}\\]\n也就是把最大值减到 0，这样 \\(\\exp(\\cdot)\\) 的最大输入为 0，不会爆掉：\n\\[\n\\mathrm{softmax}(v)_i\n=\n\\frac{\\exp(v_i-\\max(v))}{\\sum_j \\exp(v_j-\\max(v))}.\n\\tag{16}\\]\n\n\ncs336_basics/modules/attention.py\n\ndef stable_softmax(\n    logits: torch.Tensor,\n    dim: int = -1,\n) -&gt; torch.Tensor:\n    max_logits = torch.max(logits, dim=dim, keepdim=True).values\n    exp_logits = torch.exp(logits - max_logits)\n    sum_exp_logits = torch.sum(exp_logits, dim=dim, keepdim=True)\n    softmax = exp_logits / sum_exp_logits\n    return softmax\n\n\n\n3.6.1.2 Scaled Dot Product Attention\n接着，我们来看Scaled Dot-Product Attention， 其数学定义为：\n\\[\n\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V,\n\\tag{17}\\]\n其中：\n\n\\(Q\\in\\mathbb{R}^{n\\times d_k}\\)：\\(n\\) 个 query\n\\(K\\in\\mathbb{R}^{m\\times d_k}\\)：\\(m\\) 个 key\n\\(V\\in\\mathbb{R}^{m\\times d_v}\\)：\\(m\\) 个 value（与 key 一一对应）\n\n这里的 \\(\\frac{1}{\\sqrt{d_k}}\\) 是一个非常重要的缩放项：当 \\(d_k\\) 变大时，点积的方差会变大，softmax 会更容易饱和（变得极端尖锐），缩放能让训练更稳定。\n\nWe suspect that for large values of \\(d_k\\), the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by \\(\\frac{1}{\\sqrt{d_k}}\\).  Attention Is All You Need, p. 4\n\n我们来看下面的图，展示了不同缩放因子对 softmax 分布的影响：\n\n\n\n\n\n\nFigure 5: Attention 中的缩放因子对 softmax 分布的影响， 当\\(d_k\\) 较大时，缩放能防止 softmax 过于尖锐。\n\n\n\n接下来，我们来看代码实现：\ndef scaled_dot_product_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k**0.5)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    attn_weights = stable_softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, value)\n    return output\n在这里，我们看到有一个Masking，这个mask的作用是什么呢？\n\n\n3.6.1.3 Causal Masking\n在很多场景下我们需要 mask（例如 causal LM 中不允许看未来 token，或 padding 位置不参与注意力）。mask 的形状是：\n\\[\nM =\n\\begin{bmatrix}\nTrue & False & \\cdots & False \\\\\nTrue & True  & \\cdots & False \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nTrue & True  & \\cdots & True\n\\end{bmatrix}\n\\]\n注意这里有个小约定（容易混淆）：\n\nTrue 表示允许 attend（信息流通）\n\nFalse 表示不允许 attend（需要屏蔽）\n\n\n\n\n\n\n\nFigure 6: Causal Mask 示例图\n\n\n\n计算上，我们不会真的删掉被屏蔽的 key/value（那样效率低），而是在 softmax 之前的打分矩阵上动手脚：对所有 mask 为 False 的位置加上 \\(-\\infty\\)：\n\\[\nS_{ij}=\n\\begin{cases}\nS_{ij}, & M_{ij}=\\mathrm{True}\\\\\n-\\infty, & M_{ij}=\\mathrm{False}\n\\end{cases}\n\\tag{18}\\]\n这样 softmax 后：\n\\[\n\\exp(-\\infty)=0\n\\tag{19}\\]\n对应权重严格为 0，被屏蔽的位置自然不会对输出产生贡献。最终输出是：\n\\[\n\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(S)V\n\\tag{20}\\]\n在语言模型里，token \\(i\\) 预测下一个词时不应该访问 \\(i\\) 之后的 token 表示，否则会泄露答案，训练目标会被“作弊”轻易完成。 实现上可以用\n\ntorch.triu（上三角）构造 False 区域，\n用广播比较 j &lt;= i。\n\n\n\ncs336_basics/modules/attention.py\n\n# 利用 torch.tril 创建因果掩码\ndef _create_causal_mask(self, seq_len: int, device: torch.device) -&gt; torch.Tensor:\n    mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n    return mask.unsqueeze(0).unsqueeze(0)\n\n# 利用广播比较创建因果掩码\ndef _create_causal_mask(self, seq_len: int, device: torch.device) -&gt; torch.Tensor:\n    positions = torch.arange(seq_len, device=device)\n    mask = positions.unsqueeze(0) &lt;= positions.unsqueeze(1)\n    return mask.unsqueeze(0).unsqueeze(0)\n\ndef scaled_dot_product_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    ...\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    ...\n\n\nAttention 的本质:\n是“相似度打分 + softmax 归一化 + 对 V 加权求和”。工程实现时要特别注意 softmax 的数值稳定性（减最大值）和 masking（softmax 前加 \\(-\\infty\\)），这两点几乎决定了注意力实现是否稳定、是否高效。\n\n\n\n\n3.6.2 Multi Headed Attention\n在实现了单个Attention模块之后，我们看看这些如何组合在一起，实现我们的Multi Headed Attention\n\nInstead of performing a single attention function with \\(d_{\\text{model}}\\)-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to \\(d_k\\), \\(d_k\\) and \\(d_v\\) dimensions, respectively.  Attention Is All You Need, p. 4\n\nMulti-head attention 的定义是：\n\\[\n\\mathrm{MultiHead}(Q,K,V) = \\mathrm{Concat}(\\mathrm{head}_1,\\dots,\\mathrm{head}_h)\n\\tag{21}\\]\n其中每个 head 都是一次标准 scaled dot-product attention (Equation 17)：\n\\[\n\\mathrm{head}_i = \\mathrm{Attention}(Q_i, K_i, V_i)\n\\tag{22}\\]\n这里的 \\(Q_i, K_i, V_i\\) 是把 \\(Q,K,V\\) 沿 embedding 维度切分得到的第 \\(i\\) 个 slice（每个 head 的维度是 \\(d_k\\) 或 \\(d_v\\)）。\n在 self-attention 场景中，\\(Q,K,V\\) 都由同一个输入 \\(x\\) 投影得到：\n\\[\n\\mathrm{MultiHeadSelfAttention}(x) = W_O \\cdot \\mathrm{MultiHead}(W_Q x,\\; W_K x,\\; W_V x)\n\\tag{23}\\]\n可学习参数为：\n\\[\nW_Q \\in \\mathbb{R}^{h d_k \\times d_{\\text{model}}},\\quad\nW_K \\in \\mathbb{R}^{h d_k \\times d_{\\text{model}}},\\quad\nW_V \\in \\mathbb{R}^{h d_v \\times d_{\\text{model}}},\\quad\nW_O \\in \\mathbb{R}^{d_{\\text{model}} \\times h d_v}.\n\\tag{24}\\]\n一个很重要的工程视角是：因为后面会把输出维度 reshape 成 \\((h,\\text{head\\_dim})\\)，所以你可以把 \\(W_Q,W_K,W_V\\) 看成“每个 head 各有一份投影矩阵”，只不过它们在实现上被拼到同一个大矩阵里。\n\n\n3.6.3 Shape Transformations in Attention\n在继续完成 MHA 之前，我们先理清楚 shape 变化。假设：\n\n输入 \\(x\\) 的 shape 是 (batch_size, seq_len, d_model)\nhead 数量是 num_heads\n每个 head 的维度是 d_k = d_model // num_heads\n\n那么，计算 \\(Q,K,V\\) 的线性投影后，我们需要把它们 reshape 成 (batch_size, num_heads, seq_len, d_k)，以便每个 head 独立计算注意力。实现上通常用以下两步：\n\n先用 view() 把最后一维拆成 (num_heads, d_k)，变成 (batch_size, seq_len, num_heads, d_k)\n再用 transpose() 把 num_heads 维度移到第二维，变成 (batch_size, num_heads, seq_len, d_k)\n\n之后，我们可以计算我们的scores:\nQ (batch_size, seq_len, num_heads, d_k) @ K^T (batch_size, num_heads, d_k, seq_len)  -&gt; Score (batch_size, num_heads, seq_len, seq_len)\nsoftmax 和 mask，不会改变 shape，最后对 V 做加权求和后，输出 shape 是 (batch_size, num_heads, seq_len, d_k)。最后一步是把多头输出拼回原始维度：\n\n先用 transpose() 把 num_heads 维度移回第三维，变成 (batch_size, seq_len, num_heads, d_k)\n再用 contiguous().view() 把最后两维拼回去，变成 (batch_size, seq_len, d_model)。\n最后通过一个线性层 \\(W_O\\) 投影回原始维度。\n最终输出 shape 是 (batch_size, seq_len, d_model)。\n\nx : (B, S, D)\n    +--&gt; Q = x W_Q : (B,S,D) --&gt; view (B,S,H,d_k) --&gt; transpose -&gt; (B,H,S,d_k)\n    |\n    +--&gt; K = x W_K : (B,S,D) --&gt; view (B,S,H,d_k) --&gt; transpose -&gt; (B,H,S,d_k)\n    |\n    +--&gt; V = x W_V : (B,S,D) --&gt; view (B,S,H,d_k) --&gt; transpose -&gt; (B,H,S,d_k)\n\n             K^T : (B,H,d_k,S)\nscores = Q @ K^T  -----------------&gt; scores : (B,H,S,S)\n        (B,H,S,d_k) @ (B,H,d_k,S)\n\nscores / sqrt(d_k) ----------------&gt; (B,H,S,S)\n+ mask (add -inf) -----------------&gt; (B,H,S,S)\nsoftmax (last dim) ----------------&gt; attn : (B,H,S,S)\n\nout_heads = attn @ V  -------------&gt; out_heads : (B,H,S,d_k)\n            (B,H,S,S) @ (B,H,S,d_k)\n\ntranspose(1,2) --------------------&gt; (B,S,H,d_k)\ncontiguous().view(B,S,D) ----------&gt; out : (B,S,D)\nW_O (Linear) ----------------------&gt; y : (B,S,D)\n\n\n3.6.4 RoPE in Attention\n在使用 RoPE 的版本中，需要对 Q 和 K 做同样的位置旋转：\n\n对每个 head 的 \\(Q\\) 应用 RoPE\n对每个 head 的 \\(K\\) 应用 RoPE\n不要对 \\(V\\) 应用 RoPE\n\n原因是：RoPE 影响的是“相似度打分”（\\(QK^\\top\\)）的相对位置信息；而 \\(V\\) 是被加权汇聚的内容本身，通常不需要做旋转。\n另外，RoPE 的一个实现细节是：在 multi-head 中，head 维可以视为 batch 维来处理。也就是说，同一个位置 \\(i\\) 对应的旋转（cos/sin）应该对 所有 head 共享，每个 head 独立做 attention，但旋转规则一致。\n有了这些模块，我们就得到了最终的MHA\n\n\ncs336_basics/modules/attention.py\n\nclass MHA(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        num_heads: int,\n        use_rope: bool = False,\n        theta: float = 10000.0,\n        max_seq_len: int = 2048,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n\n        from cs336_basics.modules.linear import Linear\n        from cs336_basics.modules.rope import RoPEEmbedding\n\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.q_linear = Linear(d_model, d_model, device=device, dtype=dtype)\n        self.k_linear = Linear(d_model, d_model, device=device, dtype=dtype)\n        self.v_linear = Linear(d_model, d_model, device=device, dtype=dtype)\n        self.out_linear = Linear(d_model, d_model, device=device, dtype=dtype)\n\n        self.use_rope = use_rope\n        if use_rope:\n            self.rope = RoPEEmbedding(\n                theta=theta,\n                d_k=self.d_k,\n                max_seq_len=max_seq_len,\n                device=device,\n            )\n\n    def _create_causal_mask(self, seq_len: int, device: torch.device) -&gt; torch.Tensor:\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()\n        return mask.unsqueeze(0).unsqueeze(0)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        token_positions: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        batch_size, seq_len, _ = x.size()\n        causal_mask = self._create_causal_mask(seq_len, x.device)\n\n        Q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.k_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.v_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        if self.use_rope:\n            Q, K = self.rope(Q, token_positions), self.rope(K, token_positions)\n\n        attn_output = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        output = self.out_linear(attn_output)\n\n        return output"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#transformer-block",
    "href": "posts/CS336/Ass01/ass01.html#transformer-block",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.7 Transformer Block",
    "text": "3.7 Transformer Block\n有了这些模块，我们就可以和搭积木一样，搭建我们Transformer\n对输入 \\(x\\)，第一层的更新规则是：\n\\[\ny = x + \\mathrm{MHA}(\\mathrm{RMSNorm}(x)).\n\\tag{25}\\]\n这句话可以拆开理解为三步：\n\n(1) 归一化：先把输入 \\(x\\) 做 RMSNorm，得到更稳定的输入分布\n\n(2) 主操作：把归一化后的向量送入 MHA，计算注意力输出\n\n(3) 残差：把注意力输出加回原输入 \\(x\\)，形成 \\(y\\)\n\n\n3.7.0.1 Pre-Norm\n这里我们采用的是 Pre-Norm 结构，也就是在每个子层（MHA 或 FFN）前做归一化。Pre-Norm相对于 Post-Norm（先做子层再归一化）有几个优点：\n\n训练更稳定：Pre-Norm 可以缓解深层 Transformer 的梯度消失问题，使得训练更稳定。\n更深的模型：Pre-Norm 允许我们训练更深的 Transformer，因为每个子层的输入都经过归一化，减少了内部协变量偏移。 3， 对Learning Rate更不敏感：Pre-Norm 结构对学习率的选择不那么敏感，允许使用更大的学习率进行训练。\n\n\n\n\n\n\n\nDifference nomralization structures\n\n\n\n当然，除了Pre-Norm之外，现在也有一些变体结构，比如 Hybrid Norm(Zhuo et al. 2025)（结合 Pre-Norm 和 Post-Norm 的优点） 下图比较展示了不同归一化结构：\n\n\n\n\n\n\nFigure 7: 比较不同的Normalization的结构，a) Post-Norm 结构；b) Pre-Norm 结构； c) 带 QK-Norm 的 Pre-Norm 结构；d) HybridNorm 结构\n\n\n\n\n\nTransformer Block 的代码实现如下：\n\n\ncs336_basics/model.py\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.mha = MHA(\n            d_model=config.d_model,\n            num_heads=config.num_heads,\n            use_rope=config.use_rope,\n            theta=config.rope_theta,\n            max_seq_len=config.max_seq_len,\n        )\n        self.ffn = FFN(\n            d_model=config.d_model,\n            d_ff=config.d_ff,\n        )\n        self.norm1 = RMSNorm(config.d_model)\n        self.norm2 = RMSNorm(config.d_model)\n\n    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -&gt; torch.Tensor:\n        x = x + self.mha(self.norm1(x), token_positions=token_positions) \n        x = x + self.ffn(self.norm2(x)) \n        return x"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#output-layer",
    "href": "posts/CS336/Ass01/ass01.html#output-layer",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.8 Output Layer",
    "text": "3.8 Output Layer\n在堆叠完若干个 Transformer blocks 之后，我们会得到每个位置的最终 hidden states：\n\\[\nH \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}}\n\\tag{26}\\]\n接下来需要一个 Output Layer（LM Head） 把 hidden states 映射到词表大小的 logits：\n\\[\n\\mathrm{logits} = H W_{\\text{out}}\n\\tag{27}\\]\n其中：\n\\[\nW_{\\text{out}} \\in \\mathbb{R}^{d_{\\text{model}} \\times |\\mathcal{V}|}, \\quad\n\\mathrm{logits} \\in \\mathbb{R}^{B \\times T \\times |\\mathcal{V}|}.\n\\tag{28}\\]\n在很多现代 LLM 中，通常还会在输出层前加一个最终归一化（同样是 Pre-Norm 风格）：\n\\[\n\\mathrm{logits} = \\mathrm{RMSNorm}(H)\\, W_{\\text{out}}.\n\\tag{29}\\]\n\n\ncs336_basics/model.py\n\nclass OutputLayer(nn.Module):\n    def __init__(self, d_model, vocab_size, use_norm: bool = False):\n        super().__init__()\n        self.linear = Linear(d_model, vocab_size)\n        self.norm = RMSNorm(d_model) if use_norm else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.norm(x)\n        logits = self.linear(x)\n        return logits\n\n\n3.8.1 Weight Tying\n如果模型里有 token embedding 矩阵 \\(E \\in \\mathbb{R}^{|\\mathcal{V}|\\times d_{\\text{model}}}\\)，那么常见的做法是 共享输入 embedding 和输出投影权重（weight tying）：\n\\[\nW_{\\text{out}} = E^\\top.\n\\tag{30}\\]\n这样可以减少参数量，并且经常带来更好的训练稳定性与泛化效果。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#full-transformer-model",
    "href": "posts/CS336/Ass01/ass01.html#full-transformer-model",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.9 Full Transformer Model",
    "text": "3.9 Full Transformer Model\n当我们实现完 embedding、Transformer block（MHA + FFN）、以及输出层之后，就可以按照 Figure 2 的高层结构把整个语言模型串起来了。整体流程可以概括为三步：\n\nToken Embedding：把 token id 映射到向量表示\n堆叠 num_layers 个 Transformer Blocks\n Output Layers：映射到词表分布\n\n具体的代码如下：\n\n\ncs336_basics/model.py\n\nclass TransformerLM(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_layers)])\n        self.final_norm = RMSNorm(config.d_model)\n        self.output_layer = OutputLayer(config.d_model, config.vocab_size, use_norm=config.use_final_norm)\n\n        if config.tie_weights:\n            self._tie_weights()\n\n    def forward(self, x: torch.Tensor, token_positions: torch.Tensor | None = None) -&gt; torch.Tensor:\n        x = self.token_embedding(x)\n\n        for layer in self.layers:\n            x = layer(x, token_positions=token_positions)\n\n        x = self.final_norm(x)\n        logits = self.output_layer(x)\n        return logits\n\n    def _tie_weights(self):\n        self.output_layer.linear.weight = self.token_embedding.weight\n\ntoken ids → embedding 得到 \\(X_0\\) → 经过 \\(L\\) 个 Transformer blocks 得到 \\(H\\) → 输出头（norm + linear + softmax）得到词表分布，用于 next-token prediction。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#part-02-summary",
    "href": "posts/CS336/Ass01/ass01.html#part-02-summary",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "3.10 Part 02 Summary",
    "text": "3.10 Part 02 Summary\n总的来说，Part 02 就是在 Part 01 的 Tokenization 之后，把“能训练的语言模型”真正搭起来：我们从最基础的 Linear / Embedding 出发，逐步实现 RMSNorm（Pre-Norm）、现代 LLM 常用的 SwiGLU-FFN、再到最核心也最容易写错的 (RoPE + Causal) Multi-Head Self-Attention，最终像搭积木一样组装出完整的 TransformerBlock，并串联成 TransformerLM，通过 Output Layer 输出 vocabulary logits 用于 next-token prediction。\n这一部分最值得记住的工程要点有三类：\n\n稳定性（stability）： Softmax 的数值稳定（减 max）、Pre-Norm（RMSNorm 放在子层前）、以及 causal mask 防止未来信息泄露，都是“训练能不能跑起来”的关键。\n效率（efficiency）： Q/K/V 投影应当是 3 次矩阵乘法（更进一步可以合成 1 次），mask 用 “softmax 前加 -” 而不是切子序列，RoPE 用预计算的 sin/cos buffer 复用跨 batch/跨层，避免显式构造 \\(d\\times d\\) 旋转矩阵。\n结构（architecture）： 现代 LLM 的 Block 基本都遵循 “RMSNorm → MHA/FFN → Residual” 的 Pre-Norm 模式；FFN 常用 SwiGLU（激活 + gating）；RoPE 只作用在 Q/K（不作用在 V）；最后再接一个输出头（可选 final norm / weight tying）把 hidden states 映射到词表分布。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#loss-perplexity",
    "href": "posts/CS336/Ass01/ass01.html#loss-perplexity",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "4.1 Loss & Perplexity",
    "text": "4.1 Loss & Perplexity\n我们先来定义我们的Loss Function,在神经网络训练中，Loss Function（损失函数）用于衡量模型预测与真实标签之间的差距。对于语言模型，常用的损失函数是交叉熵损失（Cross Entropy Loss）。\n\n4.1.1 Cross Entropy Loss\n在现代常见的语言模型是Next-Token Prediction模型，其训练目标是预测下一个token，也就是给定前面 \\(t\\) 个 token，预测第 \\(t+1\\) 个 token 的概率分布：\n\\[\nP(x_{t+1} | x_1, x_2, \\ldots, x_t)\n\\tag{31}\\]\n假设我们有一个长度为 \\(T\\) 的训练序列 \\((x_1, x_2, \\ldots, x_T)\\)，那么模型的目标是最大化整个序列的联合概率： \\[\nP(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t | x_1, x_2, \\ldots, x_{t-1})\n\\tag{32}\\]\n为了实现这个目标，我们通常使用交叉熵损失（Cross Entropy Loss）来衡量模型预测的概率分布与真实分布之间的差异。\nCross Entropy Loss 常用于分类任务中，定义如下：\n\\[\n\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\ln(p_{i,c})\n\\tag{33}\\]\n其中：\n\n\\(N\\) 是样本数量（在语言模型中通常是所有时间步的 token 数量）\n\\(C\\) 是类别数量（词表大小）\n\\(y_{i,c}\\) 是样本 \\(i\\) 的真实标签的 one-hot 编码（如果样本 \\(i\\) 的真实类别是 \\(c^*\\)，则 \\(y_{i,c^*} = 1\\)，其他类别为 0）\n\\(p_{i,c}\\) 是模型对样本 \\(i\\) 预测为类别 \\(c\\)的概率。\n\n实现这个损失函数时，我们通常会结合 softmax 一起使用，因为模型输出的 logits 需要先经过 softmax 转换为概率分布。\n\n\ncs336_basics/loss.py\n\ndef cross_entropy(logits: torch.Tensor, labels: torch.Tensor):\n    logits = logits - torch.max(logits, dim=1, keepdim=True).values\n    log_probs = logits - torch.log(torch.sum(torch.exp(logits), dim=1, keepdim=True))\n\n    labels = labels.unsqueeze(1)\n\n    loss = log_probs.gather(1, labels).squeeze(1) \n    loss = -loss.mean()\n    return loss\n\n其中 log_probs.gather(1, labels) 这一行代码的作用是从 log_probs 张量中提取出每个样本对应的真实标签的对数概率值。具体来说：\n\nlog_probs 的 shape 是 (N, C)，表示 \\(N\\) 个样本在 \\(C\\) 个类别上的对数概率分布。\nlabels 的 shape 是 (N, 1)，表示每个样本的真实类别索引。\ngather(1, labels) 会根据 labels 中的索引，从 log_probs 的第二维（类别维度）中提取对应的对数概率值，结果的 shape 是 (N, 1)。\n\n在使用这个交叉熵损失函数时，我们通常会将模型的输出 logits 和对应的真实标签展开成一维向量。这样做的好处是，可以简化计算过程，使得每个时间步的预测都被视为一个独立的样本，从而方便地计算整体的损失。\n\n\ncs336_basics/train_engine.py\n\nlogits = model(inputs)\nlogits = logits.view(-1, logits.size(-1))\ntargets = targets.view(-1)\n\n\n\n4.1.2 Perplexity\n在语言模型中，除了交叉熵损失，我们还常用一个指标叫做困惑度（Perplexity, PPL）来评估模型的性能。困惑度衡量的是模型对测试数据的预测能力，数值越低表示模型越好。 它的定义如下：\n\\[\n\\mathrm{PPL} = \\exp\\left(\\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_i\\right)\n\\tag{34}\\]\n它是交叉熵损失的指数形式，其中 \\(\\mathcal{L}_i\\) 是第 \\(i\\) 个样本的交叉熵损失，\\(N\\) 是样本总数， 也就是所有时间步的 token 数量。 它的直观意义是：模型在预测下一个 token 时，平均每个 token 有多少种可能的选择。 我们展开PPL的定义：\n\\[\n\\mathrm{PPL} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\ln(p_{i,c^*})\\right) = \\left(\\prod_{i=1}^{N} \\frac{1}{p_{i,c^*}}\\right)^{\\frac{1}{N}}\n\\tag{35}\\]\nPPL 等价于“模型给真实 token 的概率 p”的倒数 1/p 的几何平均，所以越小表示模型平均给真值的概率越大。\n这个看起来很抽象，我们来看一个具体例子： 假设我们有一个非常简单的词表，只有 4 个 token：{A, B, C, D}。现在我们有一个测试序列 A B C D，模型在每个时间步的预测概率如下：\n\n预测 B 的概率：0.5\n预测 C 的概率：0.25\n预测 D 的概率：0.1\n\n那么，我们可以计算这个序列的困惑度： \\[\n\\mathrm{PPL} = \\exp\\left(-\\frac{1}{3} (\\ln(0.5) + \\ln(0.25) + \\ln(0.1))\\right) \\approx 4.64\n\\tag{36}\\]\n这意味着，模型在预测下一个 token 时，平均每个 token 有大约 4.64 种可能的选择。 因为词表只有 4 个 token，这个困惑度表明模型的预测能力还不够好。\n代码实现如下：\n\n\ncs336_basics/loss.py\n\ndef perplexity(loss: torch.Tensor) -&gt; torch.Tensor:\n    return torch.exp(loss)"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#optimizer-learning-rate-scheduler",
    "href": "posts/CS336/Ass01/ass01.html#optimizer-learning-rate-scheduler",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "4.2 Optimizer & Learning Rate Scheduler",
    "text": "4.2 Optimizer & Learning Rate Scheduler\n有了Loss Function之后，我们接下来要定义Optimizer和Learning Rate Scheduler，来指导模型参数的更新。\n\n4.2.1 AdamW\n在这个作业中，我们使用 AdamW(Loshchilov and Hutter 2019) 作为优化器。AdamW 是 Adam 优化器的一个变体，它通过将权重衰减（weight decay）与梯度更新解耦来改善模型的泛化能力。\n首先我们来看一下Adam是什么， Adam 是一种自适应学习率优化算法，它结合了动量（Momentum）和RMSProp的思想，通过计算梯度的一阶矩估计（均值）和二阶矩估计（未中心化的方差）来调整每个参数的学习率。Adam 对每个参数 \\(\\theta\\) 都维护两份状态（state）：\n\n一阶矩（动量） \\(m_t\\)：梯度的指数滑动平均（类似 momentum）\n二阶矩 \\(v_t\\)：梯度平方的指数滑动平均（刻画梯度尺度）\n\n它们的更新规则如下：\n\\[\n\\begin{split}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n\\theta_t &= \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\\\\n\\end{split}\n\\tag{37}\\]\n其中，\\(g_t\\) 是当前梯度，\\(\\beta_1\\) 和 \\(\\beta_2\\) 是控制滑动平均的超参数，\\(\\alpha\\) 是学习率，\\(\\epsilon\\) 是防止除零的小常数。\\(\\hat{m}_t\\) 和 \\(\\hat{v}_t\\) 是偏差修正后的估计：\n\\[\n\\begin{split}\n\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n\\end{split}\n\\tag{38}\\]\n在 Adam 中，权重衰减（weight decay）通常会被实现成 L2 正则：也就是在损失函数中加入 \\(\\frac{\\lambda}{2}\\|\\theta\\|^2\\)， 损失函数变为\\(\\mathcal{L} = \\mathcal{L}_{\\text{original}} + \\frac{\\lambda}{2}\\|\\theta\\|^2\\), 其对应的梯度额外多一项 \\(\\lambda \\theta\\)。因此，梯度会变成：\n\\[\ng_t \\leftarrow g_t + \\lambda \\theta_{t-1}\n\\tag{39}\\]\n如果直接把这个修改后的梯度带入 Adam 的更新规则, 我们会得到：\n$$\n\\[\\begin{split}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) (g_t + \\lambda \\theta_{t-1}) \\\\\nv_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) (g_t + \\lambda \\theta_{t-1})^2 \\\\\n\\theta_t &= \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\\\\n        &= \\theta_{t-1} - \\alpha\n\n\n\\end{split}\\]\n$${#eq-adam-with-l2}\n直觉上，weight decay 想做的事情很简单：每一步都把参数往 0 拉一点，即让参数规模受控、提升泛化, 也就是说，也就是衰减项 只是一条独立的线性收缩，不被任何自适应缩放影响。 但在 Adam 里，情况并非如此。\n在 Adam 里，更新会被 \\(\\sqrt{\\hat v_t}\\) 进行“按维度缩放”。这意味着：\n\n正则项 \\(\\lambda \\theta\\) 也会进入 \\(\\hat m_t\\), \\(\\hat v_t\\) 的计算\n进而它也会被 缩放\n结果：不同参数维度的衰减强度不一致（某些维度几乎不衰减，某些维度衰减过强）\n\n换句话说：在 Adam 这种自适应优化器里，“L2 正则 ≠ 你以为的 weight decay”。(Loshchilov and Hutter 2019) 的核心观察就是：如果我们想要真正实现“把参数往 0 拉”的正则化效果，就应该把它从 Adam 的梯度更新中拆出来。\nAdamW 的算法如以下所示：\n\n\n\n\n\n\nFigure 8: AdamW 算法\n\n\n\n可以看到，AdamW 把 weight decay 从梯度更新中解耦出来，直接在参数更新时做线性收缩： \\[\n\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\alpha \\lambda \\theta_{t-1}\n\\tag{40}\\] 这样就保证了 weight decay 的效果是一致的，不会被自适应缩放影响。\n理解了 AdamW 的原理后，我们来看一下它的代码实现：\n\n\ncs336_basics/optim.py\n\nclass AdamW(torch.optim.Optimizer):\n    def __init__(\n        self,\n        params: Iterable[torch.nn.Parameter],\n        lr: float = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 1e-2,\n    ):\n        if lr &lt; 0.0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if eps &lt;= 0.0:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if weight_decay &lt; 0.0:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n        if not isinstance(betas, tuple) or len(betas) != 2:\n            raise ValueError(f\"betas must be a tuple of length 2, got: {betas}\")\n        beta1, beta2 = betas\n        if not (0.0 &lt;= beta1 &lt; 1.0):\n            raise ValueError(f\"Invalid beta1 value: {beta1}\")\n        if not (0.0 &lt;= beta2 &lt; 1.0):\n            raise ValueError(f\"Invalid beta2 value: {beta2}\")\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure: Optional[callable] = None):\n        \"\"\"Performs a single optimization step.\"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            lr: float = group[\"lr\"]\n            beta1, beta2 = group[\"betas\"]\n            eps: float = group[\"eps\"]\n            weight_decay: float = group[\"weight_decay\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad\n                if grad.is_sparse:\n                    raise RuntimeError(\"AdamW does not support sparse gradients\")\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avg = state[\"exp_avg\"]\n                exp_avg_sq = state[\"exp_avg_sq\"]\n\n                state[\"step\"] += 1\n                t = state[\"step\"]\n\n                # Update biased first and second moment estimates\n                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=(1.0 - beta2))\n\n                # Bias correction\n                bias_correction1 = 1.0 - beta1**t\n                bias_correction2 = 1.0 - beta2**t\n\n                # Compute step size\n                step_size = lr / bias_correction1\n\n                # Denominator: sqrt(v_hat) + eps\n                denom = (exp_avg_sq / bias_correction2).sqrt().add_(eps)\n\n                # Decoupled weight decay\n                if weight_decay != 0.0:\n                    p.mul_(1.0 - lr * weight_decay)\n\n                # Parameter update\n                p.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n\n\n\n4.2.2 Cosine Annealing Learning Rate Scheduler\n有了优化器之后，我们还需要一个Learning Rate Scheduler来动态调整学习率。在深度学习训练过程中，最合适的学习率会随阶段变化：\n\n训练早期：模型还没学到东西，参数离目标很远\n\n用 更大的学习率，更新更快，loss 下降更快\n\n训练后期：模型接近收敛\n\n用 更小的学习率，避免在最优点附近震荡，提升稳定性与最终效果\n\n\n因此我们需要Learning Rate Scheduler来在不同时期调整不同的Learning Rate。一个常见的Learning Rate Scheduler叫做 Cosine Annealing Scheduler，其核心思想是：\n\n它把学习率 \\(\\alpha_t\\) 分成 三段：\n\nWarm-Up: 从 0 线性提升到 \\(\\alpha_{\\text{max}}\\) （防止一开始梯度太乱，直接大 LR 会不稳定）\nCosine Annealing: 从 \\(\\alpha_{\\max}\\) 平滑降到 \\(\\alpha_{\\min}\\) （下降过程更平滑，比“突然降学习率”更稳）\nPost-Annealing: 保持\\(\\alpha_{\\min}\\) 不变 （让训练在低 LR 下慢慢精修（fine-tune 风格））\n\n\n我们先来定义几个符号：\n\n\n\n\n\n\n\n\n\n\n\n符号\n含义\n\n\n\n\n\\(t\\)\n当前训练 step（迭代次数）\n\n\n\\(\\alpha_{\\max}\\)\n最大学习率（峰值）\n\n\n\\(\\alpha_{\\min}\\)\n最小/最终学习率\n\n\n\\(T_w\\)\nwarm-up 的步数（预热多久）\n\n\n\\(T_c\\)\nCosine Annealing结束的步数（到这个 step 学习率降到 \\(\\alpha_{\\min}\\)）\n\n\n\\(\\alpha_t\\)\n当前学习率\n\n\n\n\n\nTable 1: 符号说明\n\n\n\n\n接下来我们看一下如何调整这三个不同的阶段：\n\n阶段1: Warm-Up\n当 \\(t &lt; T_{w}\\) 时，我们线性增长到 \\(\\alpha_{\\text{max}}\\)\n\\[\n\\alpha_t=\\frac{t}{T_w}\\alpha_{\\max}\n\\tag{41}\\]\n\n阶段2: Cosine Annealing\n这个是相对复杂的阶段，在这个阶段我们的学习率用余弦曲线下降：\n\\[\n\\alpha_t=\\alpha_{\\min}+\\frac{1}{2}\\left(1+\\cos\\left(\\frac{t-T_w}{T_c-T_w}\\pi\\right)\\right)(\\alpha_{\\max}-\\alpha_{\\min})\n\\tag{42}\\]\n这个式子做了两件事：\n\n用 \\(\\cos(\\cdot)\\) 产生一个从 1 平滑到 -1 的曲线\n再把它映射成一个从 \\(\\alpha_{\\max}\\) 平滑到 \\(\\alpha_{\\min}\\) 的学习率\n\n我们查看一下两个端点：\n\n当 \\(t = T_{w}\\): \\(\\cos(0) = 1\\), \\(\\alpha_{t} = \\alpha_{\\min} +\\alpha_{\\max} - \\alpha_{\\min} = \\alpha_{\\max}\\)\n当 \\(t = T_{c}\\): \\(\\cos(\\pi) = -1\\), \\(\\alpha_{t} = \\alpha_{\\min} +0 = \\alpha_{\\min}\\)\n\n\n阶段3: Post-Annealing\n当 \\(t \\geq T_{c}\\) 时，我们将 \\(\\alpha_{t}\\) 设定为 \\(\\alpha_{\\min}\\)，且保持不变.\n代码实现也很简单：\n\n\ncs336_basics/optim.py\n\ndef cosine_annealing_lr(\n    t: int,\n    alpha_max: float,\n    alpha_min: float,\n    Tw: int,\n    Tc: int,\n) -&gt; float:\n    # Warm-up\n    if Tw &gt; 0 and t &lt; Tw:\n        return (t / Tw) * alpha_max\n\n    # Cosine annealing (including the exact boundary t==Tw)\n    if t &lt;= Tc:\n        # If Tc == Tw, there is no annealing window; at t==Tw return alpha_max.\n        if Tc == Tw:\n            return alpha_max\n\n        progress = (t - Tw) / (Tc - Tw)  # in [0, 1]\n        return alpha_min + 0.5 * (1.0 + math.cos(math.pi * progress)) * (alpha_max - alpha_min)\n\n    # Post-annealing\n    return alpha_min\n\n\n\n\n\n\n\nFigure 9: Figure: Cosine Annealing Learning Rate Scheduler Example  total_steps = 10000 , alpha_max = 0.001 , alpha_min = 1e-4 , Tw = 500 , Tc = total_steps // 2\n\n\n\n\n\n4.2.3 Gradient Clipping\n在训练神经网络时，有时会遇到某些 “特别难/特别极端” 的训练样本，它们会让模型产生 非常大的梯度。如果直接用这种梯度更新参数，可能会导致：\n\nLoss Spike: Loss 突然爆炸（数值不稳定）\n参数更新步子太大，导致训练发散\n训练曲线抖动的很厉害，难以收敛\n\n为了解决这个问题，实践中常用 Gradient Clipping。它的核心思想非常简单： ::: {.callout-paper} 把所有参数的梯度合起来，视为一个向量\\(g\\), 计算它的 L2-Norm \\(\\|g\\|_{2}\\),我们可以将其理解为整体梯度的强度/总能量, 然后设定一个阈值 \\(M\\)（最大允许的梯度范数） :::\n因此，我们有两种情况：\n\n\\(\\|g\\|_{2} \\leq M\\): 说明梯度在安全范围内，直接保持原样。\n\\(\\|g\\|_{2} &gt; M\\): 说明这个梯度过大，可能会造成问题，因此我们等比例缩小更新这个梯度, 缩小的系数是 \\(\\frac{M}{\\|g\\|_{2} + \\epsilon}\\). 更新后的梯度为：\n\n\\[\ng \\leftarrow g \\cdot \\frac{M}{\\|g\\|_2 + \\epsilon}\n\\tag{43}\\]\n通过这种缩放的方式，我们可以实现：\n\n方向不变：裁剪不会改变梯度的方向（只是整体缩放）\n步子变小：当梯度太大时，相当于强行让更新不要跨太大步\n更稳定：尤其对 RNN、Transformer 或训练早期很常见的“梯度爆炸”问题很有帮助\n\n代码实现也很简单，直观：\n\n\ncs336_basics/optim.py\n\n\n@torch.no_grad()\ndef gradient_clip(\n    parameters: Iterable[torch.nn.Parameter],\n    max_l2_norm: float,\n    eps: float = 1e-6,\n) -&gt; None:\n    # Calculate L2-Norm\n    total_norm = 0.0\n    for p in parameters:\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm**0.5\n\n    # Update gradient value accroding to the factor\n    clip_coef = max_l2_norm / (total_norm + eps)\n    if clip_coef &lt; 1.0:\n        for p in parameters:\n            if p.grad is not None:\n                p.grad.data.mul_(clip_coef)\n\n\n\n4.2.4 Put Together\n有了这三个优化的组件之后，我们就可以把它们放在一起，形成一个完整的训练步骤：\n\n\ncs336_basics/train_engine.py\n\ninputs, targets = data_loading_sequential(\n    x=x,\n    batch_size=train_config.batch_size,\n    context_length=model.config.max_seq_len,\n    device=train_config.device,\n    state=state,\n)\n\n# Forward pass\nwith ctx:\n    logits = model(inputs)\n    logits = logits.view(-1, logits.size(-1))\n    targets = targets.view(-1)\n    loss = cross_entropy(logits, targets)\n\n# Backward pass and optimization step\noptimizer.zero_grad(set_to_none=True)\nloss.backward()\n# Gradient clipping\ngradient_clip(model.parameters(), max_l2_norm=train_config.max_grad_norm)\n\n# Learning rate scheduling\nlr = cosine_annealing_lr(\n    t=step,\n    alpha_max=train_config.max_lr,\n    alpha_min=train_config.min_lr,\n    Tw=train_config.warmup_steps,\n    Tc=train_config.num_steps - train_config.warmup_steps,\n)\nfor param_group in optimizer.param_groups:\n    param_group[\"lr\"] = lr\noptimizer.step()"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#dataloader",
    "href": "posts/CS336/Ass01/ass01.html#dataloader",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "4.3 Dataloader",
    "text": "4.3 Dataloader\n有了模型和优化器之后，我们还需要一个数据加载器（Dataloader）来提供训练数据。在语言模型的训练中，数据通常是文本序列，我们需要将这些文本序列转换为模型可以处理的格式。还记得之前我们在 Part 01 讲过的 Tokenization 吗？我们需要先把文本转换为 token ids，然后再组织成适合模型输入的批次。我们已经训练完Tokenizer，并且把文本转换成了token ids的形式保存在.bin文件中，接下来我们需要一个数据加载器来从这些token ids中提取出训练样本。\n在这个作业中，我们使用一个简单的顺序采样（sequential sampling）方法来加载数据。具体来说，我们会从 token ids 中按顺序提取出长度为 context_length 的子序列作为输入，目标是预测下一个 token。我们会不断地从数据中提取这样的子序列，直到达到指定的批次大小（batch size）。\n\n\n\n\n\n\nNote\n\n\n\n作业中用的是Random Sampling，也就是随机指定起点，然后截取context length长度的序列作为输入。\n\n\n代码实现如下：\n\n\ncs336_basics/data.py\n\noriginal_data = np.memmap( \n    train_config.train_data_path, \n    dtype=np.uint16, \n    mode=\"r+\", \n)\nx_t = torch.from_numpy(original_data) \n\ndef get_batch_sequential(\n    x_t: torch.Tensor | np.ndarray,\n    batch_size: int,\n    context_length: int,\n    device: str | torch.device,\n    state: BatchState,\n    *,\n    stride: int | None = None,\n):\n    if stride is None:\n        stride = context_length\n\n    n = x_t.numel()\n    max_start = n - context_length - 1\n    if max_start &lt; 0:\n        raise ValueError(f\"Sequence too short: n={n}, context_length={context_length}\")\n\n    # Avoid per-sample modulo wrap. If we would run off the end, reset cursor.\n    last_start = state.pos + (batch_size - 1) * stride\n    end = last_start + context_length + 1\n    if end &gt; n:\n        state.pos = 0\n        last_start = (batch_size - 1) * stride\n        end = last_start + context_length + 1\n\n    base = x_t[state.pos : end]  # 1D contiguous slice\n\n    # 2D views: (B, T). Strides are in *elements* for PyTorch tensors.\n    inputs = base.as_strided(size=(batch_size, context_length), stride=(stride, 1)) \n    targets = base[1:].as_strided(size=(batch_size, context_length), stride=(stride, 1)) #&lt;&lt; \n\n    state.pos += batch_size * stride\n\n    # Transfer + cast (cast happens AFTER transfer =&gt; cheaper for CPU)\n    if (isinstance(device, torch.device) and device.type == \"cuda\") or (\n        isinstance(device, str) and \"cuda\" in device.lower()\n    ):\n        inputs = inputs.to(device, non_blocking=True).long() \n        targets = targets.to(device, non_blocking=True).long() \n    else:\n        inputs = inputs.long().to(device)\n        targets = targets.long().to(device)\n\n    return inputs, targets\n\n需要注意的是，这个部分可能是训练时间瓶颈，因为数据加载和预处理可能会比较慢，尤其是当数据量很大时。 我们运用了以下几个技巧来提升数据加载效率：\n\n使用 np.memmap 来内存映射数据文件，这样可以避免一次性加载整个数据集到内存中，节省内存空间。\n使用 as_strided 来创建输入和目标的视图，避免了数据的复制，提高了效率。\n使用非阻塞的数据传输（non_blocking=True）来加速数据从 CPU 到 GPU 的传输。"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#checkpoint",
    "href": "posts/CS336/Ass01/ass01.html#checkpoint",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "4.4 Checkpoint",
    "text": "4.4 Checkpoint\n在训练模型的过程中，我们需要时不时的保持 Checkpoint, 为什么呢？因为训练模型不只是“把 loss 训到低”这么简单，我们还经常需要：\n\n中途恢复训练：比如训练跑到一半机器断了、作业超时、意外退出，\n保留中间模型：方便之后分析训练过程、比较不同阶段的模型、做不同阶段的采样， Exponemtial Moving Average (EMA) 等等\n\nCheckpoint 的目标是：让你能从中断处无缝继续训练。\n因此至少要存这三类东西：\n\n模型参数（model weights）\n\n没有它，就没有模型本体了\n\n优化器状态（optimizer state）\n\n例如 AdamW 的一阶/二阶动量（moment estimates）\n不存优化器状态，恢复后训练轨迹会变（因为动量没了）\n\n当前迭代步数（iteration / step）\n\n用来恢复学习率 schedule\n否则学习率会从头开始或错位\n\n\n实现如下：\n\n\ncs336_basics/utils.py\n\ndef save_checkpoint(\n    model: torch.nn.Module,\n    optimizer,\n    iteration,\n    out: str | os.PathLike | typing.BinaryIO | typing.IO[bytes],\n    verbose: bool = False,\n) -&gt; None:\n    state = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"iteration\": iteration,\n    }\n\n    torch.save(state, out)\n\n    if verbose:\n        print_color(f\"Checkpoint saved to {out}\", \"blue\")\n\n\ndef load_checkpoint(\n    src: str | os.PathLike | typing.BinaryIO | typing.IO[bytes], model, optimizer, verbose: bool = False\n) -&gt; int:\n    state = torch.load(src, map_location=get_device())\n\n    model.load_state_dict(state[\"model_state_dict\"])\n    optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n\n    if verbose:\n        print_color(f\"Checkpoint loaded from {src}\", \"blue\")\n\n    return state[\"iteration\"]"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#training-looping",
    "href": "posts/CS336/Ass01/ass01.html#training-looping",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "4.5 Training Looping",
    "text": "4.5 Training Looping\n最后，我们把所有的组件放在一起，形成一个完整的训练循环：\n\n\ncs336_basics/train_engine.py\n\ndef train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_config: TrainingConfig):\n    tokenizer = load_tokenizer_from_dir(train_config.dataset_dir)\n\n    # Load training dataset\n    original_data = np.memmap(\n        train_config.train_data_path,\n        dtype=np.uint16,\n        mode=\"r+\",\n    )\n    x = torch.from_numpy(original_data)\n\n    best_eval_loss = float(\"inf\")\n    ctx = get_ctx(train_config.use_mixed_precision, train_config.device)\n\n    # Training loop\n    state = BatchState(pos=0)\n    for step in range(train_config.num_steps):\n        # inputs, targets = dataloader\n        inputs, targets = data_loading_sequential(\n            x=x,\n            batch_size=train_config.batch_size,\n            context_length=model.config.max_seq_len,\n            device=train_config.device,\n            state=state,\n        )\n\n        # Forward pass\n        with ctx:\n            logits = model(inputs)\n            logits = logits.view(-1, logits.size(-1))\n            targets = targets.view(-1)\n            loss = cross_entropy(logits, targets)\n\n        # Backward pass and optimization step\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        # Gradient clipping\n        gradient_clip(model.parameters(), max_l2_norm=train_config.max_grad_norm)\n\n        # Learning rate scheduling\n        lr = cosine_annealing_lr(\n            t=step,\n            alpha_max=train_config.max_lr,\n            alpha_min=train_config.min_lr,\n            Tw=train_config.warmup_steps,\n            Tc=train_config.num_steps - train_config.warmup_steps,\n        )\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n        optimizer.step()\n\n        # Logging\n        if train_config.wandb_logging:\n            wandb.log(\n                {\n                    \"train/loss\": loss.item(),\n                    \"train/perplexity\": perplexity(loss).item(),\n                    \"train/lr\": lr,\n                },\n                step=step + 1,\n            )\n\n        print_color(\n            f\"Step {step + 1}/{train_config.num_steps}, Loss: {loss.item():.4f}, LR: {lr:.6f}\", \"green\"\n        )\n\n        if train_config.eval_log_interval &gt; 0 and (step + 1) % train_config.eval_log_interval == 0:\n            # Cleanup\n            del inputs, targets, logits, loss\n            clear_memory()\n\n            print_color(\"Evaluating model...\", \"blue\")\n            eval_loss, eval_perplexity = eval_model(model, train_config, step + 1)\n            wandb.log(\n                {\n                    \"eval/loss\": eval_loss.item(),\n                    \"eval/perplexity\": eval_perplexity.item(),\n                },\n                step=step + 1,\n            )\n            print_color(\n                f\"Eval Loss: {eval_loss.item():.4f}, Eval Perplexity: {eval_perplexity.item():.4f}\", \"blue\"\n            )\n            if eval_loss &lt; best_eval_loss:\n                best_eval_loss = eval_loss\n                print_color(f\"New best eval loss: {best_eval_loss:.4f}\", \"yellow\")\n                out_path = os.path.join(\n                    train_config.save_checkpoint_dir,\n                    train_config.model_name,\n                    f\"best_model_step_{step + 1}.pt\",\n                )\n                save_checkpoint(\n                    model=model,\n                    optimizer=optimizer,\n                    iteration=step + 1,\n                    out=out_path,\n                    verbose=True,\n                )\n\n        # Sample generation\n        if train_config.sampling_log_interval &gt; 0 and (step + 1) % train_config.sampling_log_interval == 0:\n            generated_outputs = generate(\n                model=model,\n                prompt=\"Once upon a time\",\n                tokenizer=tokenizer,\n                max_new_tokens=256,\n                top_k=50,\n                temperature=0.8,\n            )\n            generated_text = generated_outputs[\"generated_text\"]\n            print_color(f\"Generated text at step {step + 1}:\", \"cyan\")\n            print(\"Once upon a time\", end=\"\")\n            print_color(f\"{generated_text}\\n\", \"cyan\")"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#greedy-sampling",
    "href": "posts/CS336/Ass01/ass01.html#greedy-sampling",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "5.1 Greedy Sampling",
    "text": "5.1 Greedy Sampling\nGreedy Sampling 是最简单的一种采样方法。在每一步生成时，模型会选择概率最高的 token 作为下一个 token。虽然这种方法简单且高效，但它可能会导致生成的文本缺乏多样性和创造性，因为它总是选择最可能的选项，容易陷入局部最优。\n\n\ncs336_basics/generation.py\n\nnext_token_id = next_token_logits.argmax(dim=-1, keepdim=True)"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#top-k-sampling",
    "href": "posts/CS336/Ass01/ass01.html#top-k-sampling",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "5.2 Top-K Sampling",
    "text": "5.2 Top-K Sampling\nTop-K Sampling 是一种改进的采样方法。在每一步生成时，模型会选择概率最高的 K 个 token，然后从这 K 个 token 中根据它们的概率分布进行采样。这样可以增加生成文本的多样性，同时仍然保持一定的质量。\n\n\ncs336_basics/generation.py\n\ndef top_k_sampling(\n    logits: torch.Tensor,\n    top_k: int,\n):\n    if top_k &lt;= 0:\n        # sample from full distribution\n        probs = F.softmax(logits, dim=-1)\n        return torch.multinomial(probs, num_samples=1).squeeze(-1)\n\n    # 1. keep only top-k logits\n    top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n\n    filtered_logits = torch.full_like(logits, float(\"-inf\"))\n    filtered_logits.scatter_(dim=-1, index=top_k_indices, src=top_k_logits)\n\n    # 2. softmax over filtered logits\n    probs = F.softmax(filtered_logits, dim=-1)\n\n    # 3. sample\n    next_token = torch.multinomial(probs, num_samples=1)\n\n    return next_token"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#top-p-sampling",
    "href": "posts/CS336/Ass01/ass01.html#top-p-sampling",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "5.3 Top-P Sampling",
    "text": "5.3 Top-P Sampling\nTop-P Sampling（也称为 Nucleus Sampling）是一种更灵活的采样方法。在每一步生成时，模型会选择累计概率达到 P 的最小 token 集合，然后从这个集合中根据它们的概率分布进行采样。这样可以动态调整候选 token 的数量，既保证了多样性，又避免了选择过于罕见的 token。\n\n\ncs336_basics/generation.py\n\ndef top_p_sampling(logits: torch.Tensor, top_p: float) -&gt; torch.Tensor:\n    \"\"\"\n    logits: (B, V)\n    returns: (B,) sampled token ids\n    \"\"\"\n    assert 0.0 &lt; top_p &lt;= 1.0\n\n    # sort\n    sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n    sorted_probs = F.softmax(sorted_logits, dim=-1)\n    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n\n    # mask tokens with cumulative prob &gt; top_p (but keep at least 1 token)\n    sorted_indices_to_remove = cumulative_probs &gt; top_p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = False\n\n    # scatter mask back to original vocab positions\n    indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)\n    indices_to_remove.scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n\n    filtered_logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n\n    probs = F.softmax(filtered_logits, dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n\n    return next_token"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#temperature",
    "href": "posts/CS336/Ass01/ass01.html#temperature",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "5.4 Temperature",
    "text": "5.4 Temperature\n当然，我们还可以通过调整 temperature 来控制生成文本的随机性。Temperature 是一个正数，通常在 \\[ (0, \\infty) \\] 范围内。它通过缩放 logits 来影响概率分布：\n\n当 temperature &lt; 1 时，概率分布会变得更陡峭，模型更倾向于选择高概率的 token，生成的文本更确定性。\n当 temperature &gt; 1 时，概率分布会变得更平坦，模型更倾向于选择低概率的 token，生成的文本更具多样性和创造性。\n\n数学上，我们通过除以 temperature 来调整 logits：\n\\[\n\\mathrm{softmax}_i(\\mathbf{z};T)\n=\\frac{\\exp\\left(\\frac{z_i}{T}\\right)}\n{\\sum_{j}\\exp\\left(\\frac{z_j}{T}\\right)}\n\\tag{44}\\]\n\n\n\n\n\n\nFigure 10: Temperature 对采样分布的影响示意图"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#part-04-summary",
    "href": "posts/CS336/Ass01/ass01.html#part-04-summary",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "5.5 Part 04 Summary",
    "text": "5.5 Part 04 Summary\n在本部分中，我们介绍了如何使用训练好的语言模型进行文本生成。我们讨论了几种常用的采样方法，包括 Greedy Sampling、Top-K Sampling 和 Top-P Sampling，并介绍了如何通过调整 temperature 来控制生成文本的随机性。通过这些方法，我们可以生成多样化且有趣的文本内容。\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Method\nDescription\nPros\nCons\n\n\n\n\nGreedy Sampling\n选择概率最高的 token\n简单高效\n可能缺乏多样性，容易陷入局部最优\n\n\nTop-K Sampling\n从概率最高的 K 个 token 中采样, K越大多样性越高\n增加多样性\n需要选择合适的 K 值，通常 K 值在 10 到 50 之间\n\n\nTop-P Sampling\n从累计概率达到 P 的 token 集合中采样，P越大多样性越高\n动态调整候选 token 数量\n需要选择合适的 P 值，通常 P 值在 0.8 到 0.9 之间\n\n\nTemperature Adjustment\n通过调整 temperature 控制随机性，temperature 越大多样性越高\n灵活控制生成文本的多样性\n需要选择合适的 temperature 值， 通常在 0.7 到 1.0 之间\n\n\n\n\n\nTable 2: 总结不同采样方法的优缺点"
  },
  {
    "objectID": "posts/CS336/Ass01/ass01.html#plots",
    "href": "posts/CS336/Ass01/ass01.html#plots",
    "title": "Assignment 01: Tokenization & Language Modeling",
    "section": "6.1 Plots",
    "text": "6.1 Plots\n\n\n\n\n\n\n\n\n\nTraining Loss\n\n\n\n\n\n\n\nTraining Perplexity\n\n\n\n\n\n\n\n\n\nEvaluation Loss\n\n\n\n\n\n\n\nEvaluation Perplexity\n\n\n\n\n\n\nFigure 11: Loss 和 Perplexity 曲线显示\n\n\n\n\n我们可以看到，在完成了10K步的训练之后，模型的训练损失和评估损失都有了明显的下降，困惑度（Perplexity）也有了显著的提升。这表明模型已经学会了一些语言模式，能够更好地预测下一个 token。最后的结果：\neval/loss:0.7857699394226074\neval/perplexity:2.194427490234375\ntrain/loss:0.7850267291069031\ntrain/perplexity:2.192465543746948\n我们看看模型生成的文本：\n\n\n\n\n\n\nFigure 12: 生成文本示例, prompt=“Once upon a time”, max_new_tokens=256, top_k=50, temperature=0.8\n\n\n\n我们可以看到，生成的句子，有一定的连贯性， 但是故事并不是完整的，且有一定的逻辑混乱。这是因为模型规模较小，训练步数有限，无法完全捕捉到复杂的语言结构和故事情节。我们可以通过以下几种方式来提升生成效果：\n\n增加训练的steps：现在是10K，如果增加到15K，20K，效果应该会更好\n增大Batch Size：相对的来说，Batch Size越大，Noise就越小，训练就更加稳当\n增加Context Length：当前的Context Length是256，将其增大到512，或者更大，可以覆盖一个整个完整的故事结构。"
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html",
    "href": "posts/CS336/Lecture02/lec02.html",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "",
    "text": "Lecture 02 主要介绍了 PyTorch 的基础知识和一些实用的工具库，比如 einops。课程内容涵盖了张量操作、数据类型、优化器等方面的内容。并且最重要的是，提出了一个 Resource Accounting， 即训练一个模型，我们需要多大的内存和计算资源。 通过 Resource Accounting，我们可以更好地理解模型训练的资源需求，从而优化模型设计和训练过程。\n课程视频如下所示：",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#memory-accounting",
    "href": "posts/CS336/Lecture02/lec02.html#memory-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.1 Memory Accounting",
    "text": "1.1 Memory Accounting\n所有的数据（包括模型参数、激活值、梯度、优化器状态等）都是以 Tensor 的形式储存。我们有很多种方式创建一个 Tensor，比如：\nx = torch.tensor([[1., 2, 3], [4, 5, 6]])\nx= torch.randn(3, 4)\nx = torch.zeros(2, 5)\nx = torch.empty(10, 10)\n每个 Tensor 都有一个数据类型 (Data Type)，默认的数据类型是 float32 (也称为 FP32)。不同的数据类型会占用不同的内存空间。接下来我们来看看几种常见的数据类型及其内存占用\n\n1.1.1 Common Data Types\n在了解不同的Float Types之前，我们先来了解一下浮点数的表示方法。计算机中的浮点数通常采用 IEEE 754 标准进行表示。浮点数由三部分组成：符号位 (Sign Bit)、指数位 (Exponent Bits) 和尾数位 (Mantissa Bits), 也叫Fraction 。\n\n\n\n\n\n\nFigure 1: 浮点数的表示\n\n\n\n浮点数的值可以通过以下公式计算： \\[\nnumber = (-1)^{Sign} \\times Base^{(Exponent - Bias)} \\times 1.Mantissa\n\\tag{1}\\]\n其中，Base 通常为2，Bias 是一个用于调整指数的偏移量，具体取决于指数位的长度, 通常为2的指数位长度减1的值 为127。\n = b_1·2^{-1} + b_2·2^{-2} + b_3·2^{-3} + … + b_{23}·2^{-23}\n对于 Figure 1 中的浮点数表示：\n\n符号位 (Sign) 为 0，表示正数\n指数位 (Exponent) 为 01111100，转换为十进制为 124，减去 Bias 127 得到 -3\n尾数位 (Mantissa) 为 01000000000000000000000 ，转换为十进制为 0.25，因此 1.Mantissa = 1 + 0.25 = 1.25\n\n将这些值代入公式 Equation 1 中，可以计算出浮点数的值为： \\[\nnumber = (-1)^0 \\times 2^{-3} \\times 1.25 = 0.15625\n\\]\n比如 bias 10000000，转换为十进制为 128，减去 Bias 127 得到 1\n\n\n\n\n\n\nFigure 2: 通过这个在线计算器，验证浮点数的表示方法。\n\n\n\n明白了Float Number的计算方法之后，我们来看看几种常见的数据类型及其内存占用。\n\n1.1.1.1 Float32\n\n\n\n\n\n\nFigure 3: Float32 使用32位 (4字节) 来表示一个浮点数。它由1位符号位、8位指数位和23位尾数位组成，可以表示大约7位十进制有效数字。\n\n\n\nFloat32 也叫 single precision 是深度学习中最常用的数据类型，几乎所有的深度学习框架都默认使用 Float32 作为张量的数据类型。 Float32 可以表示的数值范围大约在 1.18e-38 到 3.4e+38 之间，足以满足大多数深度学习任务的需求。\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(x.dtype)  # 输出: torch.float32\nprint(x.element_size())  # 输出: 4 (每个元素占用4字节)\n\n\n\n\n\n\nFigure 4: Float16 使用16位 (2字节) 来表示一个浮点数。它由1位符号位、5位指数位和10位尾数位组成，可以表示大约3位十进制有效数字。\n\n\n\nFloat16 也叫 half precision，主要用于减少内存占用和加速计算。相比于 Float32，Float16 可以显著降低内存使用量，从而允许我们训练更大的模型或者使用更大的批量大小 (Batch Size)。\nFloat16 可以表示的数值范围大约在 6.1e-5 到 6.5e+4 之间，相比于 Float32 有一定的限制，尤其是在表示非常小或者非常大的数值时可能会出现溢出（Overflow）或者下溢（Underflow）的问题。当这个问题出现时，我们可能会出现NaN的情况，由此导致训练失败。\n\n\n\n\n\n\nTip\n\n\n\n当我们训练神经网络时，损失函数出现NaN的情况，通常是因为数值溢出或者下溢导致的。\n\n\nIn [14]: x = torch.tensor([1e-8], dtype=torch.float16)\n\nIn [15]: x\nOut[15]: tensor([0.], dtype=torch.float16)\n\nIn [8]: x = torch.tensor([1e+8], dtype=torch.float16)\n\nIn [9]: x\nOut[9]: tensor([inf], dtype=torch.float16)\n\n\n1.1.1.2 BFloat16\n\n\n\n\n\n\nFigure 5: BFloat16 使用16位 (2字节) 来表示一个浮点数。它由1位符号位、8位指数位和7位尾数位组成，可以表示大约3位十进制有效数字。\n\n\n\nGoogle 在2018年提出了 BFloat16 (Brain Float Point 16) 数据类型，主要用于深度学习加速。相比于 Float16，BFloat16 保留了与 Float32 相同的指数位长度，因此可以表示更大的数值范围，从而减少了溢出和下溢的风险。相对于Float32， BFloat16 的精度较低，但在许多深度学习任务中，BFloat16 的精度已经足够使用。\nIn [11]: x\nOut[11]: tensor([1.0014e+08], dtype=torch.bfloat16)\n\nIn [12]: x = torch.tensor([1e-8], dtype=torch.bfloat16)\n\nIn [13]: x\nOut[13]: tensor([1.0012e-08], dtype=torch.bfloat16)\nIn [16]: torch.finfo(torch.float32)\nOut[16]: finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n\nIn [17]: torch.finfo(torch.float16)\nOut[17]: finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n\nIn [18]: torch.finfo(torch.bfloat16)\nOut[18]: finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\n1.1.1.3 FP8\nFP8 是一种8位浮点数表示方法，通常用于极端内存受限的场景（比如将模型部署到边缘设备）。FP8 有两种主要格式：E4M3 和 E5M2。\n\n\n\n\n\n\nFigure 6: FP8 使用8位 (1字节) 来表示一个浮点数。E4M3 由1位符号位、4位指数位和3位尾数位组成；E5M2 由1位符号位、5位指数位和2位尾数位组成。\n\n\n\nE4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).\n\n\n1.1.1.4 Other Data Types\n除了上述几种常见的数据类型之外，还有一些其他的数据类型，比如 int8 和 int4。这些数据类型通常用于量化 (Quantization) 技术，通过将浮点数转换为整数来减少内存占用和加速计算。\n\n\n\nData Type\nDescription\nBits per Value\nBytes per Value\n\n\n\n\nfloat32\nSingle Precision\n32\n4\n\n\nfloat16\nHalf Precision\n16\n2\n\n\nbfloat16\nBrain Float\n16\n2\n\n\nint8\n8-bit Integer\n8\n1\n\n\nint4\n4-bit Integer\n4\n0.5\n\n\n\n我们可以看到，不同的数据类型有不同的内存占用。选择合适的数据类型可以帮助我们在内存受限的情况下训练更大的模型或者使用更大的批量大小：\n\n用 float32 进行训练，适用于大多数任务，但内存占用较高。\n用 float16 进行训练，可以显著减少内存占用，但需要注意数值溢出和下溢的问题。\n用 bfloat16 进行训练，可以在减少内存占用的同时，保持较大的数值范围，适用于大多数深度学习任务。\n用 int8 或 int4 进行量化训练，可以极大地减少内存占用，但需要进行额外的量化和反量化操作，适用于部署阶段。\n\n因此在训练的过程中，我们通常采用一种Mixed Precision Training的方法，来平衡内存占用和数值精度的问题。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#compute-accounting",
    "href": "posts/CS336/Lecture02/lec02.html#compute-accounting",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.2 Compute Accounting",
    "text": "1.2 Compute Accounting\n在了解了内存占用之后，我们来看看计算资源的需求。计算资源主要取决于模型的参数量和训练数据的规模。\n\n1.2.1 Tensor on GPUs\n当我们创建一个 Tensor 时，它默认是在 CPU 上创建的。如果我们想要在 GPU 上进行计算，需要将 Tensor 移动到 GPU 上：\nx = torch.tensor([1.0, 2.0, 3.0])\nx.device  # 输出: cpu\nx = x.cuda()  # 将 Tensor 移动到 GPU 上\nx.device  # 输出: cuda:0\n\n\n\n\n\n\nFigure 7: 我们将Tensor从CPU的RAM上，移动到DRAM上。\n\n\n\n我们可以通过以下方式创建一个直接在 GPU 上的 Tensor：\nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n可以通过GPU的内存情况来查看当前 GPU 上的内存使用情况：\nimport torch\nprint(torch.cuda.memory_summary())\nmemory_allocated = torch.cuda.memory_allocated() \nx = torch.tensor([1.0, 2.0, 3.0], device='cuda')\nmemory_allocated_after = torch.cuda.memory_allocated()\nprint(f\"Memory allocated before: {memory_allocated}, after: {memory_allocated_after}\")",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#tensor-operations",
    "href": "posts/CS336/Lecture02/lec02.html#tensor-operations",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.3 Tensor Operations",
    "text": "1.3 Tensor Operations\nPyTorch Tensor 是一个Pointer，指向一块连续的内存区域。我们可以通过strides来访问Tensor中的数据。\n\n\n\n\n\n\nFigure 8\n\n\n\nTensor 有很多操作，比如 reshape, permute, transpose 等等。这些操作通常不会改变数据的存储方式，而是通过修改 strides 来实现对数据的不同视图 (View)。这是十分高效的，因为我们不需要进行数据的复制 (Copy)，只需要修改 Tensor 的元数据 (Metadata)。不过需要小心的是，当我们修改 Tensor 的数据时，可能会影响到原始数据，因为它们共享同一块内存区域。\n有一些操作会导致数据变得不连续 (Non-Contiguous)，比如 transpose 和 permute。这些操作会改变数据的存储顺序，从而导致数据在内存中不再是连续存储的。这时，我们可以使用 contiguous() 方法来创建一个新的连续存储的 Tensor。\n\n\n\n\n\n\nNote\n\n\n\n.transpose().contiguous() 的操作我们在一 Attention 的计算中经常会用到。\n\n\nElement-wise 操作 (比如加法、乘法等) 通常要求输入的 Tensor 是连续存储的。如果输入的 Tensor 是不连续的，PyTorch 会自动调用 contiguous() 方法来创建一个新的连续存储的 Tensor，从而保证操作的正确性。，比如：\nx = torch.tensor([1, 4, 9])\nx.pow(2)\nx.sqrt()\nx.rsqrt()\nx + x \nx * 2\nx.triu()\n\n1.3.1 Matrix Multiplication\n最重要的也是最常用的操作之一是矩阵乘法 (Matrix Multiplication)，在深度学习中，矩阵乘法被广泛应用于神经网络的前向传播和反向传播过程中。\nA = torch.randn(3, 4)\nB = torch.randn(4, 5)\nC = torch.matmul(A, B)  # C 的形状为 (3,5)\nC = A @ B  # 另一种矩阵乘法的写法\nC = A.mm(B)  # 另一种矩阵乘法的写法",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#gradient-calculation",
    "href": "posts/CS336/Lecture02/lec02.html#gradient-calculation",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.4 Gradient Calculation",
    "text": "1.4 Gradient Calculation\n了解了矩阵乘法之后，我们来看看如何计算梯度 (Gradient)。在深度学习中，梯度是用来更新模型参数的关键。PyTorch 提供了自动微分 (Autograd) 功能，可以自动计算张量的梯度。\n假设我们有个简单的神经网络层：\n\\[\nY = 0.5 * (X  W - 5)^2\n\\]\n在前置的传播过程中，我们计算输出 Y：\nx = torch.randn([1., 2, 3])  # 输入张量 X\nw = torch.randn([1., 2, 3], requires_grad=True)  # 权重张\ny = 0.5 * (x @ w - 5) ** 2  # 前向传播计算输出 Y\npred_y = x @ w\nloss = 0.5 * (pred_y - 5) ** 2\n在反向传播过程中，我们计算梯度：\nloss.backward()  # 反向传播计算梯度\nprint(w.grad)  # 输出权重 w 的梯度",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#summary",
    "href": "posts/CS336/Lecture02/lec02.html#summary",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nForward Pass: 2 * Number of data points * Number of parameters Backward Pass: 4 * Number of data points * Number of parameters\nTotal: 6 * Number of data points * Number of parameters\n\n1.5.1 Einops Library\n在处理高维张量时，张量的重排 (Rearrangement) 和变形 (Reshaping) 是非常常见的操作。传统的方法通常需要多行代码，并且容易出错。einops 是一个强大的库，可以简化这些操作，使代码更加简洁和易读。\n\n\n\n\n\n\n\nTip\n\n\n\n如果以上的内容比较难以理解的话，可以访问这个链接。 里面有非常详细的einops教程，包含了很多例子和可视化的图示。\n\n\n在这里就不具体展开了。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#tensor-flops",
    "href": "posts/CS336/Lecture02/lec02.html#tensor-flops",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "1.6 Tensor Flops",
    "text": "1.6 Tensor Flops\n了解了内存占用和计算资源之后，我们来看看如何计算 Tensor 的 FLOPs (Floating Point Operations)。FLOPs 是衡量计算复杂度的一个重要指标，表示每秒钟可以执行多少次浮点运算。其中两个常见的指标是：\n\nFLOPs: 总共需要执行的浮点运算次数\nFLOP/s 也写作FLOPS： 每秒钟可以执行的浮点运算次数\n\n对于两个 矩阵 A (形状为 m x n) 和 B (形状为 n x p) 的矩阵乘法 C = A @ B，我们可以计算出 FLOPs 如下： \\[\nFLOPs = 2 * m * n * p\n\\]\n其中，乘法操作需要 m * n * p 次， 加法操作也需要 m * n * p 次，因此总共需要 2 * m * n * p 次浮点运算。\n对于其他的张量操作，我们也可以类似地计算 FLOPs。了解 FLOPs 可以帮助我们评估模型的计算复杂度，从而优化模型设计和训练过程。比如：\n\nElement-wise 操作 (比如加法、乘法等) 的 FLOPs 通常与张量的元素数量成正比 \\(\\mathcal{O}(m \\times n)\\)\nAddition of two matrices of shape (m, n): FLOPs = m * n\n\n由此可见，矩阵乘法的计算复杂度远高于 Element-wise 操作，因此在设计模型时，我们通常会尽量减少矩阵乘法的次数，从而降低计算复杂度。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#model-definition",
    "href": "posts/CS336/Lecture02/lec02.html#model-definition",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.1 Model definition",
    "text": "2.1 Model definition\n首先，我们需要定义一个模型。模型通常由多个层 (Layer) 组成，每个层都有自己的参数 (Parameters)。现代的LLM模型通常是基于 Transformer (Vaswani et al. 2023) 架构构建的。 具体的内容，会在Lecture 03中详细介绍，在这里就先不展开了。\n\n\n\n\n\n\nNote\n\n\n\n对于Transformer比较陌生的同学，可以参考我这一篇笔记 100-Paper with Code: 01 Transformer",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#parameter-initialization",
    "href": "posts/CS336/Lecture02/lec02.html#parameter-initialization",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.2 Parameter initialization",
    "text": "2.2 Parameter initialization\n定义好模型之后，我们需要初始化模型的参数。参数的初始化方式会影响模型的训练效果和收敛速度。常见的初始化方法有随机初始化 (Random Initialization)、Xavier 初始化 (Xavier Initialization) 和 He 初始化 (He Initialization) 等等。\n许多加速器（比如 GPU 和 TPU）都对矩阵乘法进行了高度优化，利用并行计算和专用硬件单元来加速矩阵乘法的计算过程。因此，在深度学习中，尽量将计算任务转化为矩阵乘法，可以显著提升计算效率。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#optimizer-selection",
    "href": "posts/CS336/Lecture02/lec02.html#optimizer-selection",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.3 Optimizer Selection",
    "text": "2.3 Optimizer Selection\n在训练模型时，我们需要选择一个优化器 (Optimizer) 来更新模型的参数。 常见的优化器有随机梯度下降 (SGD)、动量法 (Momentum)、Adam 和 AdamW 等等。不同的优化器有不同的更新规则和超参数 (Hyperparameters)，选择合适的优化器可以帮助我们更快地收敛到最优解。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#loss-function",
    "href": "posts/CS336/Lecture02/lec02.html#loss-function",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.4 Loss Function",
    "text": "2.4 Loss Function\n在训练模型时，我们需要定义一个损失函数 (Loss Function) 来衡量模型的预测结果与真实标签之间的差距。常见的损失函数有均方误差 (Mean Squared Error, MSE)、交叉熵损失 (Cross Entropy Loss) 等等。选择合适的损失函数可以帮助我们更好地优化",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture02/lec02.html#training-loop",
    "href": "posts/CS336/Lecture02/lec02.html#training-loop",
    "title": "Lecture 02: PyTorch Basics & Resource Accounts",
    "section": "2.5 Training Loop",
    "text": "2.5 Training Loop\n当我们定义好模型、初始化参数、选择优化器和损失函数之后，我们就可以开始训练模型了。训练过程通常包括以下几个步骤：\n加载数据 (Data Loading): 从数据集中加载训练数据，通常使用批量 (Batch) 的方式进行加载。 前向传播 (Forward Pass): 将输入数据传递给模型，计算模型的输出。 计算损失 (Loss Calculation): 使用损失函数计算模型输出与真实标签之间的差距。 反向传播 (Backward Pass): 计算损失函数相对于模型参数的梯度。 参数更新 (Parameter Update): 使用优化器根据计算得到的梯度更新模型参数。\n重复以上步骤，直到模型收敛或者达到预定的训练轮数 (Epochs)。\n\n2.5.1 Randomness Control\n在训练模型时，随机性 (Randomness) 是不可避免的。比如，参数的初始化、数据的打乱 (Shuffling) 和批量的选择 (Batch Selection)等操作都涉及到随机性。为了保证实验的可重复性 (Reproducibility)，我们通常需要控制随机数生成器 (Random Number Generator, RNG) 的种子 (Seed)。\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed):    \n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(42)\n\n\n2.5.2 Data Loading\n\n\n2.5.3 Checkpointing\n在训练大型模型时，训练过程可能会非常耗时，并且容易受到各种意外情况的影响，比如断电、系统崩溃等。为了避免训练过程中的数据丢失，我们通常会使用检查点 (Checkpointing) 技术来保存模型的状态。\nModel Checkpointing 通常保存以下几个方面的信息：\n\n模型参数 (Model Parameters): 保存模型的权重和偏置等参数。\n优化器状态 (Optimizer State): 保存优化器的状态，比如动量 (Momentum ) 和学习率 (Learning Rate) 等信息。\n学习率调度器状态 (Learning Rate Scheduler State): 保存学习率调度器的状态。\n训练进度 (Training Progress): 保存当前的训练轮数 (Epochs) 和批量索引 (Batch Index) 等信息。\n\n在 GPU 上进行计算时，数据传输的速度通常是一个瓶颈。为了提高数据传输的效率，我们可以使用 Pinned Memory (也叫 Page-Locked Memory)。Pinned Memory 是一种特殊的内存区域，可以加速主机 (Host) 和设备 (Device) 之间的数据传输。\n\n\n\n\n\n\nFigure 9: 如图所示，Pinned memory 可以作为设备(Device)到主机(Host)拷贝的中转区，直接在 pinned memory 中分配主机数组，就能避免 pageable 内存与 pinned 内存之间的额外拷贝开销，从而提升数据传输效率。\n\n\n\n这篇文章中介绍了4种常见的加速数据传输的方法：\n\n利用 Numpy Memmap 处理大数据集: 通过内存映射技术，只将数据集的一部分加载到内存中，减少内存使用，提高数据加载速度。\n多利用 torch.from_numpy 函数: 直接将 NumPy 数组转换为 PyTorch 张量，避免不必要的数据复制，提高数据传输效率。\n将 num_workers 设置为大于0: 通过多线程数据加载，提高数据预处理和加载的并行度，减少数据加载时间。\n使用 Pinned Memory 加速主机与设备之间的数据传输: 通过将数据存储在固定内存中，减少数据传输的延迟，提高\n\nfrom torch.utils.data import DataLoader\n\n# some code\n\nloader = DataLoader(your_dataset, ..., pin_memory=True)\ndata_iter = iter(loader)\n\nnext_batch = data_iter.next() # start loading the first batch\nnext_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n\nfor i in range(len(loader)):\n    batch = next_batch \n    if i + 2 != len(loader): \n        # start copying data of next batch\n        next_batch = data_iter.next()\n        next_batch = [ _.cuda(async=True) for _ in next_batch]\n这几个方法可以显著提升数据传输和加载的效率，尤其在处理大规模数据集时效果尤为明显。在我们完成Assignment 01时，会用到这些技巧来优化数据加载过程。\n\n\n2.5.4 Mixed Precision Training\n在之前的内容中，我们介绍了不同的数据类型及其内存占用。在实际的模型训练过程中，我们通常会采用混合精度训练 (Mixed Precision Training) 的方法，来平衡内存占用和数值精度的问题。\n那那些需要混合精度训练呢？通常在以下几种情况下，我们会考虑使用混合精度训练：\n\nbfloat16 或者 fp8 作为前向的计算数据类型（activations）\nfloat32 作为梯度计算的数据类型, 并且是用 float32 来更新参数\n优化器状态 (Optimizer States) 使用 float32 来存储\n\nMicikevicius et al. (2018) 提出了一种混合精度训练的方法，称为 Loss Scaling。Loss Scaling 的基本思想是通过放大损失函数的值，来避免在使用低精度数据类型时出现数值下溢 (Underflow) 的问题。\nLoss Scaling 的具体步骤如下：\n\n在前向传播过程中，计算损失函数的值，并将其乘以一个放大因子 (Scaling Factor)。\n在反向传播过程中，计算梯度，并将其除以放大因子。\n使用优化器更新模型参数。 不过，当我们用 bfloat16 进行前向计算时，我们可以不需要使用 Loss Scaling，因为 bfloat16 已经有足够的数值范围来避免下溢的问题。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 02: PyTorch Basics & Resource Accounts"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html",
    "href": "posts/CS336/Lecture16&17/lec16.html",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "",
    "text": "在之前的课程中，我们学习了PPO以及DPO的算法，我们因此也从GPT得到了ChatGPT。但这距离现在的 Reasoning Model，还有一段距离。在这节课中，我们将学习新的一系列算法，也就是RLVR(Reinforcement Learning from Verified Rewards)，通过这些算法，我们可以从ChatGPT突破到ChatGPT-o1。\n在了解RLVR之前，我们先来看一下，为什么不直接用PPO和DPO来做LLM的推理强化学习。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#why-not-ppo",
    "href": "posts/CS336/Lecture16&17/lec16.html#why-not-ppo",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "1.1 Why Not PPO?",
    "text": "1.1 Why Not PPO?\nPPO理论上就一个clip目标，但落地要处理很多组件, 包括：\n\nrollout采样/缓存、old logprob、ratio、clip、KL penalty/target KL\nadvantage估计（GAE）、return计算\n\n对LLM来说，rollout很贵（推理生成token），所以还会引入“采一次rollout，更新多次”的机制，这又把实现复杂度继续抬高。并且，最主要的问题是：\n\nPPO通常需要一个 value function 来计算Advantage.\n\n这意味着：\n\n额外的模型（value model）+内存开销\n额外的训练目标（value loss）\n\n\n“Value model (memory hungry, involves additional tuning)”",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#why-not-dpo",
    "href": "posts/CS336/Lecture16&17/lec16.html#why-not-dpo",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "1.2 Why Not DPO?",
    "text": "1.2 Why Not DPO?\nDPO的强项是偏好对比数据 \\((p, y_i, y_j)\\)，但在可验证奖励的推理RL里，数据常见是：\n\n一个prompt，\n一个回答，\nreward=0/1（对/错）或分数\n\n数据不是天然的pairwise偏好，而是pointwise reward。另外，DPO常见使用方式偏Off-Policy：先收集对比数据，再训练。而推理RL通常更像在线：模型不断变强 → 不断rollout → 不断用最新策略生成新数据训练（on-policy味道更强）。GRPO/PPO天然适配这种循环。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#grpo-objective",
    "href": "posts/CS336/Lecture16&17/lec16.html#grpo-objective",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "3.1 GRPO Objective",
    "text": "3.1 GRPO Objective\nGRPO 的目标函数如下： \\[\n\\mathcal{J}_{\\text{GRPO}}(\\pi_\\theta) = \\frac{1}{G}\\sum_{i=1}^{G}\n\\textcolor{red}{\\frac{1}{|o_i|}} \\sum_{t = 1}^{|o_i|}\n\\Bigg\\{\n\\min \\Big[\n     \\rho(o_{i,t}) \\hat{A}_{i, t},\n     \\text{clip}\\Big(\n            \\rho(o_{i,t}), 1 - \\epsilon_{\\text{clip}}, 1 + \\epsilon_{\\text{clip}}\n        \\Big) \\hat{A}_{i, t}\n    \\Big]    \n\\Bigg\\} + \\textcolor{gray}{\\beta \\, \\text{KL}\\big(\\pi_\\theta || \\pi_{\\text{ref}}\\big)}\n\\tag{1}\\]\n其中：\n\\[\n\\rho(o_{i,t}) = \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,&lt;t})}{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i,&lt;t})}\n\\tag{2}\\]\n是importance sampling ratio，用来把当前策略和reference policy联系起来。\n\\(\\textcolor{gray}{\\beta \\, \\text{KL}\\big(\\pi_\\theta || \\pi_{\\text{ref}}\\big)}\\) 是 KL penalty，用来防止当前策略偏离参考策略太远。(不过在实际的实现中，我们通常会忽略这个KL penality，因为GRPO的采样本身就是从参考策略采样的，所以偏离不会太大)\n对每个 prompt 采样一组（group）回答,\n\n用组内相对奖励当 advantage（基线），\n再做 policy-gradient 更新，同时用 KL 约束不要偏离参考策略( 不过在实际实现中，KL penalty是加在loss里，而不是硬约束 )。\n\n我们来看一下Python代码实现：\ndef compute_loss_grpo(\n    log_probs: torch.Tensor,      # (B, G, T) log probs under current policy\n    old_log_probs: torch.Tensor,  # (B, G, T) log probs under old policy (detached)\n    response_mask: torch.Tensor,  # (B, G, T) mask for valid response tokens\n    advantage: torch.Tensor,    # (B, G) advantage per response\n    clip_range: float = 0.2,\n):\n    B, G, T = log_probs.size()\n\n    important_ratio = torch.exp(log_probs - old_log_probs)  # (B, G, T)\n\n    # Broadcast advantage to token level\n    advantage_tok = advantage.unsqueeze(-1).expand_as(important_ratio)  # (B, G, T)\n\n    unclipped = important_ratio * advantage_tok\n    clipped = torch.clamp(important_ratio, 1 - clip_range, 1 + clip_range) * advantage_tok\n\n    pg_loss_tok = -torch.min(unclipped, clipped)  # (B, G, T)\n\n    # Length normalization (GRPO does this)\n    len_per_response = response_mask.sum(dim=-1)  # (B, G)\n    pg_loss_tok = pg_loss_tok * response_mask  # mask out non-response tokens\n    pg_loss_seq = pg_loss_tok.sum(dim=-1) / len_per_response  # (B, G)\n\n    # Final loss: mean over batch and group\n    loss = pg_loss_seq.sum()\n    loss = loss / (B * G)\n    # loss = pg_loss_seq.mean()  # alternative: mean over all tokens\n\n    return loss\n具体的实现中，我们会把按回答长度归一化（pg_loss_seq = pg_loss_tok.sum(dim=-1) / len_per_response），这是GRPO的一个关键设计，我们后面会重点讲。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#advantage-calculation-in-grpo",
    "href": "posts/CS336/Lecture16&17/lec16.html#advantage-calculation-in-grpo",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "3.2 Advantage Calculation in GRPO",
    "text": "3.2 Advantage Calculation in GRPO\n接下来我们重点看看 GRPO 的 Advantage 计算：\n\n对每个 prompt（比如一道数学题）：\n\n采样 G 个回答：\\(\\{y_1, y_2, \\dots, y_G\\}\\)\n这个 prompt 就是一组（group），组内回答共享同一难度, 并且有了 \\(\\{(p, y_1, r_1), (p, y_2, r_2), \\dots, (p, y_G, r_G)\\}\\)，其中 \\(r_i\\) 是回答 \\(y_i\\) 的奖励。\n\n接下来，我们通过组内统计量，来计算 Advantage： \\[\n\\hat{A}_{i, t}=\\frac{r_i-\\mu}{\\sigma + \\epsilon } \\quad \\text{where} \\quad \\mu=\\frac{1}{G}\\sum_{j=1}^{G} r_j, \\quad \\sigma=\\sqrt{\\frac{1}{G}\\sum_{j=1}^{G}(r_j-\\mu)^2}\n\\tag{3}\\]\n其中：\n\n用 组内均值 \\(\\mu\\) 当 baseline（反映题目难度）\n用 组内方差 \\(\\sigma\\) 做归一化（控制更新尺度）\n\\(\\epsilon\\) 是一个小常数，防止除零\n\n其中，\\(\\hat{A}_{i, t}\\) 是回答 \\(y_i\\) 在时间步 \\(t\\) 的 advantage, 也就是每个action(token \\(o_{i,t}\\)) 对应的 Advantage, 在GRPO中，同一个回答 \\(y_i\\) 的所有token共享同一个 advantage值。\ndef compute_advantage(\n    rewards: torch.Tensor,  # (B, G)\n    epsilon: float = 1e-4\n):\n    mu = rewards.mean(dim=1, keepdim=True)  # (B, 1)\n    sigma = rewards.std(dim=1, keepdim=True)  # (B, 1)\n\n    advantage = (rewards - mu) / (sigma + epsilon)  # (B, G)\n    return advantage",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#grpo-algorithm",
    "href": "posts/CS336/Lecture16&17/lec16.html#grpo-algorithm",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "3.3 GRPO Algorithm",
    "text": "3.3 GRPO Algorithm\n了解了GRPO的Objective Function (Equation 1) 和Advantage计算(Equation 3) 后，我们来看一下GRPO的整体算法流程：\n\n\n\\begin{algorithm} \\caption{Iterative Group Relative Policy Optimization (GRPO)}\\begin{algorithmic} \\Require initial policy model $\\pi_{\\theta_{\\text{init}}}$; reward model(s) $r_\\phi$; task prompts $\\mathcal{D}$; hyperparameters $\\epsilon, \\beta, \\mu$; group size $G$; iterations $I$; steps $M$ \\Ensure trained policy $\\pi_\\theta$ \\State $\\pi_\\theta \\gets \\pi_{\\theta_{\\text{init}}}$ \\For{$\\text{iteration} = 1, \\ldots, I$} \\State $\\pi_{\\text{ref}} \\gets \\pi_\\theta$ \\Comment{reference (anchor) policy} \\For{$\\text{step} = 1, \\ldots, M$} \\State Sample a batch $\\mathcal{D}_b$ from $\\mathcal{D}$ \\State $\\pi_{\\theta_{\\text{old}}} \\gets \\pi_\\theta$ \\Comment{snapshot old policy} \\ForAll{$q \\in \\mathcal{D}_b$} \\State Sample $G$ outputs $\\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot \\mid q)$ \\State Compute rewards $\\{r_i\\}_{i=1}^{G}$ by $r_i \\gets r_\\phi(q, o_i)$ \\State Compute group-relative advantages $\\{\\hat{A}_{i,t}\\}$ for each token $t$ in each $o_i$ \\EndFor \\For{$\\text{GRPO-iter} = 1, \\ldots, \\mu$} \\State Update $\\pi_\\theta$ by maximizing the GRPO objective \\EndFor \\State Update $r_\\phi$ via continuous training using a replay mechanism \\EndFor \\EndFor \\State \\Return $\\pi_\\theta$ \\end{algorithmic} \\end{algorithm}\n\n\n对于每次迭代：\n\n先把当前策略 \\(\\pi_\\theta\\) 作为参考策略 \\(\\pi_{\\text{ref}}\\)（anchor policy）\n然后，对于每个 prompt \\(q\\)：\n\n从旧策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 采样 \\(G\\) 条回答 \\(\\{o_i\\}\\)\n计算每条回答的奖励 \\(\\{r_i\\}\\)（通过奖励模型）\n计算组内相对优势 \\(\\{\\hat{A}_{i,t}\\}\\)\n最后，用 GRPO 目标函数更新策略 \\(\\pi_\\theta\\)\n\n\n这种设计避免了 PPO 里对 Value Function 的依赖，从而简化了训练流程， 同时我们看到，对于每个Rollout，我们只更新一次策略(On-Policy)，而不是多次，这也降低了实现复杂度。（不过这也是一个很大的Trade-Off，我们可以对于同一组数据多次更新策略（Off-Policy），从而提升样本效率，但会增加实现复杂度）",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#bias-in-grpo",
    "href": "posts/CS336/Lecture16&17/lec16.html#bias-in-grpo",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "3.4 Bias in GRPO",
    "text": "3.4 Bias in GRPO\n接下来，我们来自己分析一下GRPO的Objective Function (Equation 1) ，可以发现，它其实存在两个Bias：\n\nBiased Gradient：GRPO的梯度估计是有偏的\nLength Normalization Bias：按回答长度归一化引入的偏置\n\n我们来具体分析一下这两个Bias。\n\n\n\n\n\n\nFigure 3: 该图展示了 GRPO 的两类优化偏置：对同一问题 \\(q\\) 采样多条回答 \\(o_i\\) 后，GRPO 的“有效 advantage”满足 \\(a_{i,t}=\\frac{1}{\\mathrm{std}(R)}\\cdot \\frac{\\tilde A_{i,t}}{|o_i|}\\)，其中 \\(\\tilde A_{i,t}=R(q,o_i)-\\mathrm{mean}(R)\\) 是组内中心化（相对）奖励；由于额外除以组内奖励标准差 \\(\\mathrm{std}(R)\\) 与回答长度 \\(|o_i|\\)，训练会对不同问题与不同长度回答施加不同权重：蓝色圆点大小表示 \\(1/\\mathrm{std}(R)\\) 对“问题级”更新强度的缩放，橙色箭头长度表示 \\(\\tilde A_{i,t}/|o_i|\\) 对“回答级”更新幅度的缩放（箭头向上为正 advantage、向下为负），从而使优化偏向某些题目与某些长度的输出。\n\n\n\n\n3.4.1 Biased Gradient\n第一个问题是 GRPO 的梯度估计是biased，这主要是因为它使用了依赖采样结果的 baseline。 我们知道，要保持一个un-biased梯度估计，baseline 只能是不依赖动作的常数，它可以是Value Function的估计，或者一个常数，但不能是依赖采样结果的量。\n具体来说：\nPolicy Gradient Theorem 告诉我们，策略梯度是： \\[\n\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{ y \\sim \\pi_\\theta(\\cdot|x)} \\Big[ \\textcolor{green}{r(x, y)} \\nabla_\\theta \\log \\pi_\\theta(y|x) \\Big]\n\\tag{4}\\]\n\n\nNOTE\n\n\n在这里，\\(x\\) 是环境状态（在LLM中是prompt），\\(y\\) 是动作（在LLM中是token序列），\\(r(x,y)\\) 是奖励函数，\\(\\pi_\\theta(y|x)\\) 是策略（在LLM中是语言模型）。 为了简化起见，我们忽略了状态分布 \\(D\\)，假设 \\(x\\) 是固定的。实际中，我们会对 \\(x\\) 也取期望： \\[\n\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\textcolor{red}{x \\sim D}, y \\sim \\pi_\\theta(\\cdot|x)} \\Big[ r(x, y) \\nabla_\\theta \\log \\pi_\\theta(y|x) \\Big]\n\\]\n\n\n如果我们引入一个 baseline \\(b(x)\\)，并且它不依赖动作，那么梯度变成： \\[\n\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{ y \\sim \\pi_\\theta(\\cdot|x)} \\Big[  \\textcolor{green}{\\Big(r(x, y) - b(x)\\Big)} \\nabla_\\theta \\log \\pi_\\theta(y|x) \\Big]\n\\tag{5}\\]\n只要 \\(b(x)\\) 不依赖 \\(y\\)（action, 在LLM中就是token），这个梯度估计就是un-biased。\n\\[\n\\begin{split}\n\\mathbb{E}_{y \\sim \\pi_\\theta(\\cdot|x)} \\Big[ b(x) \\nabla_\\theta \\log \\pi_\\theta(y|x) \\Big]\n& = b(x) \\mathbb{E}_{y \\sim \\pi_\\theta(\\cdot|x)} \\Big[  \\nabla_\\theta \\log \\pi_\\theta(y|x) \\Big]  \\\\\n&= b(x) \\sum_y \\pi_\\theta(y|x)\\,\\nabla_\\theta \\log \\pi_\\theta(y|x) \\\\\n&= b(x) \\sum_y \\nabla_\\theta \\pi_\\theta(y|x) \\quad \\text{(Log-Derivative Trick)} \\\\\n&= b(x) \\nabla_\\theta \\sum_y \\pi_\\theta(y|x) \\\\\n&= b(x) \\nabla_\\theta 1 \\\\\n&= 0  \\\\\n\\end{split}\n\\tag{6}\\]\n但是 GRPO 里的 \\(\\sigma(y_{1:G})\\) 依赖于组内所有采样回答的奖励 \\(\\{r_1, r_2, \\dots, r_G\\}\\)，而这些奖励本身又依赖于采样回答 \\(\\{y_1, y_2, \\dots, y_G\\}\\), 于是，我们的梯度变成：\n\\[\n\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{y_{1:G} \\sim \\pi_\\theta}\\Big[\\underbrace{\\frac{r_i-\\mu(y_{1:G})}{\\sigma(y_{1:G})}}_{\\text{依赖采样}}\\,\\nabla_\\theta\\log\\pi_\\theta(y_i|x)\\Big]\n\\tag{7}\\]\n我们来看看，这个梯度为什么是有偏的： \\[\n\\mathbb E_{y_{1:G}}\\left[\\frac{r_i-\\mu(y_{1:G})}{\\sigma(y_{1:G})}\\,\\nabla_\\theta\\log\\pi_\\theta(y_i|x)\\right]\n\\neq\n\\underbrace{\\mathbb E\\left[\\frac{r_i-\\mu(y_{1:G})}{\\sigma(y_{1:G})}\\right]}_{\\text{不能提出来}}\n\\cdot\n\\underbrace{\\mathbb E[\\nabla_\\theta\\log\\pi_\\theta(y_i|x)]}_{=0}\n\\tag{8}\\]\n因为 \\(\\frac{r_i-\\mu(y_{1:G})}{\\sigma(y_{1:G})}\\) 也依赖于采样结果 \\(y_{1:G}\\)，所以与 \\(\\nabla_\\theta\\log\\pi_\\theta(y_i|x)\\) 不能拆开期望，从而导致梯度估计有偏。\n\n\nNOTE\n\n\n当两个随机变量 \\(X, Y\\) 独立时，\\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\)；但当 \\(X, Y\\) 不独立时，这个等式不成立。这也就是为什么，在 Equation 6 中，我们可以把 \\(b(x)\\) 提出来，因为它不依赖 \\(y\\)，所以与 \\(\\nabla_\\theta \\log \\pi_\\theta(y|x)\\) 独立；但在 Equation 8 中，\\(\\frac{r_i-\\mu(y_{1:G})}{\\sigma(y_{1:G})}\\) 依赖于采样结果 \\(y_{1:G}\\)，所以与 \\(\\nabla_\\theta\\log\\pi_\\theta(y_i|x)\\) 不独立，不能拆开期望。\n\n\n这种方式，我们的梯度估计就不再是un-biased的，而是被一个与采样相关的随机因子重新加权了。\nBias 主要出在了 \\(\\sigma\\) 上，我们接下来具体分析它带来的影响:\n\n当某个 prompt 的 group 里 奖励方差很小（组内Reward全对或者全错）时，\\(\\frac{1}{\\sigma}\\) 会非常大 → 这个 prompt 对梯度更新的权重被放大。\n当奖励方差大时，更新被缩小。\n\n这就会导致，训练过程中，模型会更关注“极端题”（全对/全错），而不是“中等难度、最能学到东西的题”。\n\n\nQuestion: 为什么，\\(\\mu\\)（组内均值）不会带来Bias呢？\n\n\n因为 \\(\\mu\\) 只是一个平移，它不会改变梯度的权重，只会改变梯度的方向（让模型更关注相对更好的回答）。而 \\(\\sigma\\) 是一个缩放，它会改变梯度的权重，从而引入Bias。\n\n\n\n\n3.4.2 Length Normalization Bias\n在 Equation 1 中，有一个 按回答长度归一化 的项 \\(\\frac{1}{|o_i|}\\)，也就是把每条回答的 token-loss 平均化。\n这会带来response-level length bias：当把一条回答的梯度按长度平均时，会改变“同样一条回答”在优化中的有效权重。（很抽象对不对，我们来看两个具体情况）\n\n优势为正（答对/更好，\\(\\hat{A} &gt; 0\\)） 在这种情况下，更新方向是“提高这条回答概率”，但除以回答长度 \\(|o_i|\\)，这意味着越短的回答，单位长度的梯度越大， 每个 token 的贡献更大。于是，模型学到：正确答案要尽量短（因为更“划算”）。\n优势为负（答错/更差，\\(\\hat{A} &lt; 0\\)） 在这种情况下，更新方向是“降低这条回答概率”（惩罚），但除以回答长度 \\(|o_i|\\)，这意味着越长的回答，惩罚被摊薄，每个 token 的负更新更小。于是，模型学到：错误答案反而更愿意写长（因为“受罚更轻”）。\n\n这就是为什么有时候CoT越来越长，但是Reward还是不升反降的现象——模型学会了通过输出更长的回答来“规避惩罚”，而不是通过提高回答质量来获得更高的奖励。\n\n\n\n\n\n\nFigure 4: 该张图展示了 DeepSeek-R1-Zero 在训练过程中，模型平均每条回答长度随训练步数的变化。横轴是训练步数（Steps），纵轴是平均每条 response 的长度（token 数）。可以看到，随着训练推进，模型输出长度呈持续上升趋势：从训练初期的几百到上千 token，逐步增长到后期的一万甚至接近两万 token，同时波动幅度也明显扩大。这一现象直观反映了 R1-Zero 在仅使用可验证奖励进行强化学习时，模型倾向于生成越来越长的推理链（CoT），体现出典型的“长度膨胀”行为，而不一定对应更高的实际解题效率。\n\n\n\n接下来，我们来看 Dr.GRPO(Liu et al. 2025) 是如何解决这两个问题的。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#dr.grpo",
    "href": "posts/CS336/Lecture16&17/lec16.html#dr.grpo",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "3.5 Dr.GRPO",
    "text": "3.5 Dr.GRPO\nDr.GRPO 是 GRPO 的一个变体，解决了上面提到的两个问题：\n\n去掉 \\(\\frac{1}{\\sigma}\\)，避免 biased gradient\n去掉按回答长度归一化，避免长度偏置\n\n我们来看看改动之后的目标函数:\n\\[\n\\mathcal{J}_{\\text{Dr.GRPO}}(\\pi_\\theta) = \\frac{1}{G}\\sum_{i=1}^{G}\n\\sum_{t = 1}^{|o_i|}\n\\Bigg\\{\n\\min \\Big[\n     \\rho(o_{i,t}) \\hat{A}_{i, t},\n     \\text{clip}\\Big(\n            \\rho(o_{i,t}), 1 - \\epsilon_{\\text{clip}}, 1 + \\epsilon_{\\text{clip}}\n        \\Big) \\hat{A}_{i, t}\n    \\Big]    \n\\Bigg\\}\n\\tag{9}\\]\n其中，Advantage 变成：\n\\[\n\\hat{A}_{i, t} = r_i - \\mu \\quad \\text{where} \\quad \\mu = \\frac{1}{G}\\sum_{j=1}^{G} r_j\n\\tag{10}\\]\n也就是只用组内均值当 baseline，不做方差归一化。\n另一个改动是去掉了按回答长度归一化，也就是不再除以 \\(|o_i|\\)， 直接对所有 token 的 policy-gradient loss 求和。 不过在实现中，会除以一个MAX_TOKENS（最大token数）来稳定训练，但这个是个常数，不会引入长度偏置。\n我们来看看代码\ndef compute_loss_grpo(\n    log_probs: torch.Tensor,      # (B, G, T) log probs under current policy\n    old_log_probs: torch.Tensor,  # (B, G, T) log probs under old policy (detached)\n    response_mask: torch.Tensor,  # (B, G, T) mask for valid response tokens\n    advantage: torch.Tensor,    # (B, G) advantage per response\n    clip_range: float = 0.2,\n    max_tokens: int = 1024,\n):\n    B, G, T = log_probs.size()\n\n    important_ratio = torch.exp(log_probs - old_log_probs)  # (B, G, T)\n\n    # Broadcast advantage to token level\n    advantage_tok = advantage.unsqueeze(-1).expand_as(important_ratio)  # (B, G, T)\n\n    unclipped = important_ratio * advantage_tok\n    clipped = torch.clamp(important_ratio, 1 - clip_range, 1 + clip_range) * advantage_tok\n\n    pg_loss_tok = -torch.min(unclipped, clipped)  # (B, G, T)\n\n    # Length normalization (GRPO does this)\n    pg_loss_tok = pg_loss_tok * response_mask  # mask out non-response tokens \n    pg_loss_seq = pg_loss_tok.sum(dim=-1) / max_tokens  # (B, G) \n\n    # Final loss: mean over batch and group\n    loss = pg_loss_seq.sum()\n    loss = loss / (B * G)\n    # loss = pg_loss_seq.mean()  # alternative: mean over all tokens\n\n    return loss\n\n\n\n\n\n\nFigure 5: 主要看图中红色部分，Dr.GRPO在错误回答时，不再按长度归一化，这样长回答不会被“惩罚更轻”，从而避免了模型学会写长错误回答的现象。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#deepseek-r1",
    "href": "posts/CS336/Lecture16&17/lec16.html#deepseek-r1",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "4.1 DeepSeek-R1",
    "text": "4.1 DeepSeek-R1\nDeepSeek-R1 在2025年春节期间发布（还记得当时铺天盖地全是DeepSeek的新闻，过年都被卷到），它是第一个用 GRPO 做数学推理强化学习的模型。接下来我们来一下DeepSeek R1 (DeepSeek-AI et al. 2025)是如何训练的。\n\n\nWARNING: 关于DeepSeek-R1 Paper与课堂上的不同\n\n\nDeepSeek R1 在最近（2026年1月）重新Release了它们新的训练细节论文，该笔记基于最新论文内容进行整理。而课程中是基于最初的技术报告进行讲解，二者在细节上可能存在差异。\n\n\n首先，我看一下DeepSeek R1的整体pipeline：\n\n\n\n\n\n\nFigure 6\n\n\n\nDeepSeek R1 并不是一次性通过强化学习训练出来的模型，而是一条多阶段、逐步增强推理能力的训练流水线。\n\n4.1.1 From DeepSeek-V3 to DeepSeek-R1-Zero\n从 DeepSeek V3 Base 出发，研究者首先进行了几乎“纯 RL”的实验（R1-zero）：仅在推理类 prompt 上，用 accuracy 与 format 这类可验证的 outcome-level 奖励进行强化学习，验证一个关键假设——在没有 process supervision 的情况下，RL 是否足以激活模型的推理能力。\n\nSpecifically, we apply the RL technique on the DeepSeek-V3 base to train DeepSeek-R1-Zero. During training, we design a straightforward template, to require DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases to ensure that we can accurately observe the model’s natural progression during the RL process.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.4 \n\n结果表明推理能力确实可以被激发，同时发现了 “aha moment” 现象：模型在训练过程中会突然开始生成更长、更复杂的 CoT，伴随推理正确率的显著提升。\n\nNotably, during training, DeepSeek-R1-Zero exhibits an “aha moment” characterized by a sudden increase in the use of the word “wait” during reflections  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.5 \n\nDeeoSeek R1-zero 的成功验证了 RLVR 思路的可行性：即使没有 process-level supervision，仅凭 outcome-level 的可验证奖励，RL 也能激发 LLM 的推理能力。\n\nThe self-evolution of DeepSeek-R1-Zero underscores the power and beauty of RL: rather than explicitly teaching the model how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.5 \n\n不过课程中提到，CoT 变长、“aha/backtracking”等现象；可能是来自目标/实现细节的偏置，不一定是“学会更深思考”的必然， “aha”这类文本也可能在底座模型上偶发出现，不必过度神化“涌现”。\n\n\n\n\n\n\nFigure 7: shows two examples to demonstrate that the DeepSeek-V3-Base model already exhibits the so-called “aha moment” even before the RL-tuning (Image Source (Liu et al. 2025))\n\n\n\n在这个阶段，DeepSeek R1-zero 的训练采用了 GRPO Algorithm 1 算法，并设计了专门的奖励函数：\n\nOutcome-level 奖励：主要包括 accuracy 奖励（答案正确与否）和 format 奖励（输出格式是否符合要求）\n\n\n\n4.1.2 From DeepSeek-R1-Zero to DeepSeek-R1\nDeepSeek-R1-zero 很好，但是它存在几个问题：\n\nAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities, it faces several issues. DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing, as DeepSeek-V3-Base is trained on multiple languages, especially English and Chinese.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.6 \n\n因此，在R1-zero的基础上，DeepSeek团队设计了 DeepSeek-R1 的训练方案，来提升模型的表达质量与行为 稳定性。具体来说：\n\nSFT 冷启动：利用 R1-zero 采样得到的大量推理轨迹，经过过滤与人工/模型精修后，对 V3 Base 进行 Cold-start Long CoT 的 SFT，得到 R1 Dev 系列模型；\n多轮 GRPO 式 RL：随后再通过多轮 GRPO 式 RL，引入语言一致性奖励、偏好奖励以及更丰富的任务分布，逐步把模型从“只会解题”推向“推理强、表达稳定、行为可控”的产品级模型。\n\n\nIn the initial stage, we collect thousands of cold-start data that exhibits a conversational, human-aligned thinking process. RL training is then applied to improve the model performance with the conversational thinking process and language consistency. Subsequently, we apply rejection sampling and SFT once more. This stage incorporates both reasoning and nonreasoning datasets into the SFT process, enabling the model to not only excel in reasoning tasks but also demonstrate advanced writing capabilities. To further align the model with human preferences, we implement a secondary RL stage designed to enhance the model’s helpfulness and harmlessness while simultaneously refining its reasoning capabilities.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.6 \n\n同时，DeepSeek R1 也在奖励设计上做了改进：\n\n多维奖励设计：不仅包含可验证的 outcome-level 奖励（accuracy、format），还引入了Language consistency reward、Preference / non-verifiable rewards（通过对比学习训练的偏好模型）等，更全面地衡量模型输出质量；\n奖励混合与调优：通过对不同奖励进行加权混合与调优，确保模型在提升推理能力的同时，也能兼顾表达质量与行为稳定性。\n持续的奖励模型训练：在 RL 训练过程中，持续更新奖励模型，以适应模型能力的提升与任务分布的变化。\n\n\n\n4.1.3 DeepSeek Distillation\n在 DeepSeek R1 训练完成后，研究者还进行了蒸馏（distillation），以提升模型的推理效率与实用性.\n\nTo enable broader access to powerful AI at a lower energy cost, we have distilled several smaller models and made them publicly available.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.2 \n\n具体的方法就是：用 R1 作为 teacher 模型，生成大量高质量的推理样本，然后对更小的 student 模型进行 SFT 蒸馏训练。\n\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance.  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, p.60",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#kimi-1.5",
    "href": "posts/CS336/Lecture16&17/lec16.html#kimi-1.5",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "4.2 Kimi 1.5",
    "text": "4.2 Kimi 1.5\n和 DeepSeek R1 更像“配方驱动（SFT→RL→再对齐）”不同，课程把 Kimi 1.5 的经验总结成一句话：data is king。Kimi 的核心不是提出全新 RL 算法，而是把“可验证奖励推理 RL”变成一条更工程化的数据管线：先对任务做跨领域分类与分布平衡，避免模型只在单一类型题上变强；同时主动剔除容易通过随机猜测获得奖励的题型（例如选择题、判断题），优先保留短答案、可被规则/判定器高精度验证的任务，以减少 reward hacking 的空间。\n最关键的一步是 best-of-n 难度筛选：用当前还不够强的 SFT 模型对每道题采样多次（课上举例类似 best-of-8），然后只保留“采样多次仍然失败”的题（fail best-of-n）。直觉上，这相当于把训练数据限制在“当前模型确实不会，但又可能学会”的可学区间，显式做出一种 curriculum 的雏形：太容易的题学不到东西，太难的题只会产生大量 0 reward；而经过筛选的题更可能提供稳定的学习信号。课程的 takeaway 是：在 RLVR 场景里，数据难度控制往往比算法 tweak 更决定训练效率和最终行为。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#qwen3",
    "href": "posts/CS336/Lecture16&17/lec16.html#qwen3",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "4.3 Qwen3",
    "text": "4.3 Qwen3\n课程在讲 Qwen3 时，强调它的亮点不只是“也用了 RLVR/GRPO”，而是把推理模型在真实使用场景里的一个核心矛盾摆到台面上：推理越强，往往越贵（更长的 CoT、更高的 test-time compute）。因此 Qwen3 的思路更像是把“推理能力”和“推理成本”一起纳入训练目标：一方面沿用 “SFT（长 CoT 冷启动）→ 可验证奖励 RL（提升推理正确性）→ 再做通用对齐” 的主线；另一方面通过训练与数据/奖励设计，让模型学会在不同问题上自适应地选择推理强度——该认真推时能推得深，不需要推理时也能走更短、更便宜的路径。\n从课程视角看，Qwen3 体现了一种很实用的产品化取向：推理模型最终要在“正确率”和“成本/延迟”之间做权衡，单纯追求 CoT 变长并不等价于更好。与 DeepSeek R1 更强调“用 RL 激活推理”以及 Kimi 更强调“用数据筛选做 curriculum”相比，Qwen3 更像是在探索：如何把“计算预算可控”变成模型行为的一部分，从而避免 RL 训练把 CoT 拉爆、成本失控的副作用。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#data-goal",
    "href": "posts/CS336/Lecture16&17/lec16.html#data-goal",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.1 Data & Goal",
    "text": "5.1 Data & Goal\n任务：排序（Sorting）\n\nPrompt：长度为 LLL 的整数序列，例如 [3, 1, 2, 0]\nResponse：模型输出同样长度 LLL 的序列，希望是排序后的结果 [0, 1, 2, 3]\ngroup 结构（GRPO 的关键）：对同一个 prompt，采样 \\(k\\) 条 response \\(\\{a^{(1)}, a^{(2)}, \\dots, a^{(K)}\\}\\) 形成一组，用于计算组内 baseline（均值/标准差）。\n\ndef gen_prompts(batch_size: int, seq_len: int, vocab_size: int, device) -&gt; torch.Tensor:\n    return torch.randint(low=0, high=vocab_size, size=(batch_size, seq_len), device=device)\n\n\ndef sorted_ground_truth(prompt: torch.Tensor) -&gt; torch.Tensor:\n    return torch.sort(prompt, dim=-1).values",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#reward",
    "href": "posts/CS336/Lecture16&17/lec16.html#reward",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.2 Reward",
    "text": "5.2 Reward\n首先我们需要定义 reward 函数。Lecture 里强调：如果 reward 只给 0/1（完全正确/错误），会导致稀疏奖励，训练很容易卡住。因此这里使用 partial credit来让学习更平滑。\n\n5.2.1 Reward v2：Inclusion + Adjacent Sorted Pairs（课堂采用的更细奖励）\n给定 prompt \\(x\\) 和 response \\(y\\)：\n\nInclusion reward：prompt 里的 token 在 response 里出现就给分（按计数，多重集）\n\n\\[\n\\text{Inclusion reward:} \\quad R_{\\text{inc}}(x,y)=\\sum_{t} \\min(\\text{count}_x(t), \\text{count}_y(t))\n\\tag{11}\\]\n\nAdjacent sorted pairs：统计 response 中相邻对满足非降序的个数\n\n\\[\n\\text{Adjacent sorted pairs:} \\quad R_{\\text{adj}}(y)=\\sum_{i=1}^{L-1}\\mathbb{I}[y_i \\le y_{i+1}]\n\\tag{12}\\]\n总 reward：\n\\[\n\\text{Total reward:} \\quad R(x,y)=R_{\\text{inc}}(x,y)+R_{\\text{adj}}(y)\n\\tag{13}\\]\n\n课堂也提到：这种 reward 可能存在“漏洞”（reward hack），因此 reward 设计本身就是 RL 的难点之一。\n\ndef sort_distance_reward(prompt: list[int], response: list[int]) -&gt; float:\n    assert len(prompt) == len(response)\n    ground_truth = sorted(prompt)\n    return sum(1 for x, y in zip(response, ground_truth) if x == y)\n\n\ndef sort_inclusion_ordering_reward(prompt: list[int], response: list[int]) -&gt; float:\n    \"\"\"\n    Return how close response is to ground_truth = sorted(prompt).\n    \"\"\"\n    assert len(prompt) == len(response)\n    # Give one point for each token in the prompt that shows up in the response\n    inclusion_reward = sum(1 for x in prompt if x in response)  # @inspect inclusion_reward\n    # Give one point for each adjacent pair in response that's sorted\n    ordering_reward = sum(1 for x, y in zip(response, response[1:]) if x &lt;= y)  # @inspect ordering_reward\n    return inclusion_reward + ordering_reward",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#model",
    "href": "posts/CS336/Lecture16&17/lec16.html#model",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.3 Model",
    "text": "5.3 Model\n对于这个任务，我们就定一个简单的模型：\nclass ToySortPolicy(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int, prompt_length: int, response_length: int):\n        super().__init__()\n\n        self.embedding_dim = embedding_dim\n        self.emb = nn.Embedding(vocab_size, embedding_dim)\n        self.enc = nn.Parameter(\n            torch.randn(prompt_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim)\n        )\n        self.dec = nn.Parameter(\n            torch.randn(response_length, embedding_dim, embedding_dim) / math.sqrt(embedding_dim)\n        )\n        self.out = nn.Linear(embedding_dim, vocab_size)\n\n    def forward(self, prompt):\n        x = self.emb(prompt)  # (B,L,d1)\n        h = torch.einsum(\"bld,ldm-&gt;bm\", x, self.enc)  # (B,d2)\n        z = torch.einsum(\"bm,lmd-&gt;bld\", h, self.dec)  # (B,L,d1)\n        return self.out(z)  # (B,L,V)",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#algorithm",
    "href": "posts/CS336/Lecture16&17/lec16.html#algorithm",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.4 Algorithm",
    "text": "5.4 Algorithm\ndef grpo(cfg: GRPOCfg = GRPOCfg()):\n    dev = get_device()\n    set_seed(cfg.seed)\n\n    policy = ToySortPolicy(cfg.V, cfg.L, cfg.L, cfg.L).to(dev)\n    opt = torch.optim.Adam(policy.parameters(), lr=cfg.lr)\n\n    reward_fn = REWARD_FN_MAP[cfg.reward_fn]\n\n    # reference model (slow-moving anchor for KL)\n    ref = ToySortPolicy(cfg.V, cfg.L, cfg.L, cfg.L).to(dev)\n    ref.load_state_dict(policy.state_dict())\n    ref.eval()\n\n    for it in range(cfg.outer_iters):\n        # update reference model slowly\n        if cfg.use_kl and it &gt; 0 and (it % cfg.ref_update_every == 0):\n            ref.load_state_dict(policy.state_dict())\n            ref.eval()\n\n        # ---------- OUTER: rollout + rewards + deltas + freeze old/ref logps\n        prompt = gen_prompts(cfg.B, cfg.L, cfg.V, dev)  # (B,L)\n\n        with torch.no_grad():\n            responses = sample_responses(policy, prompt, cfg.K, cfg.temperature)  # (B,K,L)\n            rewards = compute_reward(prompt, responses, reward_fn)  # (B,K)\n            deltas = compute_deltas(rewards, cfg.delta_mode)  # (B,K)\n\n            # IMPORTANT: freeze old logps (detach)\n            old_logp_token = compute_log_probs(prompt, responses, policy).detach()  # (B,K,L)\n\n            # freeze ref logps for KL (detach)\n            ref_logp_token = compute_log_probs(prompt, responses, ref).detach() if cfg.use_kl else None\n\n        # ---------- INNER: multiple gradient steps on same samples\n        for _ in range(cfg.inner_steps):\n            opt.zero_grad(set_to_none=True)\n\n            logp_token = compute_log_probs(prompt, responses, policy)  # (B,K,L)\n\n            loss = compute_loss(\n                log_probs=logp_token,\n                old_log_probs=old_logp_token,\n                deltas=deltas,\n                mode=cfg.loss_mode,\n            )\n\n            if cfg.use_kl:\n                loss = loss + cfg.kl_beta * compute_kl_penalty(logp_token, ref_logp_token)\n\n            loss.backward()\n            opt.step()\n\n        if (it % cfg.print_every) == 0 or it == cfg.outer_iters - 1:\n            with torch.no_grad():\n                print(\n                    f\"iter {it:04d} | \"\n                    f\"reward mean {rewards.mean().item():.3f} \"\n                    f\"(min {rewards.min().item():.1f}, max {rewards.max().item():.1f}) | \"\n                    f\"loss {loss.item():.4f}\"\n                )\n\n    return policy",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#training",
    "href": "posts/CS336/Lecture16&17/lec16.html#training",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.5 Training",
    "text": "5.5 Training\n@dataclass\nclass GRPOCfg:\n    V: int = 10  # vocab size (token values 0..V-1)\n    L: int = 4  # sequence length\n    B: int = 128  # batch prompts per outer iter\n    K: int = 8  # responses per prompt (group size)\n    outer_iters: int = 200\n    inner_steps: int = 4\n\n    lr: float = 2e-2\n    temperature: float = 1.0\n\n    delta_mode: Literal[\"raw\", \"centered_rewards\", \"normalized_rewards\", \"max_rewards\"] = \"centered_rewards\"\n    loss_mode: Literal[\"naive\", \"unclipped\", \"clipped\"] = \"clipped\"\n    clip_eps: float = 0.2\n\n    use_kl: bool = True\n    kl_beta: float = 0.02\n    ref_update_every: int = 30\n\n    seed: int = 0\n    print_every: int = 20\n\n    reward_fn: Literal[\"distance\", \"inclusion_ordering\"] = \"inclusion_ordering\"",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture16&17/lec16.html#results",
    "href": "posts/CS336/Lecture16&17/lec16.html#results",
    "title": "Lecture 16 & 17: LLM Alignment SFT & RLVR（GRPO）",
    "section": "5.6 Results",
    "text": "5.6 Results\niter 0000 | reward mean 3.070 (min 0.0, max 7.0) | loss 0.0138\niter 0020 | reward mean 3.250 (min 0.0, max 6.0) | loss 0.0034\niter 0040 | reward mean 3.396 (min 0.0, max 7.0) | loss 0.0008\niter 0060 | reward mean 3.625 (min 0.0, max 7.0) | loss 0.0016\niter 0080 | reward mean 3.813 (min 1.0, max 7.0) | loss 0.0059\niter 0100 | reward mean 4.071 (min 1.0, max 7.0) | loss 0.0043\niter 0120 | reward mean 4.278 (min 1.0, max 7.0) | loss 0.0030\niter 0140 | reward mean 4.497 (min 1.0, max 7.0) | loss 0.0058\niter 0160 | reward mean 4.710 (min 1.0, max 7.0) | loss 0.0058\niter 0180 | reward mean 4.859 (min 2.0, max 7.0) | loss 0.0033\niter 0199 | reward mean 4.964 (min 2.0, max 7.0) | loss 0.0067\n\n\n=== sample check ===\nprompt: [4, 2, 4, 4] | pred: [0, 2, 4, 6] | gt: [2, 4, 4, 4] | sort reward: 1 | inclusion+ordering reward: 7\nprompt: [6, 8, 1, 9] | pred: [8, 9, 1, 1] | gt: [1, 6, 8, 9] | sort reward: 0 | inclusion+ordering reward: 5\nprompt: [4, 3, 8, 7] | pred: [0, 3, 6, 7] | gt: [3, 4, 7, 8] | sort reward: 0 | inclusion+ordering reward: 5\nprompt: [6, 0, 5, 6] | pred: [0, 5, 7, 9] | gt: [0, 5, 6, 6] | sort reward: 2 | inclusion+ordering reward: 5\nprompt: [3, 1, 4, 6] | pred: [1, 1, 4, 7] | gt: [1, 3, 4, 6] | sort reward: 2 | inclusion+ordering reward: 5\nprompt: [2, 7, 8, 0] | pred: [3, 7, 7, 8] | gt: [0, 2, 7, 8] | sort reward: 2 | inclusion+ordering reward: 5\nprompt: [7, 9, 1, 2] | pred: [9, 9, 1, 9] | gt: [1, 2, 7, 9] | sort reward: 1 | inclusion+ordering reward: 4\nprompt: [3, 6, 0, 2] | pred: [1, 1, 3, 7] | gt: [0, 2, 3, 6] | sort reward: 1 | inclusion+ordering reward: 4\n全部的代码可以在这个Notebook中看到",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 16 & 17: LLM Alignment SFT & RLVR(GRPO)"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html",
    "href": "posts/100-AI-Papers/08-vae/VAE.html",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#experiment",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#experiment",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "1.1 Experiment",
    "text": "1.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#latent-variable-models",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#latent-variable-models",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "6.1 Latent Variable Models",
    "text": "6.1 Latent Variable Models\n在深度学习中，我们经常处理 高维、复杂、带噪声的真实世界数据：图像、语音、文本…… 这些数据的背后，其实常常有一些 未被直接观测到的隐藏结构 —— 这就是 “Latent Variables”（潜变量）。\n想象一下，你看一张人脸照片。照片是观测变量（observed variable）。 但是让这张脸“看起来像某个人”的，是一些不能直接看到的因素： • 光照（lighting） • 情绪（expression） • 姿态（pose） • 脸部特征（identity） • 背景（background）\n这些因素虽然没有在数据中显式标注，却真实存在，并决定了我们看到的图像。 潜变量模型的目标，就是用数学方式把这些“隐藏因素”建模出来。\n\nLatent Variable Model（LVM）是一类 假设观测数据是由一些隐藏变量生成的概率模型。 比如我们看到的数据\\(\\mathrm{x} \\in \\mathbb{R}^{d}\\), 那它有相对应的隐藏变量 \\(z \\in \\mathbb{R}^{k}\\) 其中 \\(k \\ll d\\) . \\(z\\) 是我们观察不到的，也就是所谓的 Latent Variable.\n在使用Latent Variable Model时，我们有两个关键的任务： 1. Inference: 根据 \\(\\mathrm{x}\\) 我们来推断出 \\(\\mathrm{z}\\) 是什么，比如，给定一张人脸的照片，这个模型需要找出 - 这张脸是男生还是女生 - 表情是开心还是悲伤 - 是什么样的姿势 在数学上，就是求后验分布 \\(p(\\mathrm{z} | \\mathrm{x})\\) 2. Generation: 通过 \\(\\mathrm{z}\\)， 我们来生成一个 \\(\\mathrm{x}\\)。 这个就是生成模型。\nLatent Variable Model 是现代生成式模型的基石，基本上所有的生成式模型，比如 GAN， DDPM， Flow Model 等，都是以Latent Variable Model为基础，在此条件下，通过不同求 Latent Variable 的方法，来解决这种问题。"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#autoencoder",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#autoencoder",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "6.2 AutoEncoder",
    "text": "6.2 AutoEncoder\n\nAutoEncoder 是一种Self-Supervised Learning 的方法。 它可以让神经网络学会压缩，并且在还原数据，通过这种方法，我们可以学习到的低纬度的Latent Variable \\(\\mathrm{z}\\)。 AutoEncoder 也可以看作是Latent Variable Model的一种学习方法。 AutoEncoder 也通常用在Representation Learning 表征学习。"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#kl-divergence",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#kl-divergence",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "6.3 KL-Divergence",
    "text": "6.3 KL-Divergence\nKL 散度（Kullback–Leibler Divergence）是用来衡量两个概率分布之间差异的一种度量方法。定义如下: \\[\nD_{KL}(Q \\| P) = \\mathbb{E}_{x \\sim Q}\\left[ \\log \\frac{Q(x)}{P(x)} \\right]\n\\]\n直观的解释KL-Divergence就是: - 如果数据 \\(x\\) 是从 \\(Q\\) 分布中来的，但如果我们用 \\(P\\) 分布 来解释这些数据，会损失多少信息量\n对于高斯分布（Gaussian Distribution），我们KL- Divergence有以下的形式：\n$$\nD_{KL}(Q | P) = $$\n如果是Diagonal Gaussian， 那么KL-Divergence 可以简化为： $$ D_{KL}(q ,|, p)\n_{i=1}^{d} $$\n\\[\nD_{KL}(q(\\mathbf{z}) \\ \\| \\  \\mathcal{N}(0, I)) =\n\\frac{1}{2}\n\\sum_{i=1}^{d}\n\\left(\n\\mu_{q,i}^2 + \\sigma_{q,i}^2 - \\log \\sigma_{q,i}^2 - 1\n\\right)\n\\]\n需要注意的一个点是，KL-Divergence是不对称的， \\[\nD_{KL}(Q \\| P) \\neq D_{KL}(P \\| Q)\n\\]\n为什么我们要强调这一点，是因为： 对于不同位置的Q，P我们所求的是不一样的，简单来说，就是用 \\(\\|\\) 后面的分布，来approximate \\(\\|\\) 前面的分布，具体如下图。"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#variational-inference",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#variational-inference",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "6.4 Variational Inference",
    "text": "6.4 Variational Inference\nVariational Inference（VI，变分推断）是一种用可优化、易计算的分布来近似一个难以求解的后验分布，通过最小化两者之间的 KL 距离，从而实现高效概率推断的方法。 在 Latent Variable 模型中，我们想求： \\[\np (\\mathrm{z} | \\mathrm{x}) = \\frac{p(\\mathrm{x}, \\mathrm{z})}{p(\\mathrm{x})}\n\\]\n通常 \\(p(\\mathrm{x})\\) 是不可求的，因为: \\[\np(\\mathrm{x}) = \\int p(\\mathrm{x}, \\mathrm{z}) d\\mathrm{z}\n\\] 通常是不可能直接求的。因此我们采取一种 “曲线救国” 的方式： 我们不去计算真正的后验，而是找一个 可计算并且可优化的分布来近似它： \\[\nq(z | x) \\approx p(z | x)\n\\] 实现起来也是很简单的 1. 选一个可计算的分布族 （Variational Family） 2. 让它尽可能的接近真实的后验： \\[\n\\underset{\\phi}{\\min}  D_{KL}(q_{\\phi}(z | x ) \\| p(z | x))\n\\]\n直观的来说，Variational Inference 就是：不过不断拉升，旋转，一个椭圆型 \\(q\\), 来使它尽可能的可以和云朵形状的 \\(p\\) 来重叠"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#re-parameterization-trick",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#re-parameterization-trick",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "7.1 Re-parameterization Trick",
    "text": "7.1 Re-parameterization Trick\n 如上图可见，\\(z\\) 是由 \\(x, \\phi\\) 决定的。通过这种方式，那么我们改取怎么样的 \\(g_{\\phi}(x, \\epsilon)\\) 呢。文章中给出了3个基本的方法： 1. 方法1 2. 2 3. c\n\n\n\n\n\n\nREINFORCE\n\n\n\n对于 Re-Parametrization Trick是针对 \\(z\\) 是 Continuous的情况，如果 \\(z\\) 是离散（Discrete) 的 那么我们则可以使用 REINFOCE 的方法。"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#amortized-inference",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#amortized-inference",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "7.2 Amortized Inference",
    "text": "7.2 Amortized Inference\n还有一个比较容易被忽略的一点就是，VAE 还运用了Amortized Inference。什么意思呢？\n传统的变分推断中，我们需要为每一个观测样本 x 单独优化一个变分分布 q(z|x) 的参数，这意味着每来一个新样本都要重新做一遍变分优化，成本非常高。而在 VAE 中，我们不再为每个样本单独优化后验，而是训练一个 共享的推断网络（Encoder） 来预测 q_(z|x) 的参数。也就是说，模型通过学习一个函数 f_(x) 来一次性“摊销”所有样本的推断成本，使得对任意新样本 x，只需一次前向传播就能得到近似后验，不再需要昂贵的 per-sample 优化。这种方式极大地提升了推断效率，也让变分推断能够在深度学习规模上落地。\n也就是用神经网络来一次性学习后验\n 将上面的几个结合起来，我们就得到的了 Auto-Encoding VB Algorithm。\n## 和 AutoEncoder的关系"
  },
  {
    "objectID": "posts/100-AI-Papers/08-vae/VAE.html#experiements",
    "href": "posts/100-AI-Papers/08-vae/VAE.html#experiements",
    "title": "08: Auto-Encoding Variational Bayes (VAE)",
    "section": "7.3 Experiements",
    "text": "7.3 Experiements\nVAE Loss can be defined as this one:\nclass VAELoss(nn.Module):\n    def __init__(self, rec_loss=\"bce\", kl_beta=1.0):\n        super().__init__()\n\n        self.rec_loss = rec_loss.lower()\n        self.kl_beta = kl_beta\n\n        self.eval()\n\n    def forward(self, x, x_recon, mu, logvar):\n        B = x.shape[0]\n        if self.rec_loss == \"bce\":\n            rec = F.binary_cross_entropy(x_recon, x, reduction=\"sum\")\n        elif self.rec_loss == \"mse\":\n            rec = F.mse_loss(x_recon, x, reduction=\"sum\")\n\n        # KL divergence: D_KL(q(z|x) || p(z))\n        kl = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - logvar - 1)\n\n        total = (rec + self.kl_beta * kl) / B\n\n        return total, {\n            \"recon\": rec.detach().cpu() / B,\n            \"kl\": kl.detach().cpu() / B,\n        }\nThe Loss curve and illustration examples:"
  },
  {
    "objectID": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html",
    "href": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html",
    "title": "11: Neural Discrete Representation Learning (VQ_VAE)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#experiment",
    "href": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#experiment",
    "title": "11: Neural Discrete Representation Learning (VQ_VAE)",
    "section": "1.1 Experiment",
    "text": "1.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#experiment-1",
    "href": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#experiment-1",
    "title": "11: Neural Discrete Representation Learning (VQ_VAE)",
    "section": "7.1 Experiment",
    "text": "7.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#vector-quantization",
    "href": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#vector-quantization",
    "title": "11: Neural Discrete Representation Learning (VQ_VAE)",
    "section": "12.1 Vector Quantization",
    "text": "12.1 Vector Quantization\n向量量化（Vector Quantization）是一种把连续的向量转换为离散的“索引” 的方法。通过这个索引，在字典（codebook）中找到一个与其最相近的一个向量，这个字典也叫做： - CodeBook - Embedding Table - Centroids 这时，每个向量就变成了一个离散的编号（Index） 用数学表达就是： 我们有: - A vector \\(z \\in \\mathbb{R}^{d}\\) - A codebook \\(E \\in \\mathbb{R}^{K \\times d}\\), 其中 有 \\(K\\) 个索引\n我们通过比较 \\(z\\) 和 \\(K\\) 个向量中，找到最近的一个向量\n\\[\n\\text{quantized}(z) = e_{k} \\quad \\text{where}\\ k = \\underset{j}{\\operatorname{arg\\min}}   \\|z - e_{j} \\|^{2}\n\\]\nimport torch\n\ndef quantize(embedding_table: torch.Tensor, z: torch.Tensor):\n    \"\"\"\n    embedding_table: (K, D)\n    z: (B, D)\n    \"\"\"\n    # (B, 1, D) - (1, K, D) → (B, K, D)\n    diff = z.unsqueeze(1) - embedding_table.unsqueeze(0)\n\n    # (B, K, D) -&gt; (B, K)\n    distances = torch.linalg.norm(diff, dim=2)\n\n    #  (B, K) -&gt; (B,)\n    indices = distances.argmin(dim=1)\n\n    # Gather quantized embeddings → (B, D)\n    q = embedding_table[indices]\n\n    return q, indices\n\n\nK, D = 512, 64\nB = 8\n\ncodebook = torch.randn(K, D)\nz = torch.randn(B, D)\n\nq, idx = quantize(codebook, z)\nassert q.shape == (B, D)\nassert idx.shape == (B,)"
  },
  {
    "objectID": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#straight-through-estimator",
    "href": "posts/100-AI-Papers/11-vq-vae/VQ_VAE.html#straight-through-estimator",
    "title": "11: Neural Discrete Representation Learning (VQ_VAE)",
    "section": "12.2 Straight Through Estimator",
    "text": "12.2 Straight Through Estimator\nK, D = 512, 64\nB = 8\n\nembedding_table = torch.randn(K, D)\nz_enc = torch.randn(B, D, requires_grad=True)\nassert z_enc.grad is None\n\nz_k, _ = quantize(embedding_table, z_enc)\n\n# STE\nz_k = z_enc + (z_k - z_enc).detach()\nz_k.retain_grad()\n\nloss = (z_k**2).mean()\nloss.backward()\n\nassert z_k.grad is not None\nassert z_enc.grad is not None\nassert torch.allclose(z_enc.grad, z_k.grad)"
  },
  {
    "objectID": "posts/100-AI-Papers/09-mae/MAE.html",
    "href": "posts/100-AI-Papers/09-mae/MAE.html",
    "title": "09: Masked Autoencoders Are Scalable Vision Learners(MAE)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/09-mae/MAE.html#experiment",
    "href": "posts/100-AI-Papers/09-mae/MAE.html#experiment",
    "title": "09: Masked Autoencoders Are Scalable Vision Learners(MAE)",
    "section": "1.1 Experiment",
    "text": "1.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/07-dino/DINO.html",
    "href": "posts/100-AI-Papers/07-dino/DINO.html",
    "title": "07: Emerging Properties in Self-Supervised Vision Transformers (DINO)",
    "section": "",
    "text": "1 Preliminary\n\n\n2 DINO\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#online-softmax",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#online-softmax",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "1.1 Online Softmax",
    "text": "1.1 Online Softmax\nOnline Softmax（也叫 streaming softmax / one-pass softmax statistics）指的是：你不需要先把整行 logits \\(s_1,\\dots,s_D\\) 全部存下来再做 softmax，而是一边读入（或一边计算）logits，一边更新必要的统计量，最后得到 softmax 的归一化因子；在需要输出概率时再用这些统计量把每个 logit 变成概率。\n我们知道，对于一行向量 \\(s \\in \\mathbb{R}^{D}\\), 对于数值稳定的Softmax，我们要先减去它的 max value，这就导致了，对于计算softmax值，我们需要遍历3次这个数组。而Online Softmax则在保持原来的遍历两步的基础上，同时保持Max Softmax的特性。\n要实现这个概念的核心做法就：在遍历过程中，维护两个变量： - m：当前遍历过的logits的最大值 - l：当前遍历过的logits的归一化因子（即 \\(\\sum_{j=1}^{i} \\exp(s_j - m)\\)）\n具体的更新公式如下：\n\\[\n\\begin{split}\nm_{new} & = \\max(m, s_i) \\\\\nl & = l \\cdot \\exp(m - m_{new}) + \\exp(s_i - m_{new}) \\\\\nm & = m_{new}\n\\end{split}\n\\]\n下面是Python的实现代码：\ndef online_softmax(x):\n    m, l = float(\"-inf\"), 0.0\n\n    for i in range(len(x)):\n        m_new = max(m, x[i])\n        l = l * math.exp(m - m_new) + math.exp(x[i] - m_new)\n        m = m_new\n\n    softmax_values = [0.0] * len(x)\n    for i in range(len(x)):\n        softmax_values[i] = math.exp(x[i] - m) / l\n    return softmax_values\n\n至于为什么这个方法是正确的，我们可以通过数学归纳法来证明： - Base Case: 当只遍历了第一个元素 \\(s_1\\) 时，显然 \\(m = s_1\\)，\\(l = \\exp(s_1 - s_1) = 1\\)，此时 softmax 计算正确。 - Inductive Step: 假设在遍历到第 \\(i-1\\) 个元素时，\\(m\\) 和 \\(l\\) 已经正确地反映了前 \\(i-1\\) 个元素的最大值和归一化因子。现在考虑第 \\(i\\) 个元素 \\(s_i\\)： - 如果 \\(s_i &gt; m\\)，则新的最大值 \\(m_{new} = s_i\\)，归一化因子更新为： \\[\n      l_{new} = l \\cdot \\exp(m - s_i) + \\exp(s_i - s_i) = l \\cdot \\exp(m - s_i) + 1\n      \\] - 如果 \\(s_i \\leq m\\)，则最大值保持不变 \\(m_{new} = m\\)，归一化因子更新为： \\[\n     l_{new} = l \\cdot \\exp(m - m) + \\exp(s_i - m) = l + \\exp(s_i - m)\n     \\] 我们知道 \\(\\ell\\) 到遇到第 \\(i\\) 个元素时，在加上第 \\(i\\) 个元素的贡献之前，要根据当前的最大值进行调整，重新缩放之前的和，以确保数值稳定性。\n\n在这两种情况下，更新后的 \\(m_{new}\\) 和 \\(l_{new}\\) 仍然正确地反映了前 \\(i\\) 个元素的最大值和归一化因子。因此，通过数学归纳法，我们证明了该在线算法在遍历完整个数组后，能够正确计算出 softmax 的归一化因子。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#recomputing",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#recomputing",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "1.2 Recomputing",
    "text": "1.2 Recomputing\nRecomputing 是一种以计算换内存的技术，它的核心思想是：在前向传播时，不保存某些中间结果，而是在反向传播时重新计算这些结果，从而节省内存空间。\n我们知道，在反向传播时，需要用到前向传播中的一些中间结果来计算梯度。如果我们在前向传播时保存了所有的中间结果，那么会占用大量的内存空间。通过 Recomputing，我们可以选择性地不保存某些中间结果，而是在反向传播时重新计算它们。举个例子：假如我们有一个MLP层\\(y=W_2\\cdot \\sigma(W_1 \\cdot x)\\)，常规的做法是，在前向传播时，保存 \\(h=W_1 \\cdot x\\) 和 \\(a=\\sigma(W_1 \\cdot x)\\) 的结果，以便在反向传播时计算梯度：\\(\\frac{\\partial L}{\\partial W_2} = d y \\, a^\\top\\) 和 \\(\\frac{\\partial L}{\\partial W_1} = (W_2^\\top d y) \\odot \\sigma'(a) \\, x^\\top\\)。但是，如果我们使用 Recomputing，我们可以选择不保存 \\(a = W_1 \\cdot x\\) 和 \\(\\sigma(a)\\)，而是在反向传播时重新计算它们。这样，我们就节省了内存空间，但需要额外的计算时间来重新计算这些中间结果。\n这种技术的好处就是，减少了内存的使用，同时降低了读写内存的带宽需求，从而提升了整体的计算效率。\n通过PyTorch 的 torch.autograd.Function，我们可以很方便地实现 Recomputing。下面是一个简单的例子："
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#gpus-memory-model",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#gpus-memory-model",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "1.3 GPU’s Memory Model",
    "text": "1.3 GPU’s Memory Model\n在理解 Flash Attention 之前，我们需要先了解一下 GPU 的内存模型。在这里，主要介绍一下 GPU 的几种主要内存类型：\n\nHigh Bandwidth Memory (HBM): 这是 GPU 上的主要内存类型，具有高带宽和较低的延迟。HBM 通常用于存储大规模的数据，如模型参数和输入数据。\nSRAM: 这是 GPU 上的片上内存，具有非常高的带宽和低延迟。SRAM 通常用于存储临时数据，如中间计算结果。(SRAM 还可以细分成 L1 Cache 和 L2 Cache， Register， Shared Memory 等在这里我们统称为 SRAM)\n\n因此我们希望，尽可能多的计算在 SRAM 上完成，减少对 HBM 的读写，从而提升整体的计算效率。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#tiling",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#tiling",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "1.4 Tiling",
    "text": "1.4 Tiling"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#matrix-calculus-cheatsheet",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#matrix-calculus-cheatsheet",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "1.5 Matrix Calculus Cheatsheet",
    "text": "1.5 Matrix Calculus Cheatsheet\n在这里介绍一些常用的矩阵微积分公式，以便我们在后续的推导中使用：\n\n\n\n\n\n\n\\(O=PV, \\text{where } O \\in \\mathbb{R}^{m \\times n}, P \\in \\mathbb{R}^{m \\times k},  V \\in \\mathbb{R}^{k \\times n}\\)\n\n\n\\[\n\\frac{\\partial L}{\\partial P} = \\frac{\\partial L}{\\partial O} V^\\top, \\quad \\frac{\\partial L}{\\partial V} = P^\\top \\frac{\\partial L}{\\partial O}\n\\]\n\n\n\n\\(P = \\text{softmax}(S), \\text{where } P \\in \\mathbb{R}^{m \\times n}, S \\in \\mathbb{R}^{m \\times n}\\)\n\\[\n\\frac{\\partial L}{\\partial S} = P \\odot \\left( \\frac{\\partial L}{\\partial P} - \\text{row\\_sum}\\left(P \\odot \\frac{\\partial L}{\\partial P}\\right) \\right)\n\\]\n其中：\\(\\odot\\) 表示逐元素乘法，\\(\\text{row\\_sum}(\\cdot)\\) 表示对每一行求和并广播。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#experiment",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#experiment",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "2.1 Experiment",
    "text": "2.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#tiling-qk-v",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#tiling-qk-v",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "6.1 Tiling Q，K， V",
    "text": "6.1 Tiling Q，K， V\nFlash Attention 首先将 Q，K，V 按块（tiling）加载到片上内存（SRAM/Shared Memory）中进行计算。假设我们有一个注意力矩阵 \\(A = \\text{softmax}(QK^\\top)V\\)，其中 \\(Q \\in \\mathbb{R}^{N \\times D}\\)，\\(K \\in \\mathbb{R}^{M \\times D}\\)，\\(V \\in \\mathbb{R}^{M \\times D}\\)。我们可以将 \\(Q\\) 分成 \\(N_b\\) 个块，每个块大小为 \\(B_n \\times D\\)，将 \\(K\\) 和 \\(V\\) 分成 \\(M_b\\) 个块，每个块大小为 \\(B_m \\times D\\)。这样，我们可以将注意力矩阵的计算分解为多个小块的计算，从而减少对 HBM 的读写。\n不过，这里有一个问题就是softmax的计算：它需要一整行的logits才能计算出归一化因子，而Flash Attention是按块计算的，没法一次性拿到整行logits。那么Flash Attention是如何解决这个问题的呢？答案就是：Online Softmax。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#online-softmax-for-attention",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#online-softmax-for-attention",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "6.2 Online Softmax for Attention",
    "text": "6.2 Online Softmax for Attention\nFlash Attention 使用 Online Softmax 来计算注意力矩阵的 softmax 部分。具体来说，Flash Attention 在计算每个块的注意力时，维护两个变量：m 和 l，分别表示当前块的最大值和归一化因子。在计算每个块的注意力时，Flash Attention 会更新 m 和 l，从而实现在线计算 softmax 的归一化因子。\n具体来说，假设我们正在计算第 \\(i\\) 个块的注意力，我们有：\n\\[\nS_{i,j} = Q_i K_j^\\top\n\\]\n然后，我们使用 Online Softmax 的更新公式来更新 m 和 l：\n\\[\n\\begin{split}\nm_{new} & = \\max(m, \\max(S_{i,j})) \\\\\nl & = l \\cdot \\exp(m - m_{new}) + \\sum_{k} \\exp(S_{i,j,k} - m_{new}) \\\\\nm & = m_{new}\n\\end{split}\n\\]\n通过这种方式，Flash Attention 可以在不保存完整注意力矩阵的情况下，计算出正确的 softmax 归一化因子。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#recomputing-for-backward-pass",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#recomputing-for-backward-pass",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "6.3 Recomputing for Backward Pass",
    "text": "6.3 Recomputing for Backward Pass\n我们知道，在反向传播时，需要用到前向传播中的一些中间结果来计算梯度。如果我们在前向传播时保存了所有的中间结果，那么会占用大量的内存空间。Flash Attention 通过 Recomputing 技术，选择性地不保存某些中间结果，而是在反向传播时重新计算它们，从而节省内存空间。具体来说，Flash Attention 在前向传播时，不保存完整的注意力矩阵 \\(S\\) 和 softmax 矩阵 \\(P\\)，而是在反向传播时重新计算它们。这样，Flash Attention 就节省了大量的内存空间，同时降低了读写内存的带宽需求，从而提升了整体的计算效率。\n具体来说，假设我们有一个注意力矩阵 \\(A = PV\\)，其中 \\(P = \\text{softmax}(S)\\)。在反向传播时，我们需要计算 \\(\\frac{\\partial L}{\\partial Q}\\)，\\(\\frac{\\partial L}{\\partial K}\\) 和 \\(\\frac{\\partial L}{\\partial V}\\)。Flash Attention 通过重新计算 \\(S\\) 和 \\(P\\) 来实现这一点：\n\\[\n\\begin{split}\n\\frac{\\partial L}{\\partial V} & = P^\\top \\frac{\\partial L}{\\partial A} \\\\\n\\frac{\\partial L}{\\partial P} & = \\frac{\\partial L}{\\partial A} V^\\top \\\\\n\\frac{\\partial L}{\\partial S} & = P \\odot \\left( \\frac{\\partial L}{\\partial P} - \\text{row\\_sum}\\left(P \\odot \\frac{\\partial L}{\\partial P}\\right) \\right) \\\\\n\\frac{\\partial L}{\\partial Q} & = \\frac{\\partial L}{\\partial S} K \\\\\n\\frac{\\partial L}{\\partial K} & = Q^\\top \\frac{\\partial L}{\\partial S}\n\\end{split}\n\\]\n通过这种方式，Flash Attention 在反向传播时重新计算了必要的中间结果，从而节省了内存空间，同时降低了读写内存的带宽需求。"
  },
  {
    "objectID": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#kernel-fusion",
    "href": "posts/100-AI-Papers/17-flash-attention/Flash Attention.html#kernel-fusion",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)",
    "section": "6.4 Kernel-Fusion",
    "text": "6.4 Kernel-Fusion\nFlash Attention 通过将多个计算步骤融合到一个 GPU kernel 中，减少了内存读写的开销，从而提升了整体的计算效率。具体来说，Flash Attention 将 QKᵀ 的计算、softmax 的计算和与 V 的乘法计算融合到一个 kernel 中，从而减少了中间结果的存储和读取。通过这种方式，Flash Attention 可以在一个 kernel 中完成所有的计算，从而减少了内存读写的开销。"
  },
  {
    "objectID": "posts/100-AI-Papers/03-deit/DeiT.html#experiment",
    "href": "posts/100-AI-Papers/03-deit/DeiT.html#experiment",
    "title": "03: Training data-efficient image transformers & distillation through attention (DeiT)",
    "section": "2.1 Experiment",
    "text": "2.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/15-stable-diffusion/Latent Diffusion Model.html",
    "href": "posts/100-AI-Papers/15-stable-diffusion/Latent Diffusion Model.html",
    "title": "15: High-Resolution Image Synthesis with Latent Diffusion Models (Latent Diffusion Model)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/15-stable-diffusion/Latent Diffusion Model.html#experiment",
    "href": "posts/100-AI-Papers/15-stable-diffusion/Latent Diffusion Model.html#experiment",
    "title": "15: High-Resolution Image Synthesis with Latent Diffusion Models (Latent Diffusion Model)",
    "section": "1.1 Experiment",
    "text": "1.1 Experiment"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html",
    "href": "posts/LLM-Series/Qwen/Qwen.html",
    "title": "Qwen Model Series",
    "section": "",
    "text": "在了解 Qwen 等LLM 模型前，我们需要先了解什么是 Transformer, 在这里就不具体展开了，有兴趣的同学前去查看 这篇文章。 对于Multi-Modality的Qwen，我们需要具备的是 Vision-Transformer 的知识。\n前置知识：\n\nTransformer\nVision-Transformer\n\nQwen（通义千问） 是阿里云旗下达摩院推出的一系列大语言模型（LLM) 与多模态模型（M-LLM）。它类似 OpenAI 的 GPT 系列，是阿里打造的全栈式统一LLM体系。 接下来，我们来沿着Qwen模型发展的时间线，来感受一下LLM发展的过程。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#tokenization",
    "href": "posts/LLM-Series/Qwen/Qwen.html#tokenization",
    "title": "Qwen Model Series",
    "section": "2.1 Tokenization",
    "text": "2.1 Tokenization\nQwen 用byte pair encoding (BPE)的Tokenization的方法，这与GPT-3.5，GPT-4系列是一样的。在训练Tokenization之后，最后的Vocabulary Size 由152K。 Qwen的Tokenization的方法，实现了较低的Compression Ratio。低Compression Ratio说明了Qwen在这些语言的Training 和 Inference 中会比较高效。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#architecture",
    "href": "posts/LLM-Series/Qwen/Qwen.html#architecture",
    "title": "Qwen Model Series",
    "section": "2.2 Architecture",
    "text": "2.2 Architecture\nQwen-1 的模型借鉴了LLaMA模型，也是Decoder-Only 的 Transformer (Vaswani et al. 2023) 模型，只不过有着以下几点的改变：\n\nEmbedding and output projection: LLaMA和Transformer 都用了Weight Tying的技术，这种方式可以减少模型的参数，提高训练的效率。不过Qwen没有沿用这种方式，而是让这两个有各自的parameters。\nPositional Embedding： Qwen用了 RoPE (Su et al. 2023) 来Encoding Position消息，同时运用了 FP32 的精度，来 inverse frequency matrix.\nBias: 对于许多的Layer，移除了bias的term，不过对于QKV Layers，还是加了Bias。\nPre-Norm & RMSNorm：Qwen 模型用了 Pre-Norm 和RMSNorm来当作Normalization 的方法\nActivation Function: 用了SwiGLU当作Activation Function, 为了保持模型参数的不变，减少了d_ff到 \\(\\frac{8}{3}\\)"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#pre-training",
    "href": "posts/LLM-Series/Qwen/Qwen.html#pre-training",
    "title": "Qwen Model Series",
    "section": "2.3 Pre-Training",
    "text": "2.3 Pre-Training\nPre-Training 遵循了标准的Auto-Regressive LM的训练目标，Context Length设为2048， 运用了Flash-Attention。 利用AdamW 的optimizer。 和 Cosine Learning Rate schedule. 并且运用了 Mixed Precision Training 为了提高模型的Stability 和 训练速度。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "href": "posts/LLM-Series/Qwen/Qwen.html#extend-context-length",
    "title": "Qwen Model Series",
    "section": "2.4 Extend Context Length",
    "text": "2.4 Extend Context Length\n模型在训练时的Context Length设为了2048，不过这在Inference时，是不够的。于是Qwen利用了一种training-free techniques 的方法， NTK-aware interpolation\n\n2.4.1 NTK-aware Interpolation\nNTK-aware Interpolation 与普通的Position Interpolation不同，NTK-aware Interpolation adjust the base of RoPE。 Qwen的团队在NTK-aware的基础上，为了更好的压榨出NTK的性能，实现了一个NTK的extension，叫做 dynamic NTK-aware interpolation。\n\n\n2.4.2 Attention\n除了Position Encoding，attention的计算效率也是阻碍Context length的原因之一。 Qwen的团队用了两个Attention的技巧： - LogN-Scaling - Layer-wise Window Attention: 不同的Layer 有不同的Window Size\n\nWe also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.  QWEN TECHNICAL REPORT, p.8 \n\n运用了以上几个技巧之后，Qwen 模型将context length 从2048提升到了8192， 在没有损害模型能力的前提下。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#alignment",
    "href": "posts/LLM-Series/Qwen/Qwen.html#alignment",
    "title": "Qwen Model Series",
    "section": "2.5 Alignment",
    "text": "2.5 Alignment\n在Pre-train Qwen之后，我们不能直接使用，\n\n2.5.1 Supervised Fine-Tuning\nSFT的训练，可以让Qwen模型遵循Chat类型的回答。\n\n2.5.1.1 Data\n{\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful, harmless and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"帮我写一段关于Qwen模型的简介。\"},\n    {\"role\": \"assistant\", \"content\": \"Qwen 是阿里云推出的一系列大语言模型...\"}\n  ]\n}\n\n\n2.5.1.2 Training\n与Pre-Training类似，SFT 用了相同的训练目标 Next-Token Prediction，不过与Pre-Training不同的是，SFT用了一个Loss Mask来mask掉system 和 user inputs。\n同样运用了 AdamW optimizer\n\n\n\n2.5.2 RLHF\n在SFT之后，模型可能overfitting，并且缺少generalization 和 creativity。 为了让模型获得这些能力，在SFT之后，我们需要RLHF，这个过程涉及到两个步骤： 1. Reward Model Training 2. Policy Training\n\n2.5.2.1 Reward Model Training\nReward Model Training 也叫做 Preference Model Pretraining (PMP)， 这个同样需要pre- training 然后Fine-Tunining。 PMP 的训练数据, 也是由一系列的comparison data 组成。\n{\n  \"prompt\": \"Explain why the Earth has seasons.\",\n  \"chosen\": \"The Earth has seasons because its axis is tilted about 23.5 degrees. As the planet orbits the Sun, this tilt causes different hemispheres to receive more or less direct sunlight throughout the year, creating seasonal temperature and daylight changes.\",\n  \"rejected\": \"The Earth has seasons because sometimes it randomly moves closer to the Sun and sometimes farther away.\"\n}\n训练这个Reward Model。 Qwen的团队用了Pre-trained Language Model 也就是Qwen， 来当作Initiate 权重。 在这个Qwen模型之上，加上了一层Pooling Layer来提取出 Reward （一个Scalar Value） #### Policy Training\n在训练完Reward Model之后，下一步就是运用Reinforcement的算法来训练LLM。Qwen的团队运用了PPO的算法，来训练，这个算法由4个部分： - Policy Model - Value Model - Reference Model - Reward Model\n至此，Qwen的Foundation Model，以及训练结束了。接下来可以通过不同的训练数据，让Qwen 获得不同的能力，比如Code-Qwen，以及Math-Qwen"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#code-qwen",
    "href": "posts/LLM-Series/Qwen/Qwen.html#code-qwen",
    "title": "Qwen Model Series",
    "section": "2.6 Code Qwen",
    "text": "2.6 Code Qwen"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#math-qwen",
    "href": "posts/LLM-Series/Qwen/Qwen.html#math-qwen",
    "title": "Qwen Model Series",
    "section": "2.7 Math Qwen",
    "text": "2.7 Math Qwen"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "href": "posts/LLM-Series/Qwen/Qwen.html#qwen-1-summary",
    "title": "Qwen Model Series",
    "section": "2.8 Qwen 1 Summary",
    "text": "2.8 Qwen 1 Summary"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#model-architecture",
    "href": "posts/LLM-Series/Qwen/Qwen.html#model-architecture",
    "title": "Qwen Model Series",
    "section": "3.1 Model Architecture",
    "text": "3.1 Model Architecture\nQwen-VL的模型主要包含了以下几个组件： - Large Language Model： Qwen-VL是基于之前的 Qwen-7B来当作大语言组件 - Visual Encoder： Visual Encoder的结构是，与 Vision-Transformer (Dosovitskiy et al. 2021) 的结构是一样的。通过加载 OpenCLIP的权重, 来初始化ViT - Position-aware Vision-Language Adapter: 为了减少 image feature的长度，Qwen-VL 利用了 Vision Language Adapter. 这个是一组Cross-Attention Module 随机初始化，这种方法将Visual Feature的长度，压缩到了256. 在这个Adapter 中，2D absolute positional encoding 也添加了进来，用来减少可能消息的丢失。\n ## Inputs and Outputs 添加了Visual Tokens之后，模型需要一种方法，来辨别出哪些是Visual Tokens, 哪些是Text Tokens\n\nImages are processed through the visual encoder and adapter, yielding fixed-length sequences of image features. To differentiate between image feature input and text feature input, two special tokens ( and ) are appended to the beginning and end of the image feature sequence respectively, signifying the start and end of image content  Qwen-VL- A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond, p.4 \n\n对于不同的输入和要求，模型的输出的内容是不一样的。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#training-1",
    "href": "posts/LLM-Series/Qwen/Qwen.html#training-1",
    "title": "Qwen Model Series",
    "section": "3.2 Training",
    "text": "3.2 Training\nQwen-VL 的训练分为以下几个步骤，如下图所示：\n\n\n3.2.0.1 Pre-Training\n在这个阶段，模型训练出认识图片的能力，冻结了LLM，训练ViT和VL adapter。\n\n\n3.2.0.2 Multi-Task Pre-Training\n在这个阶段，模型已经有了对Image的基本认知，接下来就是训练模型对于不同要求的输出，也就是所谓的Multi-Task。 在这个阶段，所有的权重，都进行微调。通过这个训练，模型获得了完成不同任务的能力。\n\n\n3.2.0.3 Supervised Finetuning\n在这个阶段下，Qwen团队训练了Qwen-VL，使它获得Instruction- Following的能力。 也就是Qwen-VL-Chat Model，\nSupervised Finetuning 的数据如下图所示： \n至此，Qwen-VL的训练已经结束了，"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "href": "posts/LLM-Series/Qwen/Qwen.html#qwen1-vl-sumary",
    "title": "Qwen Model Series",
    "section": "3.3 Qwen1-VL Sumary",
    "text": "3.3 Qwen1-VL Sumary"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#tokenizer",
    "href": "posts/LLM-Series/Qwen/Qwen.html#tokenizer",
    "title": "Qwen Model Series",
    "section": "5.1 Tokenizer",
    "text": "5.1 Tokenizer\nQwen2 的Tokenization的方法，与Qwen1 是一样的，都采用了BPE Tokenization Algorithms。Vocabulary Size 有151,643 Regular tokens和3 个control tokens。"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#architecture-1",
    "href": "posts/LLM-Series/Qwen/Qwen.html#architecture-1",
    "title": "Qwen Model Series",
    "section": "5.2 Architecture",
    "text": "5.2 Architecture\n与Qwen1 相比，Qwen2 的模型改动并没有很大，主要体现在以下几个方面\n\n5.2.1 Dense Model\n\n5.2.1.1 Gouged Query Attention\n运用了Grouped Query Attention, 这种方式可以优化KV Cache\n\n\n5.2.1.2 Dual Chunk Attention with YARN\n运用了Dual Chunk Attention 和 YARN 来提高Context 的长度。\n\n\n\n5.2.2 Mixture-Of-Expert Model"
  },
  {
    "objectID": "posts/LLM-Series/Qwen/Qwen.html#architecture-2",
    "href": "posts/LLM-Series/Qwen/Qwen.html#architecture-2",
    "title": "Qwen Model Series",
    "section": "6.1 Architecture",
    "text": "6.1 Architecture\n\n\n6.1.1 Naive Dynamic Resolution\nNaive Dynamic Resolution 在 NaViT (Dehghani et al. 2023) 中提出，它可以让Vision Transformer 在不同长度的图片中训练。\n除此之外，Qwen2-VL 摒弃了Absolute Position Embedding，转用2D-RoPE (Su et al. 2023), 除此之外，在ViT 生成Representation之后，Qwen2-VL 还在之后添加了一层MLPLayer，它的作用是减少Tokens的数量，通过临近的 \\(2\\times 2\\) 的tokens，MLP讲这些tokens merge在一起，将Tokens的数量减少了 4 倍。比如一张 \\(224 \\times 224\\) 的图片, 分成大小为14的patches，得到了256 个tokens，将这些tokens merge在一起，我们就得到了 64 个tokens。\n\n\n6.1.2 Multi-Modal Rotary Position Embedding (M-RoPE)\nQwen2-VL 利用M-RoPE来提取出不同Modality之间的position 的信息。 通过将Rotary Embedding 分解成： - Temporal - Height - Width 三个部分。 对于Text 的部分，这几个部分会得到相同的IDs，这使得它可以类似于1D-RoPE的作用。对于图片的输入，Temporal 部分的ID保持不变，Height 和 Width的部分，会得到不同的IDs。 对于视频的输入，Temporal，Height，Width都会改变。\n\nM-RoPE not only enhances the modeling of positional information but also reduces the value of position IDs for images and videos, enabling the model to extrapolate to longer sequences during inference.  Qwen -VL- Enhancing Vision-Language Model’s Perception of the World at Any Resolution, p.5 \n\n\nFor text, these three use identical position IDs → equivalent to standard 1D RoPE.\nFor images, temporal ID is constant; height & width IDs encode patch position.\nFor videos, temporal ID increases per frame; height & width same as image."
  },
  {
    "objectID": "posts/DLFaC/index.html",
    "href": "posts/DLFaC/index.html",
    "title": "Deep Learning Foundation and Concepts(DLFaC) Learning Notes",
    "section": "",
    "text": "Note\n\n\n\n Due to the time constraint, these notes may contain errors or omissions. And to speed up the writing process, I only wrote the notes in CHINESES. If you are interested in collaborating to improve these notes,or transalting to different languages, please feel free to contact me. \n\n\nRelated Resources:\n\nBook Website: Deep Learning Foundations and Concepts\nMy GitHub Repo: GitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 01: Introduction to the Deep Learning\n\n\nChapter 01 主要介绍了什么Deep Learning 与其的应用，并且通过一个简单的 Polynomial Regression 例子来说明 Deep Learning 模型训练的基本流程。通过Chapter 01的学习，读者可以初步了解深度学习的基本概念、应用领域以及模型训练的基本步骤。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 02: Probabilities & Information Theory\n\n\nChapter 02 介绍了概率论和信息论的基本概念，包括随机变量、概率分布、条件概率、熵等内容。这些概念在深度学习中尤为重要，因为它们帮助我们理解模型如何处理不确定性和信息。\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DLFaC/Chapter02/Chapter02.html",
    "href": "posts/DLFaC/Chapter02/Chapter02.html",
    "title": "Chapter 02: Probabilities & Information Theory",
    "section": "",
    "text": "在深度学习中，我们经常会遇到不确定性的问题。概率论为我们提供了一套工具，用于量化和处理这种不确定性。信息论则帮助我们理解信息的传递和存储方式。在本章中介绍了这些基本概念，为后续章节的学习打下了坚实的基础。其中，不确定可以大致分为两类：\n我们可以使用概率论和信息论的工具来量化和处理这些不确定性，从而提升模型的性能和鲁棒性。首先我们如何定义 Probability 呢？从频率学派（Frequentist View）的角度来看，概率可以被定义为在大量重复实验中某一事件发生的频率。假设我们有一个随机变量 \\(X\\)，它可以取值 \\(x_1, x_2, \\ldots, x_n\\)，那么事件 \\(X = x_i\\) 的概率可以表示为：\\[\nP(X = x_i) = \\lim_{N \\to \\infty} \\frac{N_i}{N}\n\\]其中，\\(N\\) 是实验的总次数，\\(N_i\\) 是事件 \\(X = x_i\\) 发生的次数。 这种定义强调了概率的客观性，即概率是通过实际观察和实验得出的。然而，在实际应用中，我们往往无法进行无限次的实验，因此我们需要使用统计方法来估计概率分布。 另一方面，贝叶斯学派（Bayesian View）则将概率视为对事件发生的不确定性的主观度量。根据贝叶斯观点，概率反映了我们对某一事件发生的信念程度，而不是仅仅依赖于频率。贝叶斯定理是贝叶斯学派的核心，它描述了如何根据新的证据更新我们的信念。贝叶斯定理可以表示为：\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]其中，\\(P(A|B)\\) 是在事件 \\(B\\) 发生的条件下事件 \\(A\\) 发生的概率，\\(P(B|A)\\) 是在事件 \\(A\\) 发生的条件下事件 \\(B\\) 发生的概率，\\(P(A)\\) 是事件 \\(A\\) 的先验概率，\\(P(B)\\) 是事件 \\(B\\) 的边际概率。 贝叶斯方法允许我们结合先验知识和观察数据，从而更灵活地处理不确定性问题。在深度学习中，贝叶斯方法被广泛应用于模型选择、参数估计和不确定性量化等方面。\n接下来，我们具体来看一下概率论"
  },
  {
    "objectID": "posts/DLFaC/Chapter02/Chapter02.html#sum-rule",
    "href": "posts/DLFaC/Chapter02/Chapter02.html#sum-rule",
    "title": "Chapter 02: Probabilities & Information Theory",
    "section": "1.1 Sum Rule",
    "text": "1.1 Sum Rule\nSum Rule 用于计算联合概率分布中的边际概率。假设我们有两个随机变量 \\(X\\) 和 \\(Y\\)，它们的联合概率分布为 \\(P(X, Y)\\)。根据 Sum Rule，我们可以通过对另一个变量进行求和来计算边际概率： \\[\nP(X) = \\sum_{y} P(X, Y = y)\n\\]\n对于连续随机变量，我们使用积分来代替求和：\n\\[\nP(X) = \\int P(X, Y = y) dy\n\\]\nSum Rule 的直观理解是，通过考虑所有可能的 \\(Y\\) 的取值，我们可以得到 \\(X\\) 的总概率。"
  },
  {
    "objectID": "posts/DLFaC/Chapter02/Chapter02.html#product-rule",
    "href": "posts/DLFaC/Chapter02/Chapter02.html#product-rule",
    "title": "Chapter 02: Probabilities & Information Theory",
    "section": "1.2 Product Rule",
    "text": "1.2 Product Rule\nProduct Rule 用于计算联合概率分布中的条件概率。根据 Product Rule，联合概率可以表示为条件概率和边际概率的乘积： \\[\nP(X, Y) = P(X|Y) \\cdot P(Y)\n\\] 同样地，我们也可以交换 \\(X\\) 和 \\(Y\\) 的位置： \\[\nP(X, Y) = P(Y|X) \\cdot P(X)\n\\]\nProduct Rule 的直观理解是，联合事件 \\(X\\) 和 \\(Y\\) 的概率可以通过先计算 \\(Y\\) 发生的概率，然后在 \\(Y\\) 已经发生的条件下计算 \\(X\\) 发生的概率来得到。\n当然，结合 Sum Rule 和 Product Rule，我们可以推导出更多复杂的概率关系，例如贝叶斯定理。"
  },
  {
    "objectID": "posts/DLFaC/Chapter02/Chapter02.html#bayes-theorem",
    "href": "posts/DLFaC/Chapter02/Chapter02.html#bayes-theorem",
    "title": "Chapter 02: Probabilities & Information Theory",
    "section": "1.3 Bayes’ Theorem",
    "text": "1.3 Bayes’ Theorem\n贝叶斯定理是概率论中的一个重要定理，它描述了如何根据新的证据更新我们的信念。贝叶斯定理可以表示为： \\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\] 其中，\\(P(A|B)\\) 是在事件 \\(B\\) 发生的条件下事件 \\(A\\) 发生的概率，\\(P(B|A)\\) 是在事件 $A 发生的条件下事件 \\(B\\) 发生的概率，\\(P(A)\\) 是事件 \\(A\\) 的先验概率，\\(P(B)\\) 是事件 \\(B\\) 的边际概率。 贝叶斯定理允许我们结合先验知识和观察数据，从而更灵活地处理不确定性问题。\n好的，这种概念很抽象，我们通过一个简单的例子来理解贝叶斯定理的应用。"
  },
  {
    "objectID": "posts/DLFaC/Chapter02/Chapter02.html#medical-screening-example",
    "href": "posts/DLFaC/Chapter02/Chapter02.html#medical-screening-example",
    "title": "Chapter 02: Probabilities & Information Theory",
    "section": "1.4 Medical Screening Example",
    "text": "1.4 Medical Screening Example\n假设我们有一个医疗测试，用于检测某种疾病。设事件 \\(D\\) 表示“患有疾病”，事件 \\(T\\) 表示“测试结果为阳性”。已知以下信息：\n\n该疾病在总体中的患病率为 \\(P(D) = 0.01\\)（即 1% 的人患有该疾病）。\n测试的灵敏度（即患病者测试为阳性的概率）为 \\(P(T|D) = 0.99\\)。\n测试的特异度（即未患病者测试为阴性的概率）为 \\(P(T^c|D^c) = 0.95\\)，其中 \\(T^c\\) 表示测试结果为阴性，\\(D^c\\) 表示未患病。\n因此，未患病者测试为阳性的概率为 \\(P(T|D^c) = 1 - P(T^c|D^c) = 0.05\\)。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html",
    "href": "posts/DLFaC/Chapter01/Chapter01.html",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "",
    "text": "Chapter 01主要介绍了什么是Deep Learning，为什么我们要学习Deep Learning 的原因。通过几个实际的例子，说明了Deep Learning 在各个领域的应用。最后通过一个简单的Polynomial Regression 例子，介绍了Deep Learning 模型训练的基本流程以及相关的概念。 接下来让我们具体看一下本章的内容。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#applications-of-deep-learning",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#applications-of-deep-learning",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "1.1 Applications of Deep Learning",
    "text": "1.1 Applications of Deep Learning\n深度学习已经在多个领域展现出了强大的能力，书中举了4个经典的例子：\n\nMedical diagnosis： 通过训练一个Neural Network模型来分析医学影像数据，从而实现对疾病的自动诊断和预测。\nProtein structure： 通过深度学习模型来预测蛋白质的三维结构，从而加速新药的研发和生物学研究。\nImage synthesis： 通过深度学习技术生成高质量的图像，用于艺术创作、游戏开发等领域。\nLarge language models（LLM）： 通过训练大规模的神经网络模型，实现对自然语言的理解和生成，现在的ChatGPT就是基于LLM，并在此基础上进行其他训练得到的。\n\n具体的例子内容在这里就不展开了，有兴趣的同学可以参考书中的内容。在这里整理一下这些例子的相同点与不同点，并且由此总结一下常见的深度学习的任务类型：\n\n1.1.1 Classification vs. Regression\n上面提到的Medical diagnosis和Large language models都是属于Classification任务： 给定输入数据，模型需要预测其所属的类别标签。例如，Medical diagnosis中模型需要判断患者是否患有某种疾病，Large language models中模型需要预测下一个单词的类别。与其相对的，Regression任务则是预测一个连续的数值。例如，预测房价、股票价格等。\nClassification和Regression是监督学习（Supervised Learning）中最常见的两种任务类型。而监督学习是指模型在训练过程中需要使用带有标签的数据进行学习，从而使模型能够对新数据进行预测。比如对于Medical diagnosis任务，训练数据中每个医学影像都对应一个疾病标签，我们有训练数据 \\(\\mathcal{D}\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)\\}\\) 模型通过学习这些标签来进行预测。不过，与Medical diagnosis不同，Large language models的训练数据通常是无标签的文本数据（只是一段段文字），不过我们实际是有标签的（下一个单词是什么），只不过没有显式的表现出来。这种通过构造一种任务（Pre-text task）来进行训练的方式，称为自监督学习（Self-Supervised Learning），也是监督学习的一种形式。\n\n\n1.1.2 Generative vs. Discriminative Models\n要论现在最火的深度学习模型，非生成式AI（Generative AI）莫属。ChatGPT、Midjourney等都是生成式模型的典型代表。生成式模型的目标是学习数据的分布，从而能够生成与训练数据相似的新数据。例如，给定一段文本，生成式语言模型可以生成与之相关的文本内容；给定一张图像，生成式图像模型可以生成与之相似的图像。与之相对的，判别式模型（Discriminative Models）则是直接学习输入数据与标签之间的映射关系，从而进行分类或回归任务。例如，Medical diagnosis, Protein structure等任务通常使用判别式模型进行预测。\n\n\n1.1.3 Learning vs. Inference\n深度学习模型的训练过程通常分为两个阶段：学习（Learning）和推断（Inference）。学习阶段是指模型通过训练数据进行参数优化，从而使模型能够更好地拟合数据分布。推断阶段则是指模型在训练完成后，对新数据进行预测或生成的过程。学习阶段通常需要大量的计算资源和时间，而推断阶段则相对较快，适合实时应用。这本书的后续章节主要介绍学习阶段的内容，包括模型的构建、损失函数的设计、优化算法等。而推断阶段则相对简单，主要涉及模型的部署和应用。\n\n\n1.1.4 Fine-tuning vs. From Scratch\n在实际应用中，深度学习模型的训练通常有两种方式：从头开始训练（From Scratch）和微调（Fine-tuning）。从头开始训练是指模型的参数全部随机初始化，然后通过训练数据进行优化。这种方式通常需要大量的训练数据和计算资源。微调则是指在一个预训练模型的基础上，使用特定任务的数据对模型进行进一步的训练，从而使模型更好地适应特定任务。微调通常能够显著提高模型的性能，尤其是在训练数据有限的情况下。其中，Transfer Learning（迁移学习）是一种常见的微调方法，通过将预训练模型的知识迁移到新任务中，从而提高模型的泛化能力。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#summary-of-deep-learning-applications",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#summary-of-deep-learning-applications",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "1.2 Summary of Deep Learning Applications",
    "text": "1.2 Summary of Deep Learning Applications\n通过上面的几个例子，我们初步了解了深度学习在各个领域的应用，以及常见的任务类型和训练方式。深度学习作为一种强大的机器学习方法，已经在医疗诊断、蛋白质结构预测、图像合成和自然语言处理等多个领域展现出了巨大的潜力。随着计算资源的提升和数据量的增加，深度学习有望在未来继续推动人工智能的发展，带来更多创新和变革。不理解这些的同学也不用担心，等待后续章节的学习，你会对这些概念有更深入的理解。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#datasets",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#datasets",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "2.1 Datasets",
    "text": "2.1 Datasets\n在这个例子中，我们使用一个简单的合成数据集。假设我们有一个输入变量 \\(x\\)，其取值范围为 \\([0, 1]\\)，输出变量 \\(t\\) 与输入变量之间的关系为一个二次多项式函数，并且添加了一些随机噪声。具体来说，数据集中的每个样本 \\((x_i, t_i)\\) 满足以下关系：\n\\[\nt_i= \\sin(2 \\pi x_i) + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, 0.1)\n\\]\n其中，\\(\\epsilon_i\\) 是服从均值为0，方差为0.1的高斯噪声。我们生成了100个样本作为训练数据集，另外生成了50个样本作为测试数据集。\n从这个构造的训练数据中，我们可以看到现实世界中的训练数据的构建：\n\n数据通常是带有噪声的，模型需要学会从噪声中提取有用的信息。\n我们的训练目标是希望模型可以在没有见过的测试数据上也有良好的表现。这就是所谓的泛化能力（Generalization）。\n\n从而我们构建出了训练数据集 \\(\\mathcal{D}_{train}=\\{(x_1, t_1), (x_2, t_2), \\ldots, (x_{10}, t_{10})\\}\\) 和测试数据集 \\(\\mathcal{D}_{test}=\\{(x_1, t_1), (x_2, t_2), \\ldots, (x_{50}, t_{50})\\}\\)。\n\n\n\n\n\n\nFigure 2: 10 个训练数据点的分布情况，可以看到数据点大致分布在 \\(\\sin(2 \\pi x)\\) 曲线附近，但由于噪声的存在，数据点并不完全在曲线上。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#model",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#model",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "2.2 Model",
    "text": "2.2 Model\n有了数据集\\(\\mathcal{D}_{train}\\)之后，接下来我们需要构建一个深度学习模型。模型是许多研究的核心内容，在这里我们使用一个简单的Linear Model来进行Polynomial Regression。具体来说，我们使用一个二次多项式模型来拟合数据：\n\\[\ny(x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\ldots + w_M x^M = \\sum_{j=0}^{M} w_j x^j\n\\]\n其中，\\(\\mathbf{w} = [w_0, w_1, \\ldots, w_M]^T\\) 是模型的参数向量，\\(M\\) 是多项式的阶数。在这个例子中，我们选择 \\(M=9\\)，即使用一个9次多项式模型来拟合数据。\n\n\n\n\n\n\nWhy called Linear Model?\n\n\n\n虽然这个模型是一个多项式模型，但它仍然被称为线性模型（Linear Model），因为它在参数 \\(\\mathbf{w}\\) 上是线性的。也就是说，模型的输出 \\(y(x, \\mathbf{w})\\) 是参数 \\(\\mathbf{w}\\) 的线性组合，而不是输入变量 \\(x\\) 的线性组合。这种线性性质使得模型的训练和优化更加简单和高效。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#loss-function-optimization",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#loss-function-optimization",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "2.3 Loss Function & Optimization",
    "text": "2.3 Loss Function & Optimization\n有了模型之后，接下来我们需要定义一个损失函数（Loss Function）来衡量模型的预测结果与真实标签之间的差距。在这里，我们使用Sum of Squared Errors (SSE) 作为损失函数，定义如下：\n\\[\nE(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} (y(x_i, \\mathbf{w}) - t_i)^2\n\\]\n其中，\\(N\\) 是训练数据集中的样本数量，\\(y(x_i, \\mathbf{w})\\) 是模型对输入 \\(x_i\\) 的预测结果，\\(t_i\\) 是对应的真实标签。损失函数 \\(E(\\mathbf{w})\\) 衡量了模型在训练数据集上的表现，目标是通过调整参数 \\(\\mathbf{w}\\) 来最小化损失函数，从而使模型能够更好地拟合数据。\\(\\frac{1}{2}\\) 是为了在后续计算梯度时更加方便。\n我们希望通过最小化损失函数 \\(E(\\mathbf{w})\\)，使得模型的预测结果尽可能接近真实标签，从而提高模型的泛化能力。我们知道 \\(E(\\mathbf{w})\\) 是 \\(\\mathbf{w}\\) 的函数，因此我们可以通过优化算法来调整参数 \\(\\mathbf{w}\\)，使得损失函数达到最小值。在这个简单的例子中，我们可以使用解析解（Analytical Solution）来直接计算出最优参数 \\(\\mathbf{w}^*\\)，因为损失函数是一个关于 \\(\\mathbf{w}\\) 的二次函数，具有唯一的全局最小值（高中知识， 二次函数的图像是一个抛物线，取其导数为0的点 ）。具体来说，最优参数 \\(\\mathbf{w}^*\\) 可以通过以下公式计算得到： \\[\n\\mathbf{w}^* = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{t}\n\\]\n， 但在现实中 通常会使用数值优化方法（如梯度下降）来进行参数优化。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#model-complexity-model-selection",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#model-complexity-model-selection",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "2.4 Model Complexity & Model Selection",
    "text": "2.4 Model Complexity & Model Selection\n接下来，我们看看这个模型在测试集 \\(\\mathcal{D}_{test}\\) 上的表现。我们可以计算模型在测试集上的均方误差（Mean Squared Error, MSE）来评估模型的泛化能力： \\[\n\\text{MSE} = \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} (y(x_i, \\mathbf{w}^*) - t_i)^2\n\\] 其中，\\(N_{test}\\) 是测试数据集中的样本数量，\\(y(x_i, \\mathbf{w}^*)\\) 是模型对输入 \\(x_i\\) 的预测结果，\\(t_i\\) 是对应的真实标签。通过计算MSE，我们可以评估模型在未见过的数据上的表现，从而判断模型的泛化能力。\n\nMake accurate predictions on previously unseen inputs is a key goal in machine learning and is known as generalization.  Deep Learning Foundations and Concepts, p.6 \n\nTrain Set:\n  SSE of Train Set: 0.0000\n  RMSE of Train Set: 0.0000\nTest Set:  \n  SSE of Test Set: 9.0839  \n  RMSE of Test Set: 0.4262  \n\n\n\n\n\n\nFigure 3\n\n\n\n我们可以看到，模型在训练集上的表现非常好，几乎没有误差（SSE和RMSE都接近于0）。但是在测试集上的表现就差很多了，SSE达到了9.0839，RMSE也达到了0.4262。这说明模型在训练数据上过拟合（Overfitting）了，无法很好地泛化到未见过的数据上。这就是模型复杂度（Model Complexity）和模型选择（Model Selection）的问题。\n我们可以看出，这个模型对于我们的训练数据来说，过于的复杂，导致模型在训练数据上表现很好，但在测试数据上表现很差。对于复杂的模型，其中一个方法就是增加训练数据的数量\n\n在这个例子中，我们使用了一个9次多项式模型来拟合数据，这个模型的复杂度较高，容易导致过拟合。为了提高模型的泛化能力，我们可以考虑使用更简单的模型（如低阶多项式）或者引入正则化（Regularization）来控制模型的复杂度。\n\n2.4.1 Regularization\n我们可以看看 \\(\\mathbf{w}\\) 的值：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw0\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\n\n\n\n\n0.113824\n4.405167\n94.162776\n-1514.731222\n9498.903764\n-31346.008432\n58321.808397\n-61525.006624\n34368.919801\n-7902.500007\n\n\n\n\n可以看到，模型的参数变化非常大，这也是过拟合的一个表现。模型相当于“定制”了训练数据中的噪声，从而导致在测试数据上的表现很差。为了缓解过拟合问题，我们可以引入正则化项（Regularization Term）来控制模型的复杂度。常见的正则化方法有L2正则化（Ridge Regression）和L1正则化（Lasso Regression）。以L2正则化为例，我们可以将损失函数修改为： \\[\nE_{reg}(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{N} (y(x_i, \\mathbf{w}) - t_i)^2 + \\frac{\\lambda}{2} \\sum_{j=0}^{M} w_j^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw0\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\n\n\n\n\n0.607766\n3.463379\n-11.168293\n-1.675222\n3.747767\n4.911149\n3.725974\n1.447197\n-1.24926\n-4.015138\n\n\n\n\n可以看到，引入正则化项后，模型的参数变得更加平滑，变化范围也变小了。这有助于提高模型的泛化能力，减少过拟合的风险。\n\n\n\n\n\n\nFigure 4\n\n\n\n但是要选择合适的正则化参数 \\(\\lambda\\) 也是一个挑战。过大的 \\(\\lambda\\) 会导致模型欠拟合（Underfitting），而过小的 \\(\\lambda\\) 则无法有效地控制模型复杂度。因此，我们需要通过交叉验证（Cross Validation）等方法来选择合适的正则化参数，从而提高模型的泛化能力。\n当我们取的到合适的正则化参数 \\(\\lambda\\) 后，我们可以看到模型在测试集上的表现有了显著的提升：\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n2.4.2 Cross Validation\n想这种选择 \\(M\\) 和 \\(\\lambda\\) 的过程，我们称之为模型选择（Model Selection）。模型选择的目标是找到一个既能很好地拟合训练数据，又能在测试数据上表现良好的模型。常见的模型选择方法有交叉验证（Cross Validation）、信息准则（Information Criteria）等。\n在这里我们介绍最常见的Cross Validation。Cross Validation是一种评估模型泛化能力的方法，通过将训练数据划分为多个子集，依次使用其中一个子集作为验证集，其他子集作为训练集，来评估模型在未见过的数据上的表现。通过多次重复这个过程，我们可以得到模型在不同验证集上的平均表现，从而更准确地评估模型的泛化能力。\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n显然，Cross Validation 可以帮助我们更好地选择模型的复杂度参数 \\(M\\) 和正则化参数 \\(\\lambda\\)，从而提高模型的泛化能力。当它也存在一个明显的缺点，就是计算开销较大：对于每一组参数组合，我们都需要进行多次训练和评估，这对于大规模数据集和复杂模型来说，可能会非常耗时。因此在实际应用中，我们通常根据经验和先验知识来缩小参数搜索空间，从而减少计算开销。\n\n\n\n\n\n\nNote\n\n\n\n这就是为什么，好多人称深度学习是一个实验性很强的领域。因为很多模型的超参数（Hyper-parameters）是需要通过实验来调优的。大家都把这个过程叫做炼丹，因为这个过程就像炼丹一样，需要不断地试验和调整，才能找到最优的配方。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#model-training-summary",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#model-training-summary",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "2.5 Model Training Summary",
    "text": "2.5 Model Training Summary\n通过这个简单的Polynomial Regression例子，我们了解了深度学习模型的整个过程：\n\n构建数据集：收集和准备训练数据和测试数据。\n定义模型：选择合适的模型结构和参数。合适的模型结构能够更好地捕捉数据的特征，从而提高模型的表现。\n定义损失函数：选择合适的损失函数来衡量模型的表现。\n优化模型参数：使用优化算法来调整模型参数，使损失函数最小化。\n评估模型表现：使用测试数据来评估模型的泛化能力。\n\n万变不离其宗，深度学习的核心就是通过构建合适的模型和优化算法，从数据中学习有用的特征表示，从而实现对复杂任务的解决。哪怕是现在的LLM模型，其实也是遵循这个基本流程，只不过模型结构更加复杂，优化算法更加先进，数据量也更加庞大罢了。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#preceptron",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#preceptron",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "3.1 Preceptron",
    "text": "3.1 Preceptron\n感知机（Perceptron）是由Frank Rosenblatt在1958年提出的一种简单的神经网络模型。它是最早的神经网络之一，主要用于二分类任务。感知机的基本结构包括输入层、权重、偏置和激活函数。其工作原理如下：\n\n输入层：感知机接收多个输入信号，每个输入信号对应一个权重。\n权重和偏置：每个输入信号乘以对应的权重，然后将所有加权输入信号相加，再加上一个偏置项。\n激活函数：将加权和通过一个激活函数（通常是阶跃函数），如果结果大于某个阈值，则输出1，否则输出0。\n学习过程：感知机通过调整权重和偏置来最小化预测结果与真实标签之间的误差，从而学习输入数据与输出标签之间的关系。\n\n其中激活函数通常使用阶跃函数（Step Function）： \\[\nf(z) = \\begin{cases}\n1, & \\text{if } z \\geq 0 \\\\\n0, & \\text{if } z &lt; 0\n\\end{cases}\n\\]\n感知机的提出展示了神经网络在模式识别中的潜力，然而它也存在一些局限性。例如，单层感知机只能解决线性可分的问题，无法处理非线性问题。为了解决这个问题，研究人员引入了多层感知机（Multi-Layer Perceptron, MLP），通过增加隐藏层来提高模型的表达能力。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#feed-forward-neural-networks",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#feed-forward-neural-networks",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "3.2 Feed Forward Neural Networks",
    "text": "3.2 Feed Forward Neural Networks\nPreceptron的局限性促使研究人员探索更复杂的神经网络结构。1986年，Rumelhart等人提出了反向传播算法（Backpropagation），使得多层神经网络的训练成为可能。主要的变化包括：\n\n引入Differential Calculus，使得我们可以计算损失函数对于每个参数的梯度。\n应用了Gradient-Based Optimization 方法（如 Stochastic Gradient Descent, SGD），通过梯度下降来更新模型参数，从而最小化损失函数。\n使用了非线性激活函数（如Sigmoid, ReLU），使得神经网络能够捕捉复杂的非线性关系。\n\n这些改进使得多层神经网络能够更好地拟合复杂的数据分布，推动了神经网络研究的发展。"
  },
  {
    "objectID": "posts/DLFaC/Chapter01/Chapter01.html#deep-neural-networks",
    "href": "posts/DLFaC/Chapter01/Chapter01.html#deep-neural-networks",
    "title": "Chapter 01: Introduction to the Deep Learning",
    "section": "3.3 Deep Neural Networks",
    "text": "3.3 Deep Neural Networks\n随着计算能力的提升和大规模数据集的出现，深度神经网络（Deep Neural Networks, DNNs）在21世纪初得到了广泛关注。2006年，Hinton等人提出了深度置信网络（Deep Belief Networks, DBNs），通过无监督预训练来初始化深度神经网络的参数，从而缓解了梯度消失问题。随后，卷积神经网络（Convolutional Neural Networks, CNNs）在图像识别任务中取得了突破性进展，特别是2012年AlexNet在ImageNet竞赛中的胜利，标志着深度学习的崛起。其基本的概念与FFNN类似，只不过引入了卷积层（Convolutional Layer）和池化层（Pooling Layer），从而更好地处理图像数据。\n随着深度学习的发展，不同的技术的提出，比如Auto-Differentiation 等，使得训练深度神经网络变得更加高效和便捷。由此也催生出了许多不同的模型架构，如循环神经网络（Recurrent Neural Networks, RNNs），Transformer于处理序列数据，生成对抗网络（Generative Adversarial Networks, GANs）用于生成任务等。\n可以看到，人工智能的成功不是一蹴而就的，而是经历了多个阶段的发展和积累。每一个重要的突破都离不开前人的努力和创新。通过了解深度学习的发展历史，我们可以更好地理解其现状和未来趋势，为进一步的学习和研究打下坚实的基础。"
  },
  {
    "objectID": "posts/LLM-Series/index.html",
    "href": "posts/LLM-Series/index.html",
    "title": "LLM Model Series Learning Notes",
    "section": "",
    "text": "Qwen Model Series\n\n\n在这篇文章中，我们将探索一系列的Qwen模型，沿着Qwen模型的发展时，来看看不同时期的Qwen模型运用了怎么样的不同的技术\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/100-AI-Papers/100_Papers_index.html",
    "href": "posts/100-AI-Papers/100_Papers_index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "Note\n\n\n Due to the large number of papers included in this series. I will mainly write in Chinese to speed up the writing process. If you have any questions or suggestions, please feel free to contact me. Or if you would like to contribute translations of any of the articles into English or other languages, please let me know! \n\n\n\nBlow is a roadmap of th e 100 papers with code series, categorized by different AI subfields. Each section contains a list of key papers along with brief descriptions. Click on the paper names to access the full articles with code implementations.\n\n\n\n📚 100 Papers Roadmap (click to expand)\n\n\n\n\n\n\n\n\n\n\n\nPaper Name\n\n\nDescription\n\n\nLinks\n\n\nStatus\n\n\n\n\nNatural Language Processing\n\n\n\n\nAttention is All You Need (Transformer)\n\n\nTransformer 是一种基于自注意力机制的深度学习架构，能够并行处理序列，在语言、视觉和多模态任务中表现出色，并且作为 GPT、BERT 等大型语言模型（LLM）的核心基础，推动了当今生成式人工智能的快速发展。在本篇文章中，我们将深入探讨 Transformer 的基本原理，以及关键组件，包括Word Embedding、Position Embedding、Attention、Normalization Layer 和 Feed Forward Layer。并且通过在Ted Talks数据集上的实验，展示Transformer在实际任务中的应用效果。\n\n\n[Blog] [Code]\n\n\n🟢\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nLanguage Models are Unsupervised Multitask Learners (GPT-2)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDeep contextualized word representations (ELMo)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nTransformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Transformer-XL)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Switch Transformer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nREFORMER: THE EFFICIENT TRANSFORMER (ReFormer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nReformer: The Efficient Transformer (Reformer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDiffusion LLM ()\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nLarge Language Diffusion Models (LLaDA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nComputer Vision\n\n\n\n\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin Transformer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nTraining data-efficient image transformers & distillation through attention (DEiT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nEnd-to-End Object Detection with Transformers (DETR)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSegment Anything (SAM)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nViViT: A Video Vision Transformer (ViViT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPhoto-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Super Resolution)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers (SegFormer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nBEiT: BERT Pre-Training of Image Transformers (BEiT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nMulti-Modality\n\n\n\n\nLearning Transferable Visual Models From Natural Language Supervision (CLIP)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (BLIP)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nFlamingo: a Visual Language Model for Few-Shot Learning (Flamingo)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nVisual Instruction Tuning (LLaVA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPerceiver: General Perception with Iterative Attention (Perceiver)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nImageBind: One Embedding Space To Bind Them All (ImageBind)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPaLM-E: An Embodied Multimodal Language Model (PaLM-E)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nRT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control (RT-2)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSpace-Time Attention All You Need for Video Understanding? (TimeSformer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention (Image Caption)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Imagen)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision (ViLT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nGrounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection (Grounding DINO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nNATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS (Tacotron2)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMultimodal Large Diffusion Language Models (MMaDA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nJanus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation (Janus)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\n(Deep) Reinforcement Learning\n\n\n\n\nProximal Policy Optimization Algorithms (PPO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPlaying Atari with Deep Reinforcement Learning (DQN)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPolicy Gradient Methods for Reinforcement Learning (REINFORCE)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nContinuous control with deep reinforcement learning (DDPG)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDecision Transformer: Reinforcement Learning via Sequence Modeling (Decision Transformer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nTrust Region Policy Optimization (TRPO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nTraining language models to follow instructions with human feedback (Instruct-GPT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (GRPO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDeep Reinforcement Learning from Human Preferences (RLHF)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nConcrete Problems in AI Safety ()\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\n(Deep) Generative Model\n\n\n\n\nAuto-Encoding Variational Bayes (VAE)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nNeural Discrete Representation Learning (VQ-VAE)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nHigh-Resolution Image Synthesis with Latent Diffusion Models (Latent Diffusion Model (Stable Diffusion))\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nConditional Image Generation with PixelCNN Decoders (PixelCNN)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nGenerative Pretraining from Pixels (iGPT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nGenerative Adversarial Networks (GAN)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nScalable Diffusion Models with Transformers (DiT)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis (MMDiT (Stable Diffusion 3))\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMovie Gen: A Cast of Media Foundation Models (MovieGen)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nLatte: Latent Diffusion Transformer for Video Generation (Latte)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nZero-Shot Text-to-Image Generation (DALL·E)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nConsistency Models (Consistency Models)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nScore-Based Generative Modeling through Stochastic Differential Equations (Score Matching)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nAdding Conditional Control to Text-to-Image Diffusion Models (Control Net)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nAutoregressive Image Generation without Vector Quantization ()\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nRepresentation Learning\n\n\n\n\nEmerging Properties in Self-Supervised Vision Transformers (DINO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMasked Autoencoders Are Scalable Vision Learners (MAE)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMomentum Contrast for Unsupervised Visual Representation Learning (MoCo)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nA Simple Framework for Contrastive Learning of Visual Representations (SimCLR)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nBootstrap your own latent: A new approach to self-supervised Learning (BYOL)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSelf-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (JEPA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nFixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence ()\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSupervised Contrastive Learning (SupCon)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nMeta Learning\n\n\n\n\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nPrototypical Networks for Few-shot Learning (ProtoNet)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMatching Networks for One Shot Learning (Matching Net)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nLearning to learn by gradient descent by gradient descent (Learned Optimizer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nNeural Network Architecture\n\n\n\n\nMamba: Linear-Time Sequence Modeling with Selective State Spaces (Mamba)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nKAN: Kolmogorov-Arnold Networks (KAN)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nVision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model (Vision-Mamba)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation (U-Net)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDeep Residual Learning for Image Recognition (Residual Network)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nmHC: Manifold-Constrained Hyper-Connections (mHC)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nMachine Learning Systems\n\n\n\n\nZeRO: Memory Optimizations Toward Training Trillion Parameter Models (ZeRO)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Flash Attention)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMixed Precision Training (Mixed Precision Training)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Model Parallelism)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nNative Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention (NSA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nMath Theory & Optimization\n\n\n\n\nNeural Tangent Kernel: Convergence and Generalization in Neural Networks (NTK)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nReconciling modern machine learning practice and the bias-variance trade-off (Double Descent)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nScaling Laws for Neural Language Models (Scaling Law)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nAdam: A Method for Stochastic Optimization (Adam)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nDecoupled Weight Decay Regularization (AdamW)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nSymbolic Discovery of Optimization Algorithms (Lion)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nMuon is Scalable for LLM Training (Muon)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nAgentic\n\n\n\n\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (RAG)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nToolformer: Language Models Can Teach Themselves to Use Tools (ToolFormer)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nReAct: Synergizing Reasoning and Acting in Language Models (ReAct)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nModel Compression\n\n\n\n\nDistilling the Knowledge in a Neural Network (Knowledge Distllation)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (Quantization)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n\nOthers\n\n\n\n\nLoRA: Low-Rank Adaptation of Large Language Models (LoRA)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Wav2Vec)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nEfficient Estimation of Word Representations in Vector Space (Word2Vec)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nData Programming: Creating Large Training Sets, Quickly (Data Programming)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nOvercoming Catastrophic Forgetting in Neural Networks (EWC)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\nExplaining and Harnessing Adversarial Examples (FGSM)\n\n\n\n\n[Read] [Code]\n\n\n🔴\n\n\n\n\n  \n\nAll Completed Papers with Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01: Attention is All You Need (Transformer)\n\n\n\nNLP\n\nArchitecture\n\nTransformer\n\n⭐️⭐️⭐️⭐️⭐️\n\n\n\nTransformer 是一种基于自注意力机制的深度学习架构， 能够并行处理序列，在语言、视觉和多模态任务中表现出色， 并且作为 GPT、BERT 等大型语言模型（LLM）的核心基础， 推动了当今生成式人工智能的快速发展。 在本篇文章中，我们将深入探讨 Transformer 的基本原理， 以及关键组件，包括 Word Embedding、 Position Embedding、 Attention、 Normalization Layer 和 Feed Forward Layer。 并且通过在  Ted Talks  数据集上的实验，展示 Transformer 在实际任务中的应用效果。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)\n\n\n\nComputer Vision\n\nTransformer\n\n\n\nVision Transformer (ViT) 通过将图像切分为 Patch 并直接应用标准 Transformer 架构，实现了图像分类任务。本文介绍了 ViT 的核心组件，包括 Patch Embedding、Position Embedding、[CLS] Token 以及 Transformer 编码器块，探讨了 ViT 相较于传统 CNN 的归纳偏置差异(Inductive Bias)，并展示了 ViT 在大规模数据集上的优异表现。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03: Training data-efficient image transformers & distillation through attention (DeiT)\n\n\n\nComputer Vision\n\nTransformer\n\nKnowledge Distillation\n\n\n\nTraining data-efficient image transformers & distillation through attention（DeiT）提出了一种通过知识蒸馏（distillation token 与 attention-based distillation）显著提升 Vision Transformer 数据效率的方法，使 ViT 能在中小规模数据集上高效训练并达到与 CNN 可比的性能。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)\n\n\n\nComputer Vision\n\nAttention\n\nTransformer\n\n\n\nSwin Transformer 是一种使用层次化结构和滑动窗口自注意力机制的ViT模型，既保留了局部建模的高效性，又通过窗口偏移实现跨区域信息交互，可作为通用视觉骨干网络，适用于图像分类、目标检测和语义分割等多种视觉任务。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05: ViViT: A Video Vision Transformer(ViViT)\n\n\n\nComputer Vision\n\nTransformer\n\n\n\nViViT: A Video Vision Transformer提出将 Vision Transformer 系统性扩展到视频建模，通过时空分解与高效注意力设计直接对视频序列进行建模，在视频分类等任务上取得强性能与良好可扩展性。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06: Learning Transferable Visual Models From Natural Language Supervision (CLIP)\n\n\n\nMulti Modality\n\nRepresentation Learning\n\n\n\n一种通过对齐图像与自然语言文本的对比学习框架，在海量图文对上训练统一表示，从而获得强零样本泛化能力的视觉模型。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n07: Emerging Properties in Self-Supervised Vision Transformers (DINO)\n\n\n\nSelf Supervised Learning\n\nRepresentation Learning\n\n\n\n一种无需标签的自监督学习方法，通过教师–学生自蒸馏训练 Vision Transformer，自发涌现出语义一致的全局表示与清晰的注意力分割能力。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n08: Auto-Encoding Variational Bayes (VAE)\n\n\n\nSelf Supervised Learning\n\nGenerative Model\n\nRepresentation Learning\n\n\n\nVAE（变分自动编码器）是一类结合概率图模型与神经网络的生成模型，它通过引入可参数化的近似后验 \\(q_\\phi(z|x)\\) 来摊销推断成本，并用最大化 ELBO 的方式同时学习数据的潜在表示与生成过程：其中重建项确保模型能从潜变量还原数据，KL 项则将潜空间约束为接近先验的连续结构。借助重参数化技巧，VAE 能在端到端训练中高效地学习一个平滑、可采样的潜空间，从而实现表示学习、插值、生成等功能，是现代深度生成模型的重要基础。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n09: Masked Autoencoders Are Scalable Vision Learners(MAE)\n\n\n\nSelf Supervised Learning\n\nRepresentation Learning\n\nAutoEncoder\n\n\n\n一种通过随机遮挡大比例图像 patch 并重建缺失内容进行自监督学习的方法，使 Vision Transformer 能以更高效率和更好可扩展性学习通用视觉表示。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10: Conditional Image Generation with PixelCNN Decoders (Pixel Gated CNN)\n\n\n\nGenerative Model\n\n\n\n一种基于像素级自回归建模的条件图像生成方法，通过引入门控卷积（Gated CNN）在给定条件（如类别或上下文）下逐像素生成高质量图像。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11: Neural Discrete Representation Learning (VQ_VAE)\n\n\n\nRepresentation Learning\n\nSelf Supervised Learning\n\n\n\n一种通过离散化潜在表示并使用码本进行重构的生成模型，将连续表示转为离散符号，从而学习高质量、可组合的视觉表示并支持高效生成。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15: High-Resolution Image Synthesis with Latent Diffusion Models (Latent Diffusion Model)\n\n\n\nGenerative Model\n\n\n\n一种在低维潜在空间中进行扩散建模的生成方法，在显著降低计算成本的同时，实现高分辨率、高质量的图像生成。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16: Scalable Diffusion Models with Transformers (DiT)\n\n\n\nGenerative Model\n\nDiffusion Model\n\n\n\n一种将 Transformer 架构引入扩散模型的生成方法，通过序列化建模与规模化训练，在大模型与大数据设置下实现更强的生成质量与可扩展性。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is All You Need(Transformer)\n\n\n\nNLP\n\nCV\n\nTransformer\n\nAttention\n\n\n\nIntroduce the Transformer architecture, which relies entirely on self-attention mechanisms for sequence modeling, enabling parallel computation and significantly improving performance on natural language processing tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness(Flash Attention)\n\n\n\nTransformer\n\n\n\nFlashAttention 是一IO-aware的exact Attention 实现：它把 QKᵀ 和 softmax 的计算按块（tiling）搬进片上 SRAM/共享内存，用在线 softmax（维护 running max 与 sum 的 log-sum-exp 归一化）在不保存完整注意力矩阵的情况下完成计算，并且通过Recomputing的技术，从而显著减少 HBM 读写、降低显存占用并加速。\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/100-AI-Papers/10-pixel-gated-cnn/Pixel Gated CNN.html",
    "href": "posts/100-AI-Papers/10-pixel-gated-cnn/Pixel Gated CNN.html",
    "title": "10: Conditional Image Generation with PixelCNN Decoders (Pixel Gated CNN)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/10-pixel-gated-cnn/Pixel Gated CNN.html#experiment",
    "href": "posts/100-AI-Papers/10-pixel-gated-cnn/Pixel Gated CNN.html#experiment",
    "title": "10: Conditional Image Generation with PixelCNN Decoders (Pixel Gated CNN)",
    "section": "1.1 Experiment",
    "text": "1.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "",
    "text": "在阅读本章之前，我们需要先了解一下什么是Transformer(Vaswani et al. 2023)。 如果有不熟悉的同学，欢迎阅读我们 100 Paper with Code 系列的第一篇:01: Attention is all you need (Transformer)",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#patch-embedding",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.1 Patch Embedding",
    "text": "2.1 Patch Embedding\n在Transformer这一篇，我们了解到，它是作用于Sequence Modeling的，很显然，Image不是 Sequence的, 它有长\\(H\\)和宽\\(W\\)。很直观的第一种想法就是，将图片直接展开，从二维 \\((3, H, W)\\) 展开成一维的 \\((3 \\times H \\times W)\\). 这样我们就得到的图片的 Sequence Model。如下图 Figure 2 所示\n\n\n\n\n\n\nFigure 2: Illustration of flattening an image into a sequence (Image Source: iGPT)\n\n\n\n这种方法有一种明显的问题就是:Sequence的长度太长。 举个例子，对于 \\(3\\times 256 \\times 256\\) 的图片，我们有 \\(256 \\times 256 = 65,336\\) 个tokens，通过这种方法，所需要的训练时长很长 (在Transformer 这一节，我们了解过，Attention的时间复杂度是 \\(\\mathcal{O}(n^{2}d)\\))。 除了这个问题，另一个问题是:它没有用到图片的特性: 相邻的pixel 之间，是有很高的correlation。\n所以我们很自然的想到:如果把相邻的pixels和在一组，组成一个patch，这样不就既减少了tokens的数量，又用到了pixel之间的correlation。这就是Vision Transformer的Patch Embedding组件。\n\nThe standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches\\(x \\in \\mathbb{R}^{N \\times (P^{2} \\times C)}\\), where (\\(H, W\\)) is the resolution of the original image, \\(C\\) is the number of channels, (\\(P, P\\)) is the resolution of each image patch, and \\(N = HW/P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\n有了Patch之后，我们就将图片从 \\((H, W, C)\\) 变成了 \\((N, P, P, C)\\)， 其中 \\(N = \\frac{H \\times W}{P^{2}}\\)。 我们来看看代码怎么实现:\n# Load Image and resize it to certain size\nimage_path = IMAGE_PATH\nimg_bgr = cv2.imread(image_path)\nimg_resized = cv2.resize(img_bgr, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\nimg = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB) \n\n# Patchify \npatches = einops.rearrange( \n     img, \"(h ph) (w pw) c -&gt; (h w) ph pw c\", ph=PATCH_SIZE, pw=PATCH_SIZE \n) \n通过这个Patchify之后，我们将图片分成了 \\(\\left( \\frac{H}{P} \\times \\frac{W}{P}, P, P, C \\right)\\) 个Patches\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Illustration of Patchify\n\n\n\n接下来，我们将每个Patch 展开成一个向量，变成 \\((N, P^{2} \\times C)\\)，然后传入一个Linear Layer，将其映射到一个隐藏空间，变成 \\((N, d_{model})\\)。 这样，我们就得到了Transformer可以接受的输入。\nflat_patch = einops.rearrange( patches, \"n ph pw c -&gt; n (ph pw c)\") \n\nmlp = nn.Linear(PATCH_SIZE * PATCH_SIZE * 3, d_model)\npatch_embedding = mlp(flat_patch)\n通过这种方法，我们就得到了Transformer可以接受的任意长度的输入。不过在实际操作中，我们并不会用以上的方式，而是用一个卷积层来实现这个Patch Embedding的过程。原因有二:\n\n效率更高: 将Patchify + Flatten + Linear 合成一个卷积层，可以减少中间的内存读写，提高计算效率。\n代码更简洁: 用一个卷积层就可以实现所有的功能，代码量更少，更易读懂。\n\n如果我们用一个 卷积层，参数设置为:\n\nkernel_size = PATCH_SIZE （卷积核覆盖一个 patch）\nstride = PATCH_SIZE （不重叠地移动，相当于切 patch）\nin_channels = 3（RGB）\nout_channels = d_model\n\n那么卷积会:\n\n把输入图片分成 PATCH_SIZE x PATCH_SIZE 的不重叠块（因为 stride = kernel_size）。\n对每个 patch 做一次线性映射（因为卷积本质上就是对局部区域做加权求和，相当于 Linear）。\n输出的 shape 自动就是 (batch, num_patches, d_model)。\n\n这正好等价于 切 patch + flatten + Linear 的组合.\n代码如下:\nclass PatchEmbedder(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.config = config\n        self.num_patches_per_side = config.image_size // config.patch_size\n        self.num_patches = self.num_patches_per_side**2\n\n        self.proj = nn.Conv2d( \n            in_channels=config.num_channels, \n            out_channels=config.d_model, \n            kernel_size=config.patch_size, \n            stride=config.patch_size, \n        ) \n\n    def forward(self, x: torch.Tensor):\n        # x: (B, C, H, W)\n        x = self.proj(x)  # (B, D, H/P, W/P)\n        x = x.flatten(2)  # (B, D, N)\n        x = x.transpose(1, 2)  # (B, N, D)\n        return x\n用卷积的好处，除了可以更高效的实现Patch Embedding，代码更加简洁之外，我们还可以通过改变 stride 来使一些Patch Overlapping，获得一个多尺度的结构。（尽管这个在ViT中没有提到，但是我觉得我们可以利用这一点）\n\nTAKEAWAY:  ViT 的核心改动只有两步:patchify（切块）+ embedding（线性投影），其余几乎就是原封不动的 Transformer Encoder\n\n可以补一句点明:“ViT 原版用 non-overlap patch（更纯），后续很多工作会用 overlap/conv stem 来补 inductive bias”，把读者往后续论文自然引过去。\n我们计算一下，通过这种方法，可以减少多少Tokens的数量:\n\n图像大小 \\(224 \\times 224\\)，\\(P=16\\) → \\(N= \\frac{224\\times 224}{16 ^{ 2}}=196\\)\n\\(P=8\\) → \\(N= \\frac{224\\times 224}{8 ^{ 2}}=784\\)（注意力矩阵变成 16 倍）\n所以 patch size / token 数直接决定训练的效率",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#position-encoding",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.2 Position Encoding",
    "text": "2.2 Position Encoding\n将图片转化为 Transformer 的输入之后，接下来Transformer中的另一个组件就是传入 Position Information。 我们知道在Transformer 中，他们用的是 Sine-cosine position embedding，在那篇文章中，我们也提到了，还存在其他不同的Position Encoding的办法，ViT用的就是另一种办法，Learned Position Embedding。Learned Position Embedding的方法很简单，也很好理解，对于每一个位置，我们给他一个index，将这个index传入一个 Embedding Matrix， 我们就得到一个Position Embedding。不过与Token Embedding不同的是，我们会用到所有的Position，也整个matrix， 所以我们不用定index，直接定义整个Embedding，然后将它传入Transformer中。\nclass PosEmbedder(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.position_embeddings = nn.Parameter( \n            torch.randn(1, (config.image_size // config.patch_size) ** 2 + 1, config.d_model) \n        ) # +1 for cls token \n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, config.d_model))\n\n    def forward(self, x: torch.Tensor):\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, 1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        x = x + self.position_embeddings\n        return x\n\n为什么ViT要用Learned Position Embedding呢？在ViT这篇文章中，他们尝试过不同的Position Embedding，比如:\n\nNo Positional Information\n1-dimensional Positional Embedding\n2-dimensional Positional Embedding\nRelative Positional Embedding\n\n发现，除了No Positional Information之外，其余3种在Image Classification中的表现，都是差不多的。\n\n\n\n\n\n\nFigure 4: Compare different Position Encoding methods in Image Classification Task\n\n\n\n论文中表示，可能是因为所需要的Position的信息较小，对于不同种类的Position Embedding的方法，学习这个Position Information的能力，都是差不多的。\n\nWe speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., \\(14 \\times 14\\) as opposed to \\(224 \\times 224\\), and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.18 \n\n不过，尽管Position的方法不重要，但是不同的训练参数，还是会影响到学习到的Position Information, 下图所示:\n\n\n\n\n\n\nFigure 5: Illustration of different Position Information learned under different hyper-parameters.\n\n\n\n\n\n2.2.1 Position Interpolation\n当我们有了一个Pre-Training的模型，我们想用它Fine-Tuning到一个不同图片大小的数据库，我们改怎么做呢?\n第一个方法当然是，Resize 我们的图片，到ViT Pre-training的图片大小，但是，这个能导致较大的图片，失去很多细节。如果我们想保持图片的大小不变，同时让模型训练，我们就需要Extend Position Encoding，因为当Patch Size不变，图片大小变了的话，产生的Number of Patches 也是会改变的，我们需要做的是: 找到一种方法，增大或者减小Position的数量。 这就是所谓的Position Interpolation。\n\nThe Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.4 \n\n我们来看看代码是怎么实现Position Interpolation的:\ndef interpolate_pos_encoding(self, x, w, h):\n    npatch = x.shape[1] - 1\n    N = self.pos_embed.shape[1] - 1\n    if npatch == N and w == h:\n        return self.pos_embed\n    class_pos_embed = self.pos_embed[:, 0]\n    patch_pos_embed = self.pos_embed[:, 1:]\n    dim = x.shape[-1]\n    w0 = w // self.patch_embed.patch_size\n    h0 = h // self.patch_embed.patch_size\n    \n    patch_pos_embed = F.interpolate(\n        patch_pos_embed.reshape(\n            1, \n            int(math.sqrt(N)), \n            int(math.sqrt(N)), \n            dim\n        ).permute(0, 3, 1, 2),\n        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n        mode='bicubic',\n    )\n    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\nTAKEAWAY:  2D interpolation of the pre-trained position embeddings\n\nViT 在预训练时，通常用固定输入分辨率（比如 224×224） → 生成固定数量的 patch（比如 16×16 patch → 196 个 patch）。\n但在 fine-tuning 时，输入图片可能大小不一样，比如 384×384，这时 patch 数量就变了。\n这会导致原本的 位置编码 (position embeddings) 和新的 patch 数量对不上。\n解决办法:对预训练好的位置编码做 二维插值 (2D interpolation)，根据 patch 在原图中的空间位置，把位置编码拉伸/缩放到新的分辨率。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#cls-tokens-mlp-head",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.3 [CLS] Tokens & MLP Head",
    "text": "2.3 [CLS] Tokens & MLP Head\n在 Transformer 这一节，我们了解到:每输入一个token，Transformer会输出对应的token。这就是说，对于每个patch，Transformer会输出对应的Tokens，那么，我们应该选择哪一个token作为我们图片的表示呢。 BERT (Devlin et al. 2019)， 用了一个 [CLS], 来表示一个句子。同理，我们也可以添加一个 [CLS] token, 来表示一张图片。同时，对于 [CLS] token, 我们也要在给他一个表示位置的信息。这就是为什么在Position Encoding上，我们有 (config.image_size // config.patch_size) ** 2 + 1, 位置信息，其中 +1 就是 [CLS] 的位置信息。\n总结一下 [CLS] token 的作用就是用来聚合所有的Patch的消息，然后用来Image 的Representation。\n我们想一下，除了加一个 [CLS] token，之外，我们还有其他办法来表示图片吗。有一种很自然的方法就是，将所有的patch的消息收集起来，然后去一个平均值来表示这个图片。类似于传统的ConvNet(e.g. ResNet) 我们可以通过 AvgPooling 来实现。 不过论文中提到， 对于两种不同的Image Representation，需要有不同的Learning Rate 来训练这个网络。 通过下图，我们看到，不用的收集信息的方法，需要不同的learning rate \n\n结构上二者都可行，但需要不同 LR / recipe。 实践里很多实现默认用 CLS（与 BERT 对齐、下游更统一），也有用 GAP 的变体比如我们接下来要学的 Swin Transformer (Liu et al. 2021)\n\n有了Image Represent之后，我们只需要将这个传入一个简单的MLP，我们就可以得到一个Classifier。MLP的输入是hidden dim，输出则是我们Number of Classes。不同的Index 表示不同的Classses。\n\nAn initial attempt at using only image-patch embeddings, globally average-pooling (GAP) them, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly. However, we found that this is neither due to the extra token, nor to the GAP operation. Instead, the difference in performance is fully explained by the requirement for a different learning-rate,  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.17 \n\n\nBoth during pre-training and fine-tuning, a classification head is attached to \\(\\mathrm{z}_{L}^{0}\\). The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.3 \n\nclass MLPHead(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.fc1 = nn.Linear(config.d_model, config.d_model)\n        self.fc2 = nn.Linear(config.d_model, config.num_classes)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        cls = x[:, 0, :]\n        cls = self.dropout(F.relu(self.fc1(cls)))\n        cls = self.fc2(cls)\n\n        return cls",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#transformer-encoder-block",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.4 Transformer Encoder Block",
    "text": "2.4 Transformer Encoder Block\n至此，我们已经讲完了ViT, 与Transformer的主要不同之处。接下来，就是Transformer的Encoder。 \n这部分，和Transformer原本的Encoder很类似，只不过有几处不同:\n\nPre-Norm: 在ViT同，输入先进行一个LayerNorm，然后在传入MHA或者MLP中，反观在Transformer原本的Encoder中，我们是先将MHA或者MLP的输出与输入加在一起，之后再进行一个Normalization。这叫做Post-Norm\nMLP的实现:在Transformer Encoder中，用的是 ReLU, 而在ViT中，用的是 GELU\n\n除此之外，其他部分都是一样的。一下是ViT Encoder的实现:\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.mha = MHA(config)\n        self.ffn = FFN(config)\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n\n    def forward(self, x: torch.Tensor):\n        attn, _ = self.mha(self.norm1(x))\n        x = x + attn\n        x = x + self.ffn(self.norm2(x))\n\n        return x\n\n2.4.1 Multi-Heads Attention\n还有一个就是Attention模块，Attention模块与Transformer中的是一模一样，在这里就不过多的赘述了。\nclass MHA(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.num_heads = config.num_heads\n        self.d_model = config.d_model\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_k = config.d_model // config.num_heads\n\n        self.qkv_linear = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_linear = nn.Linear(config.d_model, config.d_model)\n\n        self.attention_dropout = nn.Dropout(config.attention_dropout_rate)\n\n    def forward(self, x: torch.Tensor):\n        B, N, C = x.shape  # Batch size, Number of tokens, Embedding dimension\n\n        q, k, v = (\n            self.qkv_linear(x).reshape(B, N, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4).unbind(0)\n        )\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(\n            torch.tensor(self.d_k, dtype=torch.float32)\n        )  # (B, num_heads, N, N)\n        attn_weight = torch.softmax(scores, dim=-1)  # (B, num_heads, N, N)\n        attn = self.attention_dropout(attn_weight)\n\n        context = torch.matmul(attn, v)  # (B, num_heads, N, d_k)\n        context = context.transpose(1, 2).reshape(B, N, C)  # (B, N, d_model)\n\n        out = self.out_linear(context)  # (B, N, d_model)\n\n        return out, attn_weight",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#cnn-vs.-vit-inductive-bias",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.5 CNN vs. ViT: Inductive bias",
    "text": "2.5 CNN vs. ViT: Inductive bias\n至此，我们已经介绍完了Vision Transformer，我们来从Inductive Bias的方面，看看 CNN 和 ViT 有什么不同\n\n\n\n\n\n\n什么是Inductive Bias\n\n\n\n在深度学习里，Inductive Bias（归纳偏置）是指模型在学习之前，因结构或设计而自带的假设或先验 ，比如 Convolution Layer, 它就是假设相邻的pixel之间，是有一定联系的，因此可以用一个Kernel来将学习这些关系。\n\n\n对于图像来说，常见的先验就是:\n\n局部像素是相关的（locality）\n相邻区域的模式有规律（2D neighborhood）\n物体无论出现在图像哪里，识别方式应该一样（Translation Equivariance）\n\n那么，CNN 的结构怎么体现这些偏置？ 1. 局部性 (Locality): - 卷积核（例如 3×3）只和局部像素打交道，而不是全图。 - 这意味着模型“相信”图像的重要特征来自局部邻域，而不是遥远区域。 2. 二维邻域结构 (2D structure): - 卷积操作是沿着 图像的二维网格进行的，天然利用了图像的行列结构。 - 这和文本（序列 1D）不一样，CNN 明确知道输入是 2D 排列的。 3. 平移等变性 (Translation equivariance): - 卷积核的参数在整张图共享。 - 所以猫在左上角还是右下角，卷积核都能检测到“猫耳朵”。 - 这让 CNN 自动具有“识别位置无关”的能力。\n这些性质不是模型通过训练学出来的，而是因为卷积操作本身的数学结构就带来的（也是我们人为设计的）:\n\nkernel 的局部连接 → 局部性\nkernel 滑动覆盖全图 → 平移等变性\n操作在二维空间定义 → 邻域结构\n\n所以，哪怕我们不给 CNN 喂太多数据，它也会利用这些偏置去学习特征。\n而对于 ViT 来说，其归纳偏置非常弱，几乎完全依赖数据和训练来学习，不过它也有利用了一些图片的Inductive Bias:\n\nPatch 切分 (Patchification) • ViT 唯一的“图像先验”之一就是把输入图片切成 patch。 • 这一操作隐含了:图像是一个二维结构，可以被分块处理。\n位置编码 (Positional Embeddings) • Transformer 本身只处理序列，没有空间结构的概念。 • ViT 通过加位置编码告诉模型 patch 在图像中的相对位置。 • 在输入分辨率变化时，会做 二维插值 (2D interpolation) 来适配，这也是一种人工引入的 2D 先验。\n其他部分 • 除了以上两点，ViT 的注意力机制是 全局的 (global)，没有局部性约束。 • 没有像 CNN 那样内置的平移等变性或局部邻域结构。\n\n这样就是为什么ViT需要更多数据和计算才能学到同样的空间归纳规律。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#vit-model-variants",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.6 ViT Model Variants",
    "text": "2.6 ViT Model Variants\nViT 有3种不同的基本变形， 如下图所示\n\nViT的名字通常表示为: ViT-L/16: 意思是，ViT-Large，然后用的16 Patch Size。 需要注意的是，Patch Size越大，我们得到的tokens就越少，也就是需要更少的训练时实现, 但通常需要更大的图片来训练。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#experiment",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#experiment",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "2.7 Experiment",
    "text": "2.7 Experiment\n我们先来看看原论文是如何训练的: - 预训练（所有模型，包括 ResNet）:作者统一使用 Adam（\\(\\beta_{1} = 0.9, \\beta_{2}=0.999\\)），batch size = 4096，并使用较大的 weight decay = 0.1。作者指出这对所有模型的迁移表现都有帮助 - 学习率策略:采用线性 warmup + 线性 decay（细节见 Appendix B.1）。 - 微调（fine-tuning）:对所有模型统一改用带 momentum 的 SGD，batch size 512. - 权重平均:同时使用 Polyak averaging（指数滑动平均，系数 0.9999）以进一步提升效果。\n接下来，我们来训练我们定义的ViT。具体的代码可以在 这里 查看。 ### Dataset 我们用CIFAT-10的训练集来训练ViT。\n\n\n\n\n\n\nNote\n\n\n\n我们之前提到了ViT要在大规模的数据集上才可以发挥它的能力，由于资源有限，我们只在这展示ViT的训练流程，在了解了这个训练流程之后，很容易拓展到其他的大数据集。\n\n\nIMG_MEAN = [0.4914, 0.4822, 0.4465]\nIMG_STD = [0.2470, 0.2435, 0.2616]\nIMG_SIZE = 32\n\n\ndata_transform = transforms.Compose(\n    [\n        transforms.Resize(\n            (IMG_SIZE, IMG_SIZE),\n            interpolation=transforms.InterpolationMode.BILINEAR,\n        ),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(IMG_MEAN, IMG_STD),\n    ]\n)\n\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=train_config.data_dir, download=True, train=True, transform=data_transform\n)\nif train_config.debug:\n    train_dataset = torch.utils.data.Subset(train_dataset, range(1000))\n\n\ndataloader = DataLoader(\n    train_dataset,\n    batch_size=train_config.batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True if train_config.device.type == \"cuda\" else False,\n)\n\n2.7.1 Optimizer & Loss Function\n论文中应用了 Adam Optimizer，我们在此也用Adam。因为我们训练的是Image Classification Task，所以损失函数是是Cross Entropy Loss:\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=train_config.lr,\n    betas=train_config.betas,\n    weight_decay=train_config.weight_decay,\n)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, train_config.num_epochs, eta_min=train_config.min_lr\n)\n\n\n2.7.2 Result\n\n\n我们来看看，这个Toy ViT 究竟训练的怎么样:\nN_ROWS = 4\nN_COLS = 8\nN_IMGS = N_ROWS * N_COLS\ntest_dataset = torchvision.datasets.CIFAR10(\n    root=train_config.data_dir,\n    download=True,\n    train=False,\n)\nIDX_TO_CLASS = {v: k for k, v in test_dataset.class_to_idx.items()}\n\norg_imgs, true_labels = test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]\ntransformed_imgs = torch.stack([data_transform(Image.fromarray(img)) for img in org_imgs])\npred_labels = model(tensor_to_device(transformed_imgs, device=train_config.device)).argmax(dim=1).cpu()\n\n\nfig, axes = plt.subplots(N_ROWS, N_COLS, figsize=(N_COLS * 2, N_ROWS * 2))\nfor i in range(N_ROWS):\n    for j in range(N_COLS):\n        idx = i * N_COLS + j\n        axes[i, j].imshow(org_imgs[idx])\n        axes[i, j].axis(\"off\")\n        if true_labels[idx] == pred_labels[idx].item():\n            axes[i, j].set_title(\n                f\"{IDX_TO_CLASS[true_labels[idx]]} (Pred: {IDX_TO_CLASS[pred_labels[idx].item()]})\",\n                fontsize=10,\n                color=\"green\",\n            )\n        else:\n            axes[i, j].set_title(\n                f\"{IDX_TO_CLASS[true_labels[idx]]} (Pred: {IDX_TO_CLASS[pred_labels[idx].item()]})\",\n                fontsize=10,\n                color=\"red\",\n            )\nplt.tight_layout()\nplt.show()\n\n我们也可以看看Attention Map的\nN_ROWS = 4\nN_COLS = 8\nN_IMGS = N_ROWS * N_COLS\ntest_dataset = torchvision.datasets.CIFAR10(\n    root=train_config.data_dir,\n    download=True,\n    train=False,\n)\nIDX_TO_CLASS = {v: k for k, v in test_dataset.class_to_idx.items()}\n\norg_imgs, true_labels = test_dataset.data[:N_IMGS], test_dataset.targets[:N_IMGS]\ntransformed_imgs = torch.stack([data_transform(Image.fromarray(img)) for img in org_imgs]).to(\n    train_config.device\n)\n\n# pred_labels = model(tensor_to_device(transformed_imgs, device=train_config.device)).argmax(dim=1).cpu()\nTARGET_LAYER = 5  # 0-based index\nHEAD = 2\nGAMMA = 0.7\nFLOOR = 0.15\n\n\ndef to_vit_attention_vis(img_uint8, attn_map, gamma=0.7, floor=0.15):\n    \"\"\"\n    gamma:     &gt;0, smaller -&gt; sharper spotlight\n    floor:     how visible the dark region is (0 = pure black background)\n    \"\"\"\n    img = img_uint8.astype(np.float32) / 255.0\n\n    # normalize attention to [0,1]\n    a = attn_map.astype(np.float32)\n    a = a - a.min()\n    a = a / (a.max() + 1e-6)\n\n    # make it more \"spotlight-like\"\n    a = a**gamma  # sharpen\n    a = floor + (1 - floor) * a  # keep some visibility outside\n    a = np.clip(a, 0, 1)\n\n    return img * a[..., None]  # darken outside attention\n\n\nmodel.eval()\nx = model.backbone.patch_embedder(transformed_imgs)\nx = model.backbone.pos_embedder(x)\nfor i in range(TARGET_LAYER):\n    x = model.backbone.encoder_layers[i](x)\n\n_, attn_weights = model.backbone.encoder_layers[TARGET_LAYER].mha(\n    model.backbone.encoder_layers[TARGET_LAYER].norm1(x)\n)\nif HEAD &gt;= 0:\n    attn_weights = attn_weights[:, HEAD, 0, 1:]  # (B, N)\nelse:\n    attn_weights = attn_weights.mean(dim=1)[:, 0, 1:]  # (B, N)\n\n# Reshape attention weights to (B, H, W)\nnum_patches_per_side = model_config.image_size // model_config.patch_size\nattn_maps = attn_weights.reshape(-1, num_patches_per_side, num_patches_per_side)  # (B, H, W)\n\n# Upsample attention maps to image size\nattn_maps_upsampled = F.interpolate(\n    attn_maps.unsqueeze(1),\n    size=(model_config.image_size, model_config.image_size),\n    mode=\"bilinear\",\n    align_corners=False,\n).squeeze(1)  # (B, H, W)\n\n# Move attention map to CPU for visualization\nattn_maps_upsampled = attn_maps_upsampled.cpu().detach().numpy()\n\n# Visualize Overlaid Attention Maps\nfig, axes = plt.subplots(N_ROWS, N_COLS, figsize=(N_COLS * 2, N_ROWS * 2))\nfor i in range(N_ROWS):\n    for j in range(N_COLS):\n        idx = i * N_COLS + j\n        vis = to_vit_attention_vis(\n            org_imgs[idx],\n            attn_maps_upsampled[idx],\n            gamma=GAMMA,\n            floor=FLOOR,\n        )\n\n        axes[i, j].imshow(vis)\n        axes[i, j].axis(\"off\")\n        if true_labels[idx] == pred_labels[idx].item():\n            axes[i, j].set_title(\n                f\"{IDX_TO_CLASS[true_labels[idx]]} (Pred: {IDX_TO_CLASS[pred_labels[idx].item()]})\",\n                fontsize=10,\n                color=\"green\",\n            )\n        else:\n            axes[i, j].set_title(\n                f\"{IDX_TO_CLASS[true_labels[idx]]} (Pred: {IDX_TO_CLASS[pred_labels[idx].item()]})\",\n                fontsize=10,\n                color=\"red\",\n            )\nplt.tight_layout()\nplt.suptitle(\n    f\"Attention Maps Over Images On Layer {TARGET_LAYER}  {'Head ' + str(HEAD) if HEAD &gt;= 0 else 'Avg Head'}\",\n    y=1.02,\n)\nplt.show()\n\n\n\n2.7.3 Training Recipe\n接下来我提供几个可能的提升Accuracy的方法（由于时间现实，暂时没有能实现，有兴趣的读者欢迎自行尝试）:\n\n优化器:AdamW（而非 Adam）+ weight decay\n学习率策略:warmup + cosine，batch size 对 LR 的线性缩放规则\n增强:RandAugment / Mixup / CutMix\n正则:DropPath（stochastic depth）、Label smoothing\n\n\n\n\n\n\n\nViT on small data（ImageNet-1k级别）常用 recipe\n\n\n\n\n\n\nAdamW + cosine + warmup + strong aug（RA/Mixup/CutMix）+ DropPath + label smoothing\n\n\n2.7.4 Training Summary\n从结果来看，这个ViT表现的并不是很好，甚至不如简单的Convolution Layer。不过这种结果是在我们预料之中的，因为我们的数据量太少了，ViT还不能从数据中学到有效的信息。 不过出乎我意料的是，Attention Map几乎是平均的， 我们期待的是，类似于论文中的Attention Map。\n\n其中一个解释就是，我们训练的图片太小了，\\(32 \\times 32\\)，导致每个Token收集到的信息很平均，这就导致了Attention Map看起来在每个地方都是一样。\n解决办法就是:\n\n用Grad-CAM(Selvaraju et al. 2020) 或者 是 Attention RolloutAbnar and Zuidema (2020)\n提高图片的Resolution，比如用ImageNet来训练\n\n在这里就不具体展开了，有兴趣的同学自行查看。\n从实验结果来看，这个 toy ViT 的表现确实不算理想，甚至不如一个简单的卷积网络。这其实在预期之内:ViT 的归纳偏置更弱（缺少卷积的局部性与平移等变性），在数据量较小、训练 recipe 不够强的情况下更容易欠拟合或泛化不足，因此很难在 CIFAR-10 这类小规模数据上占到便宜。\n比较“反直觉”的是，我们可视化得到的 Attention Map 几乎接近均匀分布，而不是像原论文那样呈现出更清晰的语义聚焦（例如对物体区域的注意力更强）:\n\n一种合理的解释是:我们的输入分辨率只有 (\\(32\\times32\\))，在常见 patch 设置下 token 数量非常有限（例如 (\\(P=4\\)) 时也只有 (\\(8\\times8=64\\)) 个 patch）。在这种低分辨率、低 token 数的设定里，每个 token 覆盖的区域相对“粗”，并且早期训练阶段模型往往更倾向于学习全局平均的相关性来最小化损失，导致注意力权重看起来更平均。另一个注意的点就是:单层注意力权重本身未必等价于“可解释性”，即使模型在做出正确决策，也可能出现注意力图不够尖锐的现象。\n如果希望得到更有信息量的可解释结果，通常有两条更稳妥的路径:\n\n使用更可靠的可解释方法，例如 Grad-CAM (Selvaraju et al. 2020)，或者结合多层注意力的 Attention Rollout (Abnar and Zuidema 2020)，而不是只观察某一层/某一头的 attention。\n提高输入分辨率与训练规模（例如在 ImageNet 或更大数据上训练/预训练后再迁移），让模型有机会学习到更细粒度的空间结构与更稳定的语义对齐。\n\n这里就不展开实现细节了，有兴趣的同学可以根据上述论文进一步尝试与对比。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#self-supervised-pre-training",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#self-supervised-pre-training",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "3.1 Self-Supervised Pre-Training",
    "text": "3.1 Self-Supervised Pre-Training\n除了做 Image Classification，ViT 团队也尝试了自监督预训练（Self-Supervised Pre-Training）。他们采用了一种非常“类似于BERT(Devlin et al. 2019)”的思路:Masked Patch Prediction——先把图像切成 patch tokens，然后随机“腐蚀（corrupt）”一部分 token，让模型去预测被腐蚀部分的内容。\n\n核心是把 ViT 当成“视觉版 BERT”:随机遮住（或替换）一部分 patch token，再预测被遮住 patch 的目标（这里用离散颜色作为预测标签）。\n\n\n他们对 50% 的 patch embedding 做 corruption，并采用与 BERT 类似的 80/10/10 策略: - 80%:用一个可学习的 [mask] embedding 替换 - 10%:替换成另一块随机 patch 的 embedding - 10%:保持不变（但仍然作为预测目标）\n这种设计的直觉是:既要让模型学会“根据上下文补全缺失信息”，又要避免模型过度依赖某一种固定的 mask 模式。\n他们最终选择了一个非常轻量但有效的预测目标:\n对每个被腐蚀 patch，预测其 3-bit mean color（一共 \\(2^9 = 512\\) 种颜色，也就是一个 512-way 分类问题）。\n同时他们也对比过几种目标设定: 1) 只预测一个 mean 3-bit color（512 分类，1 个预测）\n2) 把 16×16 patch 下采样成 4×4，再对每个小格预测 3-bit color（512 分类，16 个并行预测）\n3) 直接对完整 patch 做像素级 L2 回归（RGB 通道上的密集回归）\n结果比较有意思:三种方式都能带来不错效果，但 像素 L2 回归略差.\n\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%). This setup is very similar to the one used for language by BERT. Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations  An Image is Worth 16x16 Words- Transformers for Image Recognition at Scale, p.14 \n\n后续的工作也说明了这种方法的可行性，比如SiMIM (Xie et al. 2022)",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-1",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-1",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.1 Question 1",
    "text": "5.1 Question 1\n\nQuestion 1: 为什么 Vision Transformer 需要大规模预训练数据？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n因为 ViT 缺乏卷积神经网络中的归纳偏置（inductive bias），例如局部性（locality）和平移不变性（translation equivariance），这些能力需要通过大量数据来学习。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-2",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-2",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.2 Question 2",
    "text": "5.2 Question 2\n\nQuestion 2:Eq.(1) 里为什么要加 [CLS] token，它和全局平均池化有什么区别？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n[CLS] token 给模型一个“专门聚合信息的槽位”，通过注意力主动从所有 patch 拉取信息；而 GAP 是被动平均。论文在附录对比过两者表现接近，但学习率等配方可能需要不同调整。arXiv+1",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-3",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-3",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.3 Question 3",
    "text": "5.3 Question 3\n\nQuestion 3:ViT 用 1D position embedding 不会丢掉 2D 结构吗？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n丢掉了“显式 2D 归纳偏置”，但作者发现更复杂的 2D-aware 位置编码并没有带来显著收益；ViT 依靠数据与训练从头学习空间关系。真正需要 2D 的地方主要在分辨率迁移时的位置编码插值。arXiv+1",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-4",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-4",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.4 Question 4",
    "text": "5.4 Question 4\n\nQuestion 4:为什么论文强调“规模训练胜过归纳偏置”？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n论文实验显示:小数据预训练时强 CNN（如 BiT ResNet）更稳；随着预训练数据从 ImageNet → ImageNet-21k → JFT-300M 增大，ViT 大模型的迁移性能显著提升并反超 CNN，说明对 ViT 而言数据规模是关键瓶颈。UofT Computer Science+1",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-5",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-5",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.5 Question 5",
    "text": "5.5 Question 5\n\nQuestion 5:patch size 选 16 还是 32 的主要权衡是什么？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\npatch 越小（如 16）→ token 数 NNN 越大 → 注意力更贵但细粒度更强；patch 越大（如 32）→ 更省算力但可能损失细节。论文也明确指出序列长度与 P2P^2P2 成反比，因此小 patch 更昂贵。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-6",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-6",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.6 Question 6",
    "text": "5.6 Question 6\n\nQuestion 6:为什么 ViT 在小数据集上通常不如 CNN？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n因为 CNN 通过卷积和权重共享内置了强先验，而 ViT 需要从数据中学习这些先验，在小数据条件下不够高效。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-7",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-7",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.7 Question 7",
    "text": "5.7 Question 7\n\nQuestion 7:分辨率微调时为什么要对位置编码做 2D 插值？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n因为 token 网格大小变了（NNN 变了），预训练的 EposE_{pos}Epos​ 不能直接对齐新位置；2D 插值让位置编码在空间上“平滑伸缩”，从而复用预训练知识。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-8",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#question-8",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "5.8 Question 8",
    "text": "5.8 Question 8\n\nQuestion 8:为什么 ViT 需要把图像切成 patch，而不是直接把每个像素当 token？\n\n\n\n\n\n\n\nanswer\n\n\n\n\n\n像素级 token 会让序列长度变成 \\(H \\times W\\)，自注意力复杂度 \\(\\mathcal{O}(n^{2})\\) 直接爆炸；patchify 把 NNN 降到 \\(\\frac{HW}{P^2}\\)​，让标准全局注意力在可接受的算力下运行，同时保留端到端学习空间结构的能力。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#减少tokens的技巧",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#减少tokens的技巧",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "6.1 减少Tokens的技巧",
    "text": "6.1 减少Tokens的技巧\n\n6.1.1 Patch Merge\n类似于 Swin Transformer(Liu et al. 2021) 的做法:在不同层将相邻 patch 合并（例如 2×2 → 1），减少 token 数，使模型层级化。\n\n\n6.1.2 Pixel Shuffle",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#vision-language-model",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "6.2 Vision Language Model",
    "text": "6.2 Vision Language Model\n我们以及学习了ViT for computer Vision， Transformer for NLP， 接下来有什么办法让这两种模型结合起来呢？ CLIP (Radford et al. 2021): 将 ViT 融合到 vision-language 预训练中。我们之后会学习这篇文章。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#video-transformer",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#video-transformer",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "6.3 Video Transformer",
    "text": "6.3 Video Transformer\n在学习了如何将Transformer应用到Image中，我们可以更近一步，看看如何将Transformer应用到Video 模态中。ViViT(Arnab et al. 2021) 的提出，就是将Transformer应用到Video。中，我们之后会学习到这一篇。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#native-resolution",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#native-resolution",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "6.4 Native Resolution",
    "text": "6.4 Native Resolution\n在ViT 中，我们需要将图片Resize到相同大小的图片，那我们就想不Resize，能不能直接训“不同大小/不同长宽比”的图？ NaViT (Dehghani et al. 2023)提出每张图在“原始分辨率/原始长宽比”下切成 patch token 序列，然后把多张图的 token 序列“打包（packing）”到同一条固定长度序列里训练，用 attention mask 保证不同图片之间互不“串门”。\n\n\nPatch n’ Pack:把多张不同分辨率图片的 patch token 序列拼成一个定长 packed sequence，以减少 padding/resize 浪费\n\nAttention Mask:使用 block-diagonal mask，让同一张图的 token 才能互相 attention，不同图之间完全隔离\n位置编码:采用 factorized（x/y 分解）位置编码，并可用 fractional 坐标提升对未见分辨率的泛化\n\n\n\n\n\n\n\n# Summary 恭喜你看到了这里，ViT的论文架构思想很简单，但就是这种大道至简的方式，才让它这个工作变得出彩。尽管如此，细看ViT的论文，还是有很多可学习的地方。接下来，我们来回顾一下。\n\n\n在本章中，从已熟悉 Transformer 的前提出发，围绕 “如何将 Transformer 应用于计算机视觉” 这一核心问题，系统梳理了 Vision Transformer（ViT）的整体设计与实现思路。文章首先介绍了 ViT 的基本架构，说明其本质是将二维图像通过 Patchify 转化为一维序列，并结合 Patch Embedding、Position Embedding 与 [CLS] token，使图像能够被标准的 Transformer Encoder 处理。随后介绍了 Patch Embedding 的动机与实现:从像素级直接展开所带来的序列过长与计算复杂度问题出发，引出将相邻像素组合成 patch 的必要性，并给出了基于 einops 的直观实现以及使用 Conv2d 等价实现 的工程化写法。\n\n\n在位置编码部分，对比了 ViT 中采用的 Learned Position Embedding 与传统 Transformer 的正余弦位置编码，并结合论文实验说明，在 patch-level 输入下，不同位置编码策略在图像分类任务中的性能差异并不显著；同时介绍了在 fine-tuning 到不同输入分辨率时，如何通过 二维插值（2D interpolation） 扩展预训练位置编码以适配新的 patch 数量。随后从表示学习的角度讨论了 [CLS] token 在 ViT 中的作用，并与全局平均池化（GAP）进行对比，指出两种方式的性能差异主要来源于训练时学习率等超参数设置，而非结构本身。\n\n\n在模型结构层面，强调了 ViT Encoder 与原始 Transformer Encoder 的关键差异，包括 Pre-Norm 设计 以及使用 GELU 激活函数。接着从 Inductive Bias 的视角对比了 CNN 与 ViT:CNN 通过卷积天然引入局部性、二维结构和平移等变性等先验，而 ViT 的归纳偏置较弱，更多依赖数据和训练过程来学习这些空间规律，这也解释了 ViT 在小数据集上表现受限、但在大规模预训练条件下潜力显著的原因。最后，通过在 CIFAR-10 上训练一个 toy ViT 的实验，展示了模型的分类效果与注意力可视化，并分析了 attention map 接近均匀的现象可能与 低分辨率和数据规模有限 有关，同时给出了进一步改进与扩展的方向，为后续更高效注意力机制以及多模态、视频模型的学习奠定基础。\n\n\n\n创作不易，如果你觉得内容对你有帮助，欢迎请我喝杯咖啡/支付宝红包，支持我继续创作！你们的支持是我最大的动力！ :)",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "href": "posts/100-AI-Papers/02-vision-transformer/Vision-Transformer.html#axial-attention轴向注意力",
    "title": "02: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (Vision-Transformer)",
    "section": "7.1 Axial Attention（轴向注意力）",
    "text": "7.1 Axial Attention（轴向注意力）\n在处理 图像或视频 这类高维输入时，如果直接对所有像素做 全局 self-attention，复杂度是 \\(\\mathcal{O}(H^2 W^2)\\)当图像很大时，这个代价太高。 核心想法:把二维 attention 拆成两次一维 attention（沿着图像的两个“轴”分别做）。 1. Row-wise Attention（行注意力） • 沿着水平方向（宽度轴 W）做注意力，每一行的像素互相关注。 • 复杂度:\\(\\mathcal{O}(H \\cdot W^2)\\)。 2. Column-wise Attention（列注意力） • 沿着垂直方向（高度轴 H）做注意力，每一列的像素互相关注。 • 复杂度: \\(\\mathcal{O}(W \\cdot H^2)\\)。\n组合起来，相当于在 H 和 W 两个轴上都做了全局依赖建模。",
    "crumbs": [
      "100 AI Papers",
      "Vision Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/16-dit/DiT.html",
    "href": "posts/100-AI-Papers/16-dit/DiT.html",
    "title": "16: Scalable Diffusion Models with Transformers (DiT)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/16-dit/DiT.html#experiment",
    "href": "posts/100-AI-Papers/16-dit/DiT.html#experiment",
    "title": "16: Scalable Diffusion Models with Transformers (DiT)",
    "section": "3.1 Experiment",
    "text": "3.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/06-clip/CLIP.html",
    "href": "posts/100-AI-Papers/06-clip/CLIP.html",
    "title": "06: Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
    "section": "",
    "text": "1 Preliminary\n\n\n2 CLIP\n\n\n3 Summary\n\n\n4 Key Concepts\n\n\n5 Q & A\n\n\n6 Related resource & Further Reading"
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "",
    "text": "我们开始第一篇论文的学习： 《Attention is All You Need》 (Vaswani et al. 2023)，也就是传说中的Transformer模型。Transformer模型的提出，彻底改变了自然语言处理（NLP）以及更广泛的领域。该架构完全基于注意力机制(Attention)，不再依赖循环（RNN）或卷积（CNN），因此在训练时更易并行化、效率更高。Transformer 已成为众多前沿模型的基础，不仅在 NLP 中表现突出，也扩展到计算机视觉等领域。比如 ChatGPT、DeepSeek 等大语言模型（LLM）都以 Transformer 为核心架构。所以我们自然就把它当作我们第一篇文章的首选。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#softmax-function",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#softmax-function",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "1.1 Softmax Function",
    "text": "1.1 Softmax Function\nSoftmax Function 是一个 将实数向量转换为概率分布 的函数，定义如下：\n\\[\n\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n\\tag{1}\\]\n其中，\\(z_i\\) 是输入向量的第 \\(i\\) 个元素，\\(e\\) 是自然对数的底数。Softmax 函数的输出是一个概率分布，所有输出值的和为 1。\ndef softmax(z):\n    exp_z = torch.exp(z)\n    return exp_z / torch.sum(exp_z)",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#vector-similarity",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#vector-similarity",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "1.2 Vector Similarity",
    "text": "1.2 Vector Similarity\n在Transformer中，计算向量之间的相似性是一个重要的步骤，常用的方法有点积（Dot Product）和余弦相似度（Cosine Similarity）。在Transformer中，主要使用Dot Product来衡量向量之间的相似性，接下来我们来简单回顾一下Dot Product的计算方法：\n\\[\n\\text{Dot Product}(A, B) = \\sum_{i=1}^{n} A_i \\cdot B_i\n\\tag{2}\\]\n其中，\\(A\\) 和 \\(B\\) 是两个向量，\\(n\\) 是向量的维度，\\(A_i\\) 和 \\(B_i\\) 分别是向量 \\(A\\) 和 \\(B\\) 在第 \\(i\\) 个维度的分量：\n\nDot Product 的值越大，表示两个向量越相似。\nDot Product 的值越小，表示两个向量越不相似。\n\nDot Product也可以看作是 Unnormalized Cosine Similarity，因为它没有对向量进行归一化处理。\n我们也可以使用矩阵乘法来计算多个向量之间的相似性：\n\\[\n\\text{Dot Product Matrix}(A, B) = A B^\\top\n\\tag{3}\\]\n其中，\\(A\\) 是一个 \\(m \\times n\\) 的矩阵，\\(B\\) 是一个 \\(p \\times n\\) 的矩阵，\\(B^\\top\\) 是 \\(B\\) 的转置矩阵，结果是一个 \\(m \\times p\\) 的矩阵，表示 \\(A\\) 中的每个向量与 \\(B\\) 中的每个向量之间的点积。\ndef dot_product_matrix(A, B):\n    return torch.matmul(A, B.T)",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-word-embedding",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-word-embedding",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.1 Word Embedding Layer",
    "text": "2.1 Word Embedding Layer\nWord Embedding 基本是所有语言模型的第一步，它的作用是 将离散的词汇转换为连续的向量表示。这样，模型就可以在一个高维空间中处理词汇之间的关系和相似性。我们通常使用一个嵌入矩阵（Embedding Matrix）来实现这一点:\n\\[\n\\text{Embedding}(w) = W_{e}[w]\n\\tag{4}\\]\n其中，\\(W_{e} \\in \\mathbb{R}^{V \\times d}\\) 是嵌入矩阵, \\(w \\in {0, 1, \\dots, V-1}\\) 是词汇在词表中的索引，\\(d\\) 是嵌入维度，\\(V\\) 是词汇表大小。 该操作等价于将词汇 \\(w\\) 的 One-Hot Encoding 与嵌入矩阵相乘，即：\n\\[\n\\text{Embedding}(w) = W_{e}^{\\top} \\cdot \\text{one hot}(w), \\quad \\text{one hot}(w) \\in \\mathbb{R}^{V}\n\\tag{5}\\]\n从实现角度看，这一过程可以直接理解为：通过词汇索引 \\(w\\)，从嵌入矩阵 \\(W_e\\) 中取出第 \\(w\\) 行作为该词的向量表示。\n更直观的方式就是，我们可以将它看作一个查找表（Lookup Table），通过词汇的索引直接获取对应的嵌入向量。接下来我们来看一下代码实现：\nclass navie_embedding(nn.Module):\n    def __init__(self, v, d):\n        super().__init__()\n        self.embedding = nn.Parameter(torch.randn(v, d)) # 初始化Embedding Table\n    \n    def forward(self, x):\n        # x: (batch_size, seq_len)\n        \n        # 第一种方法: \n        # return self.embedding[x]  # 直接索引获取嵌入向量\n\n        # 第二种方法: One Hot Encoding\n        # x_one_hot = F.one_hot(x, num_classes=self.embedding.size(0)).float() # (batch_size, seq_len, v)\n        # return torch.matmul(x_one_hot, self.embedding) # (batch_size, seq_len, d)\n\n        # 第三种方法，利用Gather函数\n        # batch_size, seq_len = x.size()\n        # x = x.unsqueeze(-1).expand(-1, -1, self.embedding.size(1)) # (batch_size, seq_len, d)\n        # return torch.gather(self.embedding.unsqueeze(0).expand(batch_size, -1, -1), 1, x) # (batch_size, seq_len, d)\n在代码中，我们定义了一个简单的嵌入层 navie_embedding，它接受词汇表大小 v 和嵌入维度 d 作为参数。我们初始化了一个嵌入矩阵 self.embedding，并在前向传播中通过索引、One-Hot 编码或 gather 函数来获取对应的嵌入向量。\n在实际应用中，我们通常会使用 PyTorch 提供的 nn.Embedding 类来简化这一过程：\nclass WordEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n    \n    def forward(self, x):\n        return self.embedding(x)\n在 Transformer 中，词嵌入层不仅用于将输入词汇转换为向量表示，还用于将解码器的输出词汇转换为向量表示。为了保持输入和输出的一致性，Transformer 采用了Weight Tying的策略: 即Output Layer的权重矩阵与Embedding Layer的权重矩阵共享:\n\\[\n\\text{Output Layer Weight} = \\text{Embedding Layer Weight}^\\top\n\\tag{6}\\]\n并且在初始化时，对嵌入向量进行了缩放处理，即乘以 \\(\\sqrt{d_{model}}\\)，以确保嵌入向量的尺度适合后续的注意力计算\n\nIn our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by \\(\\sqrt{d_{model}}\\).  Attention is all you need, p.5",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-postion-embedding",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-postion-embedding",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.2 Position Embedding Layer",
    "text": "2.2 Position Embedding Layer\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.  Attention is all you need, p.6 \n\nTransformer模型中没有使用RNN或CNN，因此缺乏对序列中词汇顺序的建模能力。也就是说，Transformer是 Permutation Invariant 的模型，它无法区分输入序列中词汇的顺序。为了解决这个问题，Transformer引入了位置编码（Position Embedding）来注入位置信息，使模型能够感知词汇在序列中的位置。 其中，位置编码有两种主要的方法: 绝对位置编码和相对位置编码。在原始的Transformer论文中，使用的是绝对位置编码:\n\\[\n\\begin{split}\nPE_{(pos, 2i)} & = \\sin (pos / 10,000^{2i / d_{model}}) \\\\\nPE_{(pos, 2i+1)} & = \\cos (pos / 10,000^{2i+1 / d_{model}})\n\\end{split}\n\\tag{7}\\]\n其中，\\(pos\\) 是词汇在序列中的位置，\\(i \\in [0, d_{model} / 2 )\\) 是嵌入维度的索引，\\(d_{model}\\) 是嵌入维度的大小。通过这种方式，我们可以为每个位置生成一个唯一的向量表示。\n仔细观察上面的公式，我们可以发现:\n\n位置编码的维度与词汇嵌入的 维度相同，这样可以 方便地将两者相加。\n使用正弦和余弦函数可以确保不同位置的编码具有不同的频率，从而捕捉到不同的位置信息。\n这种方法还具有一个优点，即它可以推广到比训练时更长的序列，因为位置编码是基于位置计算的，而不是依赖于具体的词汇。\n\n\n\nQuestion: 为什么是与Word Vector相加，而不是相乘或者 concat 呢？\n\n\n如果用相乘 \\(\\odot\\), 那么位置编码中为0的维度会直接将词向量的对应维度置为0，导致信息丢失。\n如果用concat, 那么词向量和位置编码的维度会增加一倍，导致后续的Attention计算复杂度增加，同时也会改变模型的参数规模，影响训练效果。\n用相加的方式，可以保持词向量的维度不变，同时将位置信息注入到词向量中，使得模型能够同时利用词汇信息和位置信息进行学习。\n\n\n我们来仔细看一下Equation 7，假设我们固定位置 \\(pos=1\\)，并且嵌入维度 \\(d_{model}=6\\)，我们可以计算出对应的位置信息：\npos = 1\nd_model = 6\npe = torch.zeros(d_model)\nfor i in range(d_model // 2):\n    pe[2 * i] = torch.sin(pos / (10000 ** (2 * i / d_model)))\n    pe[2 * i + 1] = torch.cos(pos / (10000 ** (2 * i + 1 / d_model)))\nprint(pe)\n计算的方式很简单。接下来，我们来看一下在Transformer中，我们如何实现它。在实际的实现当中，会利用一些数学的技巧来防止Overflow:\n\\[\n\\frac{1}{(10000^{2i/d_{model}})} = e^{\\ln(10000^{- 2i/d_{model}})} = e^{-(2i/d_{model}) \\cdot \\ln(10000)}\n\\tag{8}\\]\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\n        position = torch.arange(0, max_len).unsqueeze(1) # (max_len, 1)\n        i = torch.arange(0, d_model, 2)  # (d_model/2,)\n        div_term = torch.exp(i * (-math.log(10000.0) / d_model)) # (d_model/2,)\n\n        pe = torch.zeros(max_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\n\n\n\n\n\n\n\n\n\n\n(a) Display Position Embedding in Low Dimension, with sequence length 100\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2: Display Position Embedding in High Dimension, with sequence length 100\n\n\n\n从 Figure 2 中我们可以看到：Sinusoidal PE 是一个「多尺度表示」，不同的维度对应不同的频率，从而捕捉到不同的位置信息：\n\n低维 → 高频 → 局部、精细位置信息\n高维 → 低频 → 全局、长距离位置信息\n\n\n2.2.1 Why Sinusoidal Position Embedding?\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos. We also experimented with using learned positional embeddings.  Attention is all you need, p.6 \n\n论文中提到的第一个好处就是：相对位置编码。假设我们有两个位置 \\(pos\\) 和 \\(pos + k\\)，其中 \\(k\\) 是一个固定的偏移量。那么根据Equation 7，我们可以表示为: \\[\n\\begin{split}\nPE_{(pos \\textcolor{orange}{+ k}, 2i:2i+1)}\n&=\n\\begin{bmatrix}\n\\sin\\big((pos+k)\\,\\omega_i\\big)\\\\\n\\cos\\big((pos+k)\\,\\omega_i\\big)\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\sin(pos\\,\\omega_i)\\cos(k\\,\\omega_i)+\\cos(pos\\,\\omega_i)\\sin(k\\,\\omega_i)\\\\\n\\cos(pos\\,\\omega_i)\\cos(k\\,\\omega_i)-\\sin(pos\\,\\omega_i)\\sin(k\\,\\omega_i)\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\cos(k\\,\\omega_i) & \\sin(k\\,\\omega_i)\\\\\n-\\sin(k\\,\\omega_i) & \\cos(k\\,\\omega_i)\n\\end{bmatrix}\n\\textcolor{orange}{\n    \\begin{bmatrix}\n    \\sin(pos\\,\\omega_i)\\\\\n    \\cos(pos\\,\\omega_i)\n    \\end{bmatrix}\n} \\\\\n&= \\begin{bmatrix}\n\\cos(k\\,\\omega_i) & \\sin(k\\,\\omega_i)\\\\\n-\\sin(k\\,\\omega_i) & \\cos(k\\,\\omega_i)\n\\end{bmatrix} \\textcolor{orange}{PE_{(pos, 2i:2i+1)}}\n\\end{split}\n\\tag{9}\\]\n其中， \\(\\omega_i = 1 / 10,000^{2i/d_{model}}\\)。我们可以看到，位置 \\(pos + k\\) 的编码可以表示为位置 \\(pos\\) 的编码通过一个线性变换得到的结果。这意味着模型可以通过学习这个线性变换来捕捉相对位置关系。 \\(\\begin{bmatrix}\n\\cos(k\\,\\omega_i) & \\sin(k\\,\\omega_i)\\\\\n-\\sin(k\\,\\omega_i) & \\cos(k\\,\\omega_i)\n\\end{bmatrix}\\) 是一个旋转矩阵，表示在二维空间中的旋转变换.\n\n\nNOTE: Rotate Matrix\n\n\nRotate Matrix 是一种二维空间中的线性变换，用于表示点绕原点旋转一定角度的操作。对于一个角度 \\(\\theta\\)，其旋转矩阵定义如下: \\[\nR(\\theta) = \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta)\\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix}\n\\]\n当我们将一个二维向量 \\(v = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) 乘以旋转矩阵 \\(R(\\theta)\\) 时，得到的新向量 \\(v'\\) 表示原始向量绕原点旋转了 \\(\\theta\\) 角度: \\[\nv' = R(\\theta) \\cdot v = \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta)\\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nx \\\\ y\n\\end{bmatrix} = \\begin{bmatrix}\nx \\cos(\\theta) - y \\sin(\\theta) \\\\\nx \\sin(\\theta) + y \\cos(\\theta)\n\\end{bmatrix}\n\\]\n之后我们要学习的RoPE (Su et al. 2023)，也是基于这个性质来设计的。\n\n\n\n第二个好处是：可推广性。由于位置编码是基于位置计算的，而不是依赖于具体的词汇，因此模型可以推广到比训练时更长的序列。这对于处理长文本或长序列任务非常有用。\n\nWe chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.  Attention is all you need, p.6 \n\n假设我们在训练时，模型见过的最大序列长度是 \\(L_{train}\\)，那么在测试时，如果遇到一个更长的序列，长度为 \\(L_{test} &gt; L_{train}\\)，我们仍然可以使用相同的公式来计算位置编码: \\[\nPE_{(pos, 2i)} = \\sin (pos / 10,000^{2i / d_{model}}), \\quad pos \\in [0, L_{test}-1]\n\\tag{10}\\]\n不过个人认为，可拓展的还有一个原因是：由于 (\\(pos+k\\)) 的编码可以表示为仅依赖 \\(k\\) 的线性变换作用在 \\(pos\\) 的编码上，模型更容易学习“相对位移”的规律，从而在更长序列上具备一定外推能力（论文用词为 may allow，表示倾向性而非严格保证）。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-attention",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-attention",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.3 Attention Layer",
    "text": "2.3 Attention Layer\nAttention机制是Transformer的核心组件，它允许模型在处理序列时动态地关注输入序列中的不同部分。Attention机制的基本思想是通过计算查询（Query）、键（Key）和值（Value）之间的相似性Equation 3 来决定如何加权输入信息。具体来说，Attention的计算过程如下:\n\\[\n\\boxed{\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V}\n\\tag{11}\\]\n用代码来表示就是:\ndef scaled_dot_product_attention(q, k, v):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    attn = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn, v)\n    return output, attn\n我们来拆看看一下这个公式，由四部分组成:\n\n\\(Q K^\\top\\): 计算Query和Key之间的点积，得到相似性矩阵，表示每个查询与所有键的相关性。\n\\(\\frac{1}{\\sqrt{d_k}}\\): 这是一个缩放因子，用于防止点积值过大，导致Softmax函数的梯度变得非常小，从而影响模型的训练效果。这里，\\(d_k\\) 是键向量的维度。\n\\(\\text{softmax}(\\cdot)\\): 对相似性矩阵进行归一化，得到每个查询对所有键的注意力权重。\n\\(\\cdot V\\): 使用注意力权重对值进行加权求和，得到最终的输出表示。\n\n\n\\(Q K^\\top\\)\nAttention的第一步，就是计算Query和Key之间的点积 (Equation 3) ，得到相似性矩阵, 表示每个查询与所有键的相关性。假设我们有一个查询矩阵 \\(Q \\in \\mathbb{R}^{n \\times d_k}\\) 和一个键矩阵 \\(K \\in \\mathbb{R}^{m \\times d_k}\\)，那么点积矩阵 \\(Q K^\\top\\) 的计算过程如下:\n\\[\nQ K^\\top = \\begin{bmatrix}\nq_1 \\\\\nq_2 \\\\\n\\vdots \\\\\nq_n\n\\end{bmatrix}\n\\begin{bmatrix}\nk_1^\\top & k_2^\\top & \\cdots & k_m^\\top\n\\end{bmatrix} =\n\\begin{bmatrix}\nq_1 k_1^\\top & q_1 k_2^\\top & \\cdots & q_1 k_m^\\top \\\\\nq_2 k_1^\\top & q_2 k_2^\\top & \\cdots & q_2 k_m^\\top \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nq_n k_1^\\top & q_n k_2^\\top & \\cdots & q_n k_m^\\top\n\\end{bmatrix}\n\\tag{12}\\]\n其中，\\(q_i \\in \\mathbb{R}^{1 \\times d_k}\\) 是查询矩阵 \\(Q\\) 的第 \\(i\\) 行，\\(k_j \\in \\mathbb{R}^{1 \\times d_k}\\) 是键矩阵 \\(K\\) 的第 \\(j\\) 行。结果矩阵 \\(Q K^\\top \\in \\mathbb{R}^{n \\times m}\\) 的每个元素 \\((i, j)\\) 表示查询 \\(q_i\\) 与键 \\(k_j\\) 之间的点积。 \\(QK^\\top\\) 的作用就是告诉我们，每个查询向量与所有键向量之间的相似性，用于之后从值向量中提取相关信息。\n\nDot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  Attention is all you need, p.4 \n\n\n\\(\\frac{1}{\\sqrt{d_k}}\\)\nAttention的第二步，是对点积矩阵进行缩放，使用 \\(\\frac{1}{\\sqrt{d_k}}\\) 作为缩放因子。假设 \\(q, k \\sim \\mathcal{N}(0, I)\\)，那么点积 \\(q k^\\top\\) 的期望和方差分别为:\n\\[\n\\mathbb{E}[q k^\\top] = 0, \\quad \\text{Var}(q k^\\top) = d_k\n\\tag{13}\\]\n我们可以看到，点积的方差与键向量的维度 \\(d_k\\) 成正比。随着 \\(d_k\\) 的增加，点积 logits 的尺度会不断放大，使得 softmax 的输入更容易进入饱和区（saturation regime），此时某些位置的概率接近 1，其余接近 0。在该区域内，softmax 的梯度会显著变小，从而导致反向传播不稳定、训练效率下降。通过除以 \\(\\sqrt{d_k}\\)，可以将点积 logits 的方差重新归一化到 \\(O(1)\\) 的尺度，使 softmax 始终工作在梯度较为敏感的区域，从而稳定训练过程。\n\n\n\n\n\n\nFigure 3: Effect of different scaling factor \\(\\frac{1}{\\sqrt{d_k}}\\) on the distribution of dot-product values。可以看到，未缩放的点积值分布在几个点上，随着维度增加，分布变得更分散，导致softmax更容易饱和。引入缩放因子后，点积值分布保持稳定，有助于softmax的稳定性。\n\n\n\n因此这个有时候我们称之为 “Scale Dot-Product Attention”。通过引入缩放因子 \\(\\frac{1}{\\sqrt{d_k}}\\)，我们可以将点积的方差控制在一个合理的范围内，从而稳定Softmax函数的输出，改善模型的训练效果。\n\n\nNOTE Gradient of Softmax\n\n\n\\[\n\\frac{\\partial \\,\\text{softmax}_i}{\\partial z_j} = \\text{softmax}_i (\\delta_{ij} - \\text{softmax}_j)\n\\]\n其中， \\(\\delta_{ij}\\) 是 Kronecker Delta，当 \\(i=j\\) 时为1，否则为0。\n当某一项 \\(\\text{softmax}_i \\approx 1\\) 时：\n\n\\(\\text{softmax}_i (1 - \\text{softmax}_i) \\approx 0\\)\n其他项 \\(\\text{softmax}_j \\approx 0\\)\n\n所以所有的梯度都接近于0\n\n\n\n\n\n\n\n\nFigure 4: Gradient of Softmax with different scaling factor \\(\\frac{1}{\\sqrt{d_k}}\\)\n\n\n\n\n第三项 \\(\\text{softmax}(\\cdot)\\)\nAttention的第三步，是对缩放后的点积矩阵进行Softmax归一化，得到每个查询对所有键的注意力权重。假设我们有一个缩放后的点积矩阵 \\(S = \\frac{Q K^\\top}{\\sqrt{d_k}}\\)。 这部分很直观，我们对矩阵 \\(S\\) 的每一行应用Softmax函数，得到注意力权重矩阵 \\(A\\):\n\\[\nA_{ij} = \\frac{e^{S_{ij}}}{\\sum_{k} e^{S_{ik}}}\n\\tag{14}\\]\n其中，\\(A_{ij}\\) 表示查询 \\(q_i\\) 对键 \\(k_j\\) 的注意力权重。通过Softmax归一化，我们确保每个查询的注意力权重之和为1，从而可以将其解释为概率分布。这些注意力权重反映了每个查询与所有键之间的相关性，帮助模型动态地关注输入序列中的不同部分。\n\n第四项 \\(\\cdot V\\)\nAttention的最后一步，是使用注意力权重对值进行加权求和，得到最终的输出表示。假设我们有一个值矩阵 \\(V \\in \\mathbb{R}^{m \\times d_v}\\) 和注意力权重矩阵 \\(A \\in \\mathbb{R}^{n \\times m}\\)，那么输出矩阵 \\(O \\in \\mathbb{R}^{n \\times d_v}\\) 的计算过程如下:\n\\[\nO = A V = \\begin{bmatrix}\nA_{11} & A_{12} & \\cdots & A_{1m} \\\\\nA_{21} & A_{22} & \\cdots & A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{n1} & A_{n2} & \\cdots & A_{nm}\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\vdots \\\\\nv_m\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\sum_{j=1}^{m} A_{1j} v_j \\\\\n\\sum_{j=1}^{m} A_{2j} v_j \\\\\n\\vdots \\\\       \n\\sum_{j=1}^{m} A_{nj} v_j\n\\end{bmatrix}\n\\tag{15}\\] 其中，\\(v_j \\in \\mathbb{R}^{1 \\times d_v}\\) 是值矩阵 \\(V\\) 的第 \\(j\\) 行。结果矩阵 \\(O \\in \\mathbb{R}^{n \\times d_v}\\) 的每一行表示对应查询的加权值向量。通过这种方式，模型能够根据注意力权重动态地聚合输入信息，从而生成更具表达力的输出表示。\n\n拆看来看，Attention也没有想象的这么复杂。它主要是通过计算查询和键之间的相似性，来决定如何加权输入的值，从而生成输出表示。\nAttention 就像做菜：\n\nQuery 决定你想做什么，\nKey 决定每个食材的特点，\n\\(QK^\\top\\) 是把所有食材摆在桌上，看看哪些比较Match你的需求，\n\\(\\frac{1}{\\sqrt{d_k}}\\) 是调整食材的分量，\nSoftmax 决定用多少，\nValue 决定最终味道。\n\n它不像 RNN 或者 CNN\n\n你不是按“食材顺序”处理（不是 RNN）\n也不是只看相邻几样（不是 CNN）\n而是 一次性看完整桌食材，再决定重点\n\n\n\n2.3.1 Multi-Head Attention\nMulti Head Attention 就是在 Attention 的基础上，并行地计算多个注意力头（Attention Head），从而捕捉输入序列中的不同子空间信息。其中每一个Head，都是独立的 Self-Attention 机制。Multi-Head Attention 的计算过程如下:\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h) W^O\n\\tag{16}\\]\n其中，每个注意力头 \\(\\text{head}_i\\) 的计算过程如下: \\[\n\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n\\tag{17}\\]\n每个Head独立的运行，然后将所有Head的输出进行拼接（Concat），最后通过一个线性变换 \\(W^O\\) 得到最终的输出表示。通过这种方式，Multi-Head Attention 能够同时关注输入序列中的不同部分，从而增强模型的表达能力。\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  Attention is all you need, p.5 \n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: ModelConfig, is_causal: bool):\n        super().__init__()\n\n        self.is_causal = is_causal\n        self.num_heads = config.num_heads\n        self.head_dim = config.d_model // config.num_heads\n        assert self.head_dim * self.num_heads == config.d_model, \"d_model must be divisible by num_heads\"\n\n        self.q_proj = nn.Linear(config.d_model, config.d_model)\n        self.k_proj = nn.Linear(config.d_model, config.d_model)\n        self.v_proj = nn.Linear(config.d_model, config.d_model)\n    \n    def forward(self, q, k, v):\n        b, q_len, _ = q.size()\n        kv_len = k.size(1)\n\n        # 通过创建view和transpose将q, k, v拆分成多个head\n        q = self.q_proj(q).view(b, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(k).view(b, kv_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.v_proj(v).view(b, kv_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n\n2.3.2 Self-Attention Layer\nSelf-Attention, 顾名思义，是指在计算Attention时，查询（Query）、键（Key）和值（Value）都来自同一个序列。这种机制允许模型在处理序列时，动态地关注序列中的不同位置，从而捕捉到序列内部的依赖关系。Self-Attention的计算过程如下:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V\n\\tag{18}\\]\n在没有任何掩码的情况下，Self-Attention允许每个位置的查询向量关注序列中的所有位置，包括当前位置和未来位置的信息。这种机制使得模型能够捕捉到长距离的依赖关系，从而增强了模型的表达能力。\nout = self.attention(x, x, x) # Self-Attention\n\n\n2.3.3 Causal Self-Attention Layer\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}} \\textcolor{red}{+ M}\\right) V\n\\tag{19}\\]\n其中，\\(M \\in \\mathbb{R}^{n \\times n}\\) 是一个掩码矩阵（Mask Matrix），用于阻止模型在生成序列时访问未来的信息。具体来说，掩码矩阵 \\(M\\) 的定义如下:\n\\[\nM_{ij} = \\begin{cases}\n-\\infty, & \\text{if } j &gt; i \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\tag{20}\\] 这个掩码矩阵确保了在计算注意力权重时，查询位置 \\(i\\) 只能关注到键位置 \\(j \\leq i\\) 的信息，从而实现了自回归（Auto-Regressive）的特性，防止信息泄露。\ndef create_causal_mask(q_len: int, k_len: int) -&gt; torch.Tensor:\n    return torch.tril(torch.ones((q_len, k_len), dtype=torch.bool))\n\nWe need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \\(-\\infty\\)) all values in the input of the softmax which correspond to illegal connections.  Attention is all you need, p.5 \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Causal Mask in Attention\n\n\n\n\n\n\n\n\n\n\n\n(b) Casual Mask with Padding Positions\n\n\n\n\n\n\n\nFigure 5: 在 Figure 5 (a) 中，我们可以看到掩码矩阵 \\(M\\) 的结构。上三角部分被设置为 \\(-\\infty\\)，表示这些位置的注意力权重在经过 Softmax 归一化后将变为 0，从而阻止模型关注未来的信息。在 Figure 5 (b) 中，\\((t_5, t_6, t_7)\\) 是填充位置（Padding Positions），这些位置同样被掩码掉，确保模型不会关注到这些无效的信息。\n\n\n\n\n\nNOTE: Padding Mask\n\n\n除了Causal Mask之外, 在实际应用中, 我们还需要处理变长序列中的填充位置(Padding Positions)。这些位置通常用特殊的填充值(如0)表示, 不包含有效信息。在计算Attention时, 我们需要确保模型不会关注到这些填充位置, 因此我们引入了Padding Mask。 在计算Attention中，我们可以将Padding Mask与Causal Mask结合使用, 形成一个综合的掩码矩阵 Figure 5 (b) 。具体来说, 对于填充位置, 我们同样将对应的注意力权重设置为 \\(-\\infty\\)，确保这些位置在经过Softmax归一化后不会被关注到。\nclass MultiHeadAttention(nn.Module):\n    ...\n    def construct_mask(self, pad_mask, q_len: int, k_len: int, device):\n        # True=allowed, False=masked\n        mask = None\n\n        # causal mask (decoder self-attention only)\n        if self.is_causal:\n            causal_mask = create_causal_mask(q_len, k_len)\n            causal_mask = causal_mask.to(device)\n            mask = causal_mask[None, None, :, :]\n\n        if pad_mask is not None:\n            # True means allowed\n            pad_mask = pad_mask[:, None, None, :].to(device)  # Shape: (batch, 1, 1, kv_len)\n            mask = pad_mask if mask is None else (mask & pad_mask)\n\n        return mask\n    def forward(self, q, k, v, pad_mask=None):\n        ...\n        mask = self.construct_mask(pad_mask, q_len, kv_len, q.device)\n        out, attn = scaled_dot_product_attention(q, k, v, mask)\n        ...\n\n\n用代码来表示Causal Self-Attention， 我们只需要做原来的基础上，在计算Softmax之前，添加掩码矩阵 \\(M\\) 即可:\ndef scaled_dot_product_attention(\n    q, \n    k, \n    v, \n    mask=None \n    ):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    \n    if mask is not None: \n        scores = scores.masked_fill(mask == 0, float(\"-inf\")) \n        \n    attn = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn, v)\n    return output, attn\n其中mask参数是 Padding Mask 与 Causal Mask 的结合。\n\n\n2.3.4 Cross Attention Layer\nCross-Attention 是指在计算 Attention 时，查询（Query）来自解码器的输入序列，而键（Key）和值（Value）来自编码器的输出序列。这种机制允许解码器在生成输出序列时，动态地关注输入序列中的不同部分，从而捕捉到输入和输出之间的依赖关系。Cross-Attention 的计算过程如下:\n\\[\n\\text{Attention}(Q_{dec}, \\textcolor{red}{K_{enc}}, \\textcolor{red}{V_{enc}}) = \\text{softmax}\\left(\\frac{Q_{dec} \\textcolor{red}{K_{enc}^\\top}}{\\sqrt{d_k}}\\right) \\textcolor{red}{V_{enc}}\n\\tag{21}\\]\nout = self.attention(decoder_x, encoder_x, encoder_x) # Cross-Att\n\n\n2.3.5 Time Complexity of Attention\n接下来，我们来分析一下 Self-Attention 的时间复杂度。假设输入序列的长度为 \\(n\\)，嵌入维度为 \\(d\\)，那么 Self-Attention 的时间复杂度主要包括以下几个部分:\n\n计算点积矩阵 \\(Q K^\\top\\) 的时间复杂度为 \\(\\mathcal{O}(n^2 d)\\)，因为我们需要对每个查询向量与所有键向量进行点积计算，共有 \\(n\\) 个查询和 \\(n\\) 个键，每个点积计算的时间复杂度为 \\(\\mathcal{O}(d)\\)。\n计算 Softmax 的时间复杂度为 \\(\\mathcal{O}(n^2)\\)，因为我们需要对每个查询向量的点积结果进行归一化，共有 \\(n\\) 个查询，每个查询需要对 \\(n\\) 个键进行归一化。\n计算加权和 $ V$ 的时间复杂度为 \\(\\mathcal{O}(n^2 d)\\)，因为我们需要对每个查询向量与所有值向量进行加权求和， 共有 \\(n\\) 个查询和 \\(n\\) 个值，每个加权求和的时间复杂度为 \\(\\mathcal{O}(d)\\)。\n\n\\[\n\\begin{array}{|l|l|}\n\\hline\n\\textbf{Step} & \\textbf{Time Complexity} \\\\\n\\hline\nQK^\\top & \\mathcal{O}(n^2 d) \\\\\n\\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\\n\\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\textbf{Total} & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\end{array}\n\\tag{22}\\]\n综上所述，Self-Attention 的总时间复杂度为 \\(\\mathcal{O}(n^2 d)\\)。随着输入序列长度 \\(n\\) 的增加，时间复杂度呈二次增长，这可能会导致在处理长序列时计算开销较大。因此，在实际应用中，研究人员提出了各种优化方法，如稀疏注意力（Sparse Attention）、局部注意力（Local Attention）等，以降低 Self-Attention 的时间复杂度，提高模型的效率。\n\n\nWarning: 理解Attention Complexity的重要性\n\n\n理解Attention的Complexity很重要, 因为它直接影响到Transformer模型的效率和可扩展性。也就是说, 当处理长序列时, Attention的计算复杂度会显著增加, 这可能导致训练和推理的时间成本变得非常高。因此, 研究人员提出了各种优化方法, 如稀疏注意力（Sparse Attention）、局部注意力（Local Attention），Linear Attention，Flash Attention，包括Deep Sparse Attention等，都是为了降低Attention的计算复杂度，从而提升Transformer在处理长序列时的效率和性能。可以说，理解了Attention的Complexity, 就理解了Transformer的效率瓶颈所在。\n\n\n\n\n\n\n\n\nFigure 6: Discussion on Time Complexity and Maximum Sequence Length of Self-Attention\n\n\n\n\n\n\n\n等一等，稳一稳，忍一忍\nRNN的时间复杂度是\\(\\mathcal{O}(n d^2)\\)，Transformer的时间复杂度是 \\(\\mathcal{O}(n^2 d)\\)，那RNN不是更快吗？\n不一定! Transformer 往往更快的关键不在于把总复杂度变成 \\(\\mathcal{O}(1)\\)，而在于把“序列维度上的计算”从必须串行，变成可并行的矩阵运算也就是Figure 6里的 Sequential Operations 对比）。因此在 GPU/TPU 上，Transformer 的吞吐通常更高。\n\n并行计算 / 并行深度（critical path）：RNN 存在严格的时间步依赖，必须按步计算，导致并行深度随序列长度线性增长（\\(\\mathcal{O}(n)\\)）；而 Self-Attention 在一个层内可以用几次矩阵乘法同时处理所有位置，因此并行深度是常数级\\(\\mathcal{O}(1)\\)。\n瓶颈不同（\\(d\\) vs \\(n\\)）：RNN 对隐藏维 \\(d\\) 的主要成本是二次（\\(\\mathcal{O}(n d^2)\\)），而 attention 对 \\(d\\) 近似一次、但对序列长度 \\(n\\) 是二次（\\(\\mathcal{O}(n^2 d)\\)）。所以当序列非常长时，attention 的 \\(n^2\\) 会成为瓶颈，实践中常用\n\nFlashAttention(Dao et al. 2022) (优化常数与显存/IO)\nWindow / restricted attention（将全局注意力改为局部窗口，类似图Figure 6 中的“restricted self-attention”那一行）来进一步提升长序列效率。\n\n\n\n\n一句话总结Attention就是:\n\nAttention is Weighted Sum of Values, where Weights are from Softmax of Scaled Dot-Product of Queries and Keys.",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-normalization",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-normalization",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.4 Normalization Layer",
    "text": "2.4 Normalization Layer\nLayer Normalization (Ba, Kiros, and Hinton 2016) 是一种用于深度神经网络的归一化技术，旨在提高训练的稳定性和速度。与批量归一化（Batch Normalization）不同，Layer Normalization 是在每个样本的特征维度上 (\\(d_{model}\\)) 进行归一化，而不是在批量维度上进行归一化。这使得 Layer Normalization 特别适用于循环神经网络（RNN）和 Transformer 等模型。\n\n\nQuestion：为什么Layer Normalization更适合Sequence Modeling？\n\n\n\n序列长度变化: 在处理变长序列时，批量归一化可能会受到不同长度序列的影响，而 Layer Normalization 可以独立于序列长度进行归一化。\n时间步依赖: 在 RNN 中，时间步之间存在依赖关系，批量归一化可能会破坏这种依赖关系，而 Layer Normalization 保持了时间步之间的独立性。\n小批量大小: 在某些任务中，批量大小可能非常小，甚至为1，这使得批量归一化效果不佳，而 Layer Normalization 不依赖于批量大小。\n\n\n\nLayer Normalization 的计算过程如下:\n\\[\n\\begin{split}\n\\mu & = \\frac{1}{d} \\sum_{i=1}^{d} x_i \\\\\n\\sigma^2 & = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2 \\\\\n\\hat{x_i} & = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\ny_i & = \\gamma \\odot \\hat{x_i} + \\beta\n\\end{split}\n\\tag{23}\\]\n其中，\\(x_i\\) 是输入向量的第 \\(i\\) 个元素，\\(d\\) 是向量的维度，\\(\\mu\\) 和 \\(\\sigma^2\\) 分别是均值和方差，\\(\\epsilon\\) 是一个小常数，用于防止除零错误，\\(\\gamma \\in \\mathbb{R}^d\\) 和 \\(\\beta \\in \\mathbb{R}^d\\) 是可学习的参数，用于缩放和平移归一化后的输出。\nclass LayerNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.beta = nn.Parameter(torch.zeros(dim))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, keepdim=True, unbiased=False)\n\n        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_hat + self.beta\n\n\nQuestion: 为什么 \\(\\gamma\\) 和 \\(\\beta\\) 是必要的? 并且要初始化为1和0?\n\n\n为什么是必要的？\n在Normalization之后 \\(\\hat{x_i}\\) 是归一化的输出, 其均值为0，方差为1。这很稳定，但也有一个副作用：模型失去了“想要多大尺度/什么均值”的自由度。如果没有 \\(\\gamma\\) 和 \\(\\beta\\)，模型将失去对原始数据分布的表达能力。通过引入 \\(\\gamma\\) 和 \\(\\beta\\)，模型可以学习到适合当前任务的缩放和平移，从而恢复或调整数据的分布。\n为什么初始化为1和0？\n初始化成 \\(\\gamma=1, \\beta=0\\) 时，\\(y_i = \\hat{x_i}\\)，即初始时 Layer Normalization 的输出与归一化后的输入相同。这种初始化方式确保了在训练开始时，Layer Normalization 不会对数据进行任何缩放或平移，从而避免了对模型训练的干扰。随着训练的进行，模型可以根据需要调整 \\(\\gamma\\) 和 \\(\\beta\\) 的值，以适应具体任务的需求。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-feed-forward",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-feed-forward",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.5 Feed Forward Layer",
    "text": "2.5 Feed Forward Layer\n在Transformer中，前馈神经网络（Feed Forward Network, FFN）是每个编码器和解码器层中的一个重要组成部分。它的主要作用是对每个位置的表示进行非线性变换，从而增强模型的表达能力。前馈神经网络通常由两个线性变换和一个非线性激活函数组成，具体计算过程如下: \\[\n\\text{FFN}(\\mathrm{x}) = \\underset{}{\\max} (0, \\mathrm{x} W_{1} + b_{1}) W_{2} + b_{2}\n\\tag{24}\\]\n其中，\\(\\mathrm{x} \\in \\mathbb{R}^{d_{model}}\\) 是输入向量，\\(W_{1} \\in \\mathbb{R}^{d_{model} \\times d_{ff}}\\) 和 \\(W_{2} \\in \\mathbb{R}^{d_{ff} \\times d_{model}}\\) 是权重矩阵，\\(b_{1} \\in \\mathbb{R}^{d_{ff}}\\) 和 \\(b_{2} \\in \\mathbb{R}^{d_{model}}\\) 是偏置向量，\\(d_{ff}\\) 是前馈网络的隐藏层维度，通常大于 \\(d_{model}\\)，在原始的Transformer论文中，\\(d_{ff}\\) 通常设置为 \\(4 \\times d_{model}\\)。\n为什么要使用前馈神经网络？主要有以下几个原因:\n\n非线性变换: 前馈神经网络引入了非线性激活函数（如ReLU），使模型能够学习复杂的非线性关系，从而增强了模型的表达能力。\n位置独立性: 前馈神经网络对每个位置的表示进行独立的变换，这有助于模型捕捉每个位置的特征，而不受其他位置的影响。\n增加模型容量: 通过增加前馈神经网络的隐藏层维度 \\(d_{ff}\\)，可以显著增加模型的容量，从而提升模型的性能。\n\nclass FFN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#residual-connection",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#residual-connection",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.6 Residual Connection",
    "text": "2.6 Residual Connection\n当然，如果要训练一个DEEP Transformer模型，避不开的就是Residual Connection (He et al. 2015), 它的作用是缓解深层网络中的梯度消失问题，从而使得更深层的网络能够被有效训练。其基本思想是通过引入跳跃连接（Skip Connection），将输入直接添加到输出上，从而形成一个“捷径”，使得梯度可以直接传递到更早的层。具体来说，假设我们有一个子层（Sublayer），其输入为 \\(\\mathbf   {x}\\)，输出为 \\(\\mathrm{Sublayer}(\\mathbf{x})\\)，那么引入残差连接后的输出 \\(\\mathbf{y}\\) 可以表示为:\n\\[\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))\n\\]\n\n2.6.1 Backpropagation through Residual Connection\n接下来，我们来简单分析一下，残差连接是如何帮助缓解梯度消失问题的。假设我们有一个损失函数 \\(\\mathcal{L}\\)，\n\\[\n\\mathbf{y} = \\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x})\n\\tag{25}\\]\n我们想要计算损失函数对输入 \\(\\mathbf{x}\\) 的梯度 \\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\\)。根据链式法则，我们可以得到:\n\\[\n\\begin{split}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial}{\\partial \\mathbf{x}}\\left(\\mathbf{x}+\\mathrm{Sublayer}(\\mathbf{x})\\right) \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left(\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{x}} +\n\\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}}_{\\text{straight path}} +\n\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot\n\\frac{\\partial\\,\\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\text{through the sub-layer}}\n\\end{split}\n\\tag{26}\\]\n其中，\\(y\\) 是残差连接的输出，\\(\\mathbf{I}\\) 是单位矩阵。可以看到，梯度 \\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\\) 包含了两部分:\n\nStraight Path: 这部分梯度直接来自于损失函数对输出 \\(\\mathbf{y}\\) 的梯度 \\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}\\)，它不经过任何子层的变换，因此不会受到梯度消失的影响。\nThrough the Sub-layer: 这部分梯度通过子层的变换传播，可能会受到梯度消失的影响。\n\n通过引入残差连接，模型可以确保梯度在反向传播过程中至少有一部分（Straight Path）能够直接传递到更早的层，从而缓解了梯度消失的问题。这使得深层网络能够被有效训练，从而提升了模型的性能。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-output-layer",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#sec-output-layer",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.7 Output Layer",
    "text": "2.7 Output Layer\n在Transformer的输出层，通常会使用一个线性层（Linear Layer）将解码器的输出转换为词汇表大小的向量，然后通过Softmax函数Equation 1 将其转换为概率分布，从而生成最终的预测结果。具体来说，假设解码器的输出为 \\(\\mathbf{h} \\in \\mathbb{R}^{d_{model}}\\)，词汇表大小为 \\(V\\)，那么输出层的计算过程如下:\n\\[\n\\mathbf{y} = \\text{Softmax}(\\mathbf{h} W_{o} + b_o)\n\\tag{27}\\]\n其中，\\(W_{o} \\in \\mathbb{R}^{d_{model} \\times V}\\) 是线性层的权重矩阵，\\(b_o \\in \\mathbb{R}^{V}\\) 是偏置向量，\\(\\mathbf{y} \\in \\mathbb{R}^{V}\\) 是最终的预测结果，表示每个词汇的概率分布。\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        ...\n        self.output_layer = nn.Linear(config.d_model, config.tgt_vocab_size)\n    \n    def forward(self, original, target):\n        ...\n        logits = self.output_layer(y_dec)\n        return logits",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#encoder-decoder-layer",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#encoder-decoder-layer",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.8 Encoder & Decoder Layer",
    "text": "2.8 Encoder & Decoder Layer\n有了这些基础组件，我们就可以和叠积木一样，来搭建Transformer的Encoder和Decoder层了。\nEncoder 可以表示为:\n\\[\n\\begin{split}\n\\text{EncoderLayer}_{i}(\\mathrm{x}) & = \\text{LayerNorm}_{i}\\left(\\mathrm{x} + \\text{MultiHeadSelfAttention}_{i}(\\mathrm{x}, \\mathrm{x},\\mathrm{x})\\right) \\\\\n\\text{EncoderLayer}_{i}(\\mathrm{x}) & = \\text{LayerNorm}_{i}\\left(\\mathrm{x} + \\text{FFN}_{i}(\\mathrm{x})\\right) \\\\\n\\end{split}\n\\tag{28}\\]\n其中，\\(\\mathrm{x}\\) 是输入向量，\\(\\text{MultiHeadSelfAttention}_{i}\\) 是第 \\(i\\) 个编码器层的多头自注意力机制，\\(\\text{FFN}_{i}\\) 是第 \\(i\\) 个编码器层的前馈神经网络，\\(\\text{LayerNorm}_{i}\\) 是第 \\(i\\) 个编码器层的层归一化。\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.attn = MultiHeadAttention(config, is_causal=False)\n        self.ln1 = LayerNorm(config.d_model)\n\n        self.ffn = FFN(config)\n        self.ln2 = LayerNorm(config.d_model)\n\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x: torch.Tensor, pad_mask=None) -&gt; torch.Tensor:\n        attn_output, _ = self.attn(x, x, x, pad_mask=pad_mask)\n        x = self.ln1(x + self.dropout(attn_output))\n        ffn_output = self.ffn(x)\n        x = self.ln2(x + ffn_output)\n\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.layers = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_layers)])\n\n    def forward(self, x: torch.Tensor, pad_mask=None) -&gt; torch.Tensor:\n        for layer in self.layers:\n            x = layer(x, pad_mask=pad_mask)\n        return x\n下图展示了Encoder Layer的过程：\n\n\n\n\n\n\nFigure 7: Encoding Layer Process\n\n\n\nDecoder 可以表示为: \\[\n\\begin{split}\n\\text{DecoderLayer}_{i}(\\mathrm{y}, \\mathrm{x}) & = \\text{LayerNorm}_{i}\\left(\\mathrm{y} + \\text{CausalMultiHeadSelfAttention}_{i}(\\mathrm{y}, \\mathrm{y}, \\mathrm{y})\\right) \\\\\n\\text{DecoderLayer}_{i}(\\mathrm{y}, \\mathrm{x}) & = \\text{LayerNorm}_{i}\\left(\\mathrm{y} + \\text{CrossAttention}_{i}(\\mathrm{y}, \\mathrm{x}_{enc}, \\mathrm{x}_{enc})\\right) \\\\\n\\text{DecoderLayer}_{i}(\\mathrm{y}, \\mathrm{x}) & = \\text{LayerNorm}_{i}\\left(\\mathrm{y} + \\text{FFN}_{i}(\\mathrm{y})\\right) \\\\\n\\end{split}\n\\tag{29}\\]\n其中，\\(\\mathrm{y}\\) 是解码器的输入向量，\\(\\mathrm{x}_{enc}\\) 是编码器的输出向量，\\(\\text{CausalMultiHeadSelfAttention}_{i}\\) 是第 \\(i\\) 个解码器层的因果多头自注意力机制，\\(\\text{CrossAttention}_{i}\\) 是第 \\(i\\) 个解码器层的交叉注意力机制，\\(\\text{FFN}_{i}\\) 是第 \\(i\\) 个解码器层的前馈神经网络，\\(\\text{LayerNorm}_{i}\\) 是第 \\(i\\) 个解码器层的层归一化。\nclass DecoderBlock(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.self_attn = MultiHeadAttention(config, is_causal=True)\n        self.ln1 = LayerNorm(config.d_model)\n\n        self.cross_attn = MultiHeadAttention(config, is_causal=False)\n        self.ln2 = LayerNorm(config.d_model)\n\n        self.ffn = FFN(config)\n        self.ln3 = LayerNorm(config.d_model)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        enc_output: torch.Tensor,\n        src_pad_mask=None,\n        tgt_pad_mask=None,\n    ) -&gt; torch.Tensor:\n        self_attn_output, _ = self.self_attn(x, x, x, pad_mask=tgt_pad_mask)\n        x = self.ln1(x + self_attn_output)\n\n        cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, pad_mask=src_pad_mask)\n        x = self.ln2(x + cross_attn_output)\n\n        ffn_output = self.ffn(x)\n        x = self.ln3(x + ffn_output)\n\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.layers = nn.ModuleList([DecoderBlock(config) for _ in range(config.num_layers)])\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        enc_output: torch.Tensor,\n        src_pad_mask=None,\n        tgt_pad_mask=None,\n    ) -&gt; torch.Tensor:\n        for layer in self.layers:\n            x = layer(\n                x,\n                enc_output,\n                src_pad_mask=src_pad_mask,\n                tgt_pad_mask=tgt_pad_mask,\n            )\n        return x\n下图展示了Decoder Layer的过程：\n\n\n\n\n\n\nFigure 8: Decoding Layer Process\n\n\n\n通过堆叠多个编码器层和解码器层，我们就可以构建出完整的Transformer模型。\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.vocab_embedding = WordEmbedding(config.vocab_size, config.d_model)\n        self.positional_embedding = PositionalEmbedding(config)\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n\n        self.output_proj = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n        self.apply(self._init_weights)\n        self._tie_weights()\n\n    def _tie_weights(self):\n        self.output_proj.weight = self.vocab_embedding.embedding.weight\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n        elif isinstance(module, nn.Embedding):\n            nn.init.xavier_uniform_(module.weight)\n\n        elif isinstance(module, LayerNorm):\n            nn.init.ones_(module.gamma)\n            nn.init.zeros_(module.beta)\n\n    def forward(\n        self,\n        src_input: torch.Tensor,\n        tgt_input: torch.Tensor,\n        src_pad_mask=None,\n        tgt_pad_mask=None,\n    ) -&gt; torch.Tensor:\n        # Get Src and Tgt embeddings\n        src_embeddings = self.vocab_embedding(src_input) * math.sqrt(\n            self.vocab_embedding.embedding.embedding_dim\n        ) + self.positional_embedding(src_input)\n        tgt_embeddings = self.vocab_embedding(tgt_input) * math.sqrt(\n            self.vocab_embedding.embedding.embedding_dim\n        ) + self.positional_embedding(tgt_input)\n\n        # Feed through Encoder\n        enc_output = self.encoder(src_embeddings, pad_mask=src_pad_mask)\n\n        # Feed through Decoder with Encoder output\n        dec_output = self.decoder(\n            tgt_embeddings,\n            enc_output,\n            src_pad_mask=src_pad_mask,\n            tgt_pad_mask=tgt_pad_mask,\n        )\n\n        logits = self.output_proj(dec_output)\n        return logits",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#others",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#others",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "2.9 Others",
    "text": "2.9 Others\n当然，除了以上的一个部分，Transformer中还有几个值得一提的部分,比如:\n\nDropout Layer: 用于防止过拟合\nLabel Smoothing: 用于提高模型的泛化能力\n\n\n2.9.1 Dropout Layer\nDropout (Dropout2014srivastava?) 是一种常用的正则化技术，旨在防止神经网络在训练过程中过拟合。其基本思想是在训练过程中，随机地“丢弃”一部分神经元，即将它们的输出设置为零，从而减少神经元之间的相互依赖，提高模型的泛化能力。具体来说，假设我们有一个神经网络层的输入向量 \\(\\mathbf{x} \\in \\mathbb{R}^{d}\\)，Dropout 的计算过程如下:\n\\[\n\\begin{split}\n\\mathbf{r} & \\sim \\text{Bernoulli}(p) \\\\\n\\hat{\\mathbf{x}} & = \\mathbf{x} \\odot \\mathbf{r} \\\\\n\\mathbf{y} & = \\frac{1}{p} \\hat{\\mathbf{x}}\n\\end{split}\n\\tag{30}\\]\n其中，\\(\\mathbf{r} \\in \\mathbb{R}^{d}\\) 是一个与输入向量 \\(\\mathbf{x}\\) 形状相同的二进制掩码向量，其每个元素独立地服从伯努利分布，取值为1的概率为 \\(p\\)（保留概率），取值为0的概率为 \\(1-p\\)（丢弃概率）。\\(\\odot\\) 表示逐元素乘法操作，\\(\\hat{\\mathbf{x}}\\) 是经过 Dropout 处理后的输入向量，\\(\\mathbf{y}\\) 是最终的输出向量，通过除以保留概率 \\(p\\) 来进行缩放，以保持输出的期望值不变。\n用Python实现Dropout如下:\nclass Dropout(nn.Module):\n    def __init__(self, p=0.5):\n        super().__init__()\n        self.p = p\n    def forward(self, x):\n        if self.training:\n            mask = (torch.rand_like(x) &lt; self.p).float()\n            return (x * mask) / self.p\n        else:\n            return x\n\n\n2.9.2 Label Smoothing\nLabel Smoothing (RethinkingInception2016szegedy?) 是一种用于分类任务的正则化技术，旨在提高模型的泛化能力。其基本思想是将目标标签从“硬标签”（one-hot encoding）转换为“软标签”，即在目标标签中引入一定的平滑度，从而防止模型过于自信地预测某个类别。具体来说，假设我们有一个分类任务，类别总数为 \\(C\\)，原始的目标标签为 \\(\\mathbf{y} \\in \\mathbb{R}^{C}\\)，其中只有一个元素为1，其余元素为0（one-hot encoding）。Label Smoothing 的计算过程如下:\n\\[\n\\mathbf{y}_{smooth} = (1 - \\epsilon) \\mathbf{y} + \\frac{\\epsilon}{C}\n\\tag{31}\\]\n其中，\\(\\epsilon\\) 是平滑参数，控制标签的平滑程度，\\(\\mathbf{y}_{smooth} \\in \\mathbb{R}^{C}\\) 是经过 Label Smoothing 处理后的目标标签。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#dataset",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#dataset",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.1 Dataset",
    "text": "3.1 Dataset\n首先，我们来看一下我们的数据集，在这里，我们使用的Ted Talks的数据集中的英文-中文Pairs，它包含了大量的TED演讲视频的字幕文本，涵盖了多个领域和主题。我们看一下其中几个例子：\n[{'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\",\n  'zh': '非常谢谢，克里斯。的确非常荣幸 能有第二次站在这个台上的机会，我真是非常感激。'},\n {'en': 'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.',\n  'zh': '这个会议真是让我感到惊叹不已，我还要谢谢你们留下的 关于我上次演讲的精彩评论'},\n {'en': 'And I say that sincerely, partly because  I need that.  Put yourselves in my position.',\n  'zh': '我是非常真诚的，部分原因是因为----我的确非常需要！ 你设身处地为我想想！'},\n {'en': 'I flew on Air Force Two for eight years.', 'zh': '我坐了8年的空军二号。'},\n {'en': 'Now I have to take off my shoes or boots to get on an airplane!',\n  'zh': '不过现在上飞机前我则要脱掉我的鞋子'}]\n其中 train_dataset 有231,266条数据，test_dataset 有 8,549 条数据。\n\n3.1.1 Tokenizer & Vocabulary\n在论文中，Target 和 Source 使用同一个Tokenizer，并且共享同一个词表（Vocabulary）。并且使用 BPE (Byte Pair Encoding) (Sennrich, Haddow, and Birch 2016) 来进行分词和构建词表。\n在这里，我们用Hugging Face的transformers库中的Tokenizer来进行分词和构建词表，词表大小设置为10,000。\ndef load_or_train_joint_bpe_tokenizer(\n    vocab_size: int,\n    save_prefix: str,\n    save_name: str = \"bpe_joint.json\",\n    src_corpus_file: str = \"train_src.txt\",\n    tgt_corpus_file: str = \"train_tgt.txt\",\n):\n    save_path = f\"{save_prefix}_{save_name}\"\n\n    if os.path.exists(save_path):\n        print(f\"Loading tokenizer from {save_path}\")\n        return Tokenizer.from_file(save_path)\n\n    # Train ONE tokenizer on BOTH corpora (concatenated dataset)\n    tokenizer = Tokenizer(models.BPE(unk_token=\"&lt;unk&gt;\"))\n    tokenizer.normalizer = NFKC()\n    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n    tokenizer.decoder = ByteLevelDecoder()\n\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"],\n    )\n\n    tokenizer.train([src_corpus_file, tgt_corpus_file], trainer)\n    tokenizer.save(save_path)\n    print(f\"Saved tokenizer to {save_path}\")\n\n    return tokenizer\n在训练完成后，我们提前处理好数据集，保存为PT格式，方便后续的训练使用。\ndef encode_file_to_pt(\n    tokenizer,\n    in_path: str,\n    out_path: str,\n    add_special_tokens: bool = True,\n    bos_token: str = \"&lt;s&gt;\",\n    eos_token: str = \"&lt;/s&gt;\",\n    max_lines: int | None = None,\n):\n    if os.path.exists(out_path):\n        print(f\"{out_path} already exists\")\n        return\n\n    bos_id = tokenizer.token_to_id(bos_token)\n    eos_id = tokenizer.token_to_id(eos_token)\n\n    all_ids = []\n    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            if max_lines is not None and i &gt;= max_lines:\n                break\n            text = line.rstrip(\"\\n\")\n            enc = tokenizer.encode(text)\n            ids = enc.ids\n\n            if add_special_tokens:\n                ids = [bos_id] + ids + [eos_id]\n\n            all_ids.append(torch.tensor(ids, dtype=torch.int32))\n\n    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n    torch.save(all_ids, out_path)\n    print(f\"Saved {len(all_ids)} sequences to {out_path}\")\n\n\nencode_file_to_pt(tokenizer, \"train_src.txt\", \"train_src_ids.pt\")\nencode_file_to_pt(tokenizer, \"train_tgt.txt\", \"train_tgt_ids.pt\")\nencode_file_to_pt(tokenizer, \"test_src.txt\", \"test_src_ids.pt\")\nencode_file_to_pt(tokenizer, \"test_tgt.txt\", \"test_tgt_ids.pt\")\n\n\n3.1.2 Padding Samples\n至此，我们的数据集就准备好了。在后续的训练中，我们可以直接加载这些预处理好的数据集进行训练，需要注意的一点是，我们在加载数据集时，需要对输入序列进行Padding，以确保每个Batch中的序列长度一致。在论文中有一个方式，就是将长度差不多的序列放在同一个Batch中，这样可以减少Padding的数量，从而提高训练效率。在这里，我们使用一个 Sampler 来实现这个功能。\nclass BucketBatchSampler(Sampler[list[int]]):\n    \"\"\"\n    Yields batches of indices where sequences have similar lengths.\n    length_fn: function(idx) -&gt; int\n    \"\"\"\n\n    def __init__(\n        self,\n        lengths,\n        batch_size: int,\n        bucket_size: int = 2048,\n        shuffle: bool = True,\n        drop_last: bool = False,\n        seed: int = 0,\n    ):\n        self.lengths = list(lengths)\n        self.batch_size = batch_size\n        self.bucket_size = bucket_size\n        self.shuffle = shuffle\n        self.drop_last = drop_last\n        self.seed = seed\n\n    def __iter__(self):\n        rng = random.Random(self.seed)\n\n        indices = list(range(len(self.lengths)))\n        if self.shuffle:\n            rng.shuffle(indices)\n\n        # chunk into buckets\n        for b_start in range(0, len(indices), self.bucket_size):\n            bucket = indices[b_start : b_start + self.bucket_size]\n            # sort inside bucket by length\n            bucket.sort(key=lambda i: self.lengths[i])\n\n            # make batches\n            batches = [bucket[i : i + self.batch_size] for i in range(0, len(bucket), self.batch_size)]\n            if self.drop_last and len(batches) &gt; 0 and len(batches[-1]) &lt; self.batch_size:\n                batches = batches[:-1]\n\n            if self.shuffle:\n                rng.shuffle(batches)\n\n            for batch in batches:\n                yield batch\n\n        # update seed so next epoch reshuffles differently\n        self.seed += 1\n\n    def __len__(self):\n        n = len(self.lengths)\n        if self.drop_last:\n            return n // self.batch_size\n        return math.ceil(n / self.batch_size)\n这个 BucketBatchSampler 会根据序列的长度将它们分配到不同的Bucket中，然后在每个Bucket内按长度排序，最后生成Batch。这样可以确保每个Batch中的序列长度相似，从而减少Padding的数量。\n有了一个Batch之后，我们还需要一个 collate_fn 来对Batch中的序列进行Padding:\n\ndef translation_collate(batch, pad_id: int, sos_id: int, eos_id: int, max_len: int | None = None):\n    src_list = [item[\"src_ids\"] for item in batch]\n    tgt_list = [item[\"tgt_ids\"] for item in batch]\n\n    if max_len is not None:\n        src_list = [x[:max_len] for x in src_list]\n        tgt_list = [x[:max_len] for x in tgt_list]  # leave room for EOS\n\n    decoder_list = [t[:-1] for t in tgt_list]\n    labels_list = [t[1:] for t in tgt_list]\n\n    src_max = max(x.numel() for x in src_list)\n    dec_max = max(x.numel() for x in decoder_list)\n\n    def pad_1d(x: torch.Tensor, L: int):\n        x = x.to(torch.long)\n        if x.numel() == L:\n            return x\n        return torch.cat([x, x.new_full((L - x.numel(),), pad_id, dtype=torch.long)])\n\n    encoder_input_ids = torch.stack([pad_1d(x, src_max) for x in src_list], dim=0)\n    decoder_input_ids = torch.stack([pad_1d(x, dec_max) for x in decoder_list], dim=0)\n    labels = torch.stack([pad_1d(x, dec_max) for x in labels_list], dim=0)\n\n    return {\n        \"encoder_input_ids\": encoder_input_ids,\n        \"decoder_input_ids\": decoder_input_ids,\n        \"labels\": labels,\n        \"encoder_mask\": create_padding_mask(encoder_input_ids, pad_id),\n        \"decoder_mask\": create_padding_mask(decoder_input_ids, pad_id),\n    }\n在这个 collate_fn 中，我们还同时构造了Labels， Labels是Decoder输入序列右移一位得到的[t[1:] for t in tgt_list]，这样可以确保模型在训练时，能够正确地预测下一个词。\n至此，我们的数据预处理和Batch准备工作就完成了，接下来我们来看一下模型的训练细节。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#weight-initialization",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#weight-initialization",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.2 Weight Initialization",
    "text": "3.2 Weight Initialization\n在论文中，没有提到如何Initialize的，在这里，我用Xavier initialization, 来初始化Transformer模型的权重参数。Xavier初始化旨在保持每层神经网络的输入和输出的方差相等，从而避免梯度消失或爆炸的问题。具体来说，假设我们有一个神经网络层，其输入维度为 \\(n_{in}\\)，输出维度为 \\(n_{out}\\)，那么Xavier初始化的权重矩阵 \\(W\\) 的每个元素可以从以下均匀分布中采样:\n\\[\nW_{i,j} \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)\n\\tag{32}\\]\ndef _init_weights(self, module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n\n    elif isinstance(module, nn.Embedding):\n        nn.init.xavier_uniform_(module.weight)\n具体的原因为什么Xavier initialization有效，在这里就不多赘述了，之后可能会有专门的文章来介绍这个内容。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#optimizer",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#optimizer",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.3 Optimizer",
    "text": "3.3 Optimizer\nTransformer的论文中，用的是Adam Optimizer (Kingma and Ba 2017), 它是一种自适应学习率优化算法，结合了动量法和RMSProp的优点。Adam通过计算梯度的一阶矩估计（动量）和二阶矩估计（梯度的平方的指数加权平均）来调整每个参数的学习率，从而提高训练的稳定性和收敛速度。Adam的更新规则如下:\n\n\n\\begin{algorithm} \\caption{Adam with L2 Regularization(Weight Decay)} \\begin{algorithmic} \\Require Parameters $\\theta_0$ \\Require Learning rate $\\alpha$, betas $(\\beta_1,\\beta_2)$ \\Require Weight decay coefficient $\\lambda$ \\State Initialize $m_0 \\gets 0$, $v_0 \\gets 0$, $t \\gets 0$ \\While{not converged} \\State $t \\gets t + 1$ \\State Compute gradient $g_t \\gets \\nabla_{\\theta}\\mathcal{L}(\\theta_{t-1})$ \\State Apply L2 regularization: $g_t^{\\text{wd}} \\gets g_t + \\lambda \\theta_{t-1}$ \\State First moment: $m_t \\gets \\beta_1 m_{t-1} + (1-\\beta_1) g_t^{\\text{wd}}$ \\State Second moment: $v_t \\gets \\beta_2 v_{t-1} + (1-\\beta_2)\\left(g_t^{\\text{wd}}\\right)^2$ \\State Bias-corrected moments: $\\hat m_t \\gets \\dfrac{m_t}{1-\\beta_1^t}$, $\\hat v_t \\gets \\dfrac{v_t}{1-\\beta_2^t}$ \\State Parameter update: $\\theta_t \\gets \\theta_{t-1} - \\alpha \\dfrac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon}$ \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n用Python实现Adam Optimizer如下:\nclass Adam:\n    def __init__(self, params, lr, betas=(0.9, 0.98), weight_decay=0.01, eps=1e-9):\n        self.params = list(params)\n        self.lr = lr\n        self.betas = betas\n        self.weight_decay = weight_decay\n        self.eps = eps\n\n        self.state = {}\n        for p in self.params:\n            self.state[p] = {\n                \"step\": 0,\n                \"m\": torch.zeros_like(p.data),\n                \"v\": torch.zeros_like(p.data),\n            }\n\n    def update_lr(self, new_lr):\n        self.lr = new_lr\n\n    def step(self):\n        for p in self.params:\n            if p.grad is None:\n                continue\n\n            grad = p.grad.data\n            state = self.state[p]\n\n            state[\"step\"] += 1\n            beta1, beta2 = self.betas\n\n            # Update biased first moment estimate\n            state[\"m\"] = beta1 * state[\"m\"] + (1 - beta1) * grad\n            # Update biased second raw moment estimate\n            state[\"v\"] = beta2 * state[\"v\"] + (1 - beta2) * (grad * grad)\n\n            # Compute bias-corrected first moment estimate\n            m_hat = state[\"m\"] / (1 - beta1 ** state[\"step\"])\n            # Compute bias-corrected second raw moment estimate\n            v_hat = state[\"v\"] / (1 - beta2 ** state[\"step\"])\n\n            # Update parameters\n            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n\n            # Apply weight decay\n            if self.weight_decay &gt; 0:\n                p.data -= self.lr * self.weight_decay * p.data\n\n    def zero_grad(self):\n        for p in self.params:\n            if p.grad is not None:\n                p.grad.detach_()\n                p.grad.zero_()\n\n\nNOTE: Adam Optimizer\n\n\n对于不了的Adam的同学，也不用太担心，之后我们会有一系列的文章，专门介绍这些优化器的，包括Adam(Kingma and Ba 2017)，AdamW(Loshchilov and Hutter 2019)，以及最近比较火的Muon(Jordan et al. 2024)等。\n\n\n\n3.3.1 Learning Rate Scheduler\n在Transformer的论文中，作者利用了一个自定义的学习率调度器（Learning Rate Scheduler），它在训练的初始阶段逐渐增加学习率，然后在达到预设的步数后逐渐减小学习率。具体来说，学习率的计算公式如下:\n\\[\n\\text{lrate} = d_{model}^{-0.5} \\cdot \\min\\left(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}\\right)\n\\tag{33}\\]\n其中，\\(d_{model}\\) 是模型的隐藏层维度，\\(step\\_num\\) 是当前的训练步数，\\(warmup\\_steps\\) 是预设的预热步数。在训练的前 \\(warmup\\_steps\\) 步中，学习率线性增加；在之后的训练过程中，学习率按照 \\(step\\_num^{-0.5}\\) 的比例逐渐减小。\n\n\n\n\n\n\nFigure 9: Visualization of Learning Rate Scheduler\n\n\n\ndef get_lr(cur_step, warmup_steps, d_model):\n    lrate = (d_model**-0.5) * min(cur_step**-0.5, cur_step * (warmup_steps**-1.5))\n    return lrate",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#loss-function",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#loss-function",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.4 Loss Function",
    "text": "3.4 Loss Function\n在Transformer模型的训练过程中，通常使用交叉熵损失函数（Cross Entropy Loss）作为主要的损失函数。交叉熵损失函数用于衡量模型预测的概率分布与真实标签分布之间的差异，具体来说，给定一个包含 \\(N\\) 个样本的训练集，每个样本的真实标签为 \\(y_i\\)，模型预测的概率分布为 \\(\\hat{y}_i\\)，交叉熵损失函数的计算公式如下:\n\\[\n\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n\\tag{34}\\]\n其中，\\(C\\) 是类别的总数，\\(y_{i,c}\\) 是样本 \\(i\\) 在类别 \\(c\\) 上的真实标签（one-hot encoding），\\(\\hat{y}_{i,c}\\) 是模型对样本 \\(i\\) 在类别 \\(c\\) 上的预测概率。\n结合Label Smoothing Equation 31, 交叉熵损失函数的计算公式可以调整为:\n\\[\n\\mathcal{L}_{smooth} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c}^{smooth} \\log(\\hat{y}_{i,c})\n\\tag{35}\\] 其中，\\(y_{i,c}^{smooth}\\) 是经过Label Smoothing处理后的目标标签。\ndef cross_entropy_loss(logits, labels, ignore_index=0, label_smoothing=0.0):\n    # Create mask for ignore_index\n    mask = labels != ignore_index\n    num_classes = logits.size(-1)\n\n    if label_smoothing &gt; 0.0:\n        smooth_labels = F.one_hot(labels, num_classes).float()\n        smooth_labels = smooth_labels * (1 - label_smoothing) + label_smoothing / num_classes\n    else:\n        smooth_labels = F.one_hot(labels, num_classes).float()\n\n    log_probs = F.log_softmax(logits, dim=-1)\n    loss = -torch.sum(smooth_labels * log_probs, dim=-1)\n    loss = loss * mask.float()\n    loss = loss.sum() / mask.sum()\n\n    return loss",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#evaluation-metric",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#evaluation-metric",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.5 Evaluation Metric",
    "text": "3.5 Evaluation Metric\n\n3.5.1 BLUE\n在评估Transformer模型的性能时，通常使用BLEU（Bilingual Evaluation Understudy）分数作为主要的评价指标。BLEU分数是一种用于评估机器翻译质量的自动化指标，通过比较机器生成的翻译与一个或多个参考翻译之间的相似度来衡量翻译的准确性。BLEU分数的计算过程包括以下几个步骤:\n\nN-gram匹配: 计算机器翻译输出与参考翻译之间的n-gram匹配数量，通常考虑1-gram到4-gram。\n精确率计算: 对每个n-gram，计算匹配的n-gram数量与机器翻译输出中n-gram总数的比值，得到精确率。\n几何平均: 将各个n-gram的精确率进行几何平均，以综合考虑不同长度的n-gram匹配情况。\n长度惩罚: 为了防止机器翻译输出过短，BLEU分数引入了长度惩罚项，根据机器翻译输出的长度与参考翻译的长度进行调整。\n最终计算: 将几何平均的精确率与长度惩罚相乘，得到最终的BLEU分数，范围在0到1之间，通常表示为百分比形式。\n\n计算BLEU分数的公式如下: \\[\n\\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)\n\\tag{36}\\]\n其中，\\(\\text{BP}\\) 是长度惩罚项，\\(p_n\\) 是n-gram的精确率，\\(w_n\\) 是n-gram的权重，通常均匀分配。\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\ndef compute_bleu(reference, candidate):\n    reference_tokens = [reference.split()]\n    candidate_tokens = candidate.split()\n    smoothing_function = SmoothingFunction().method1\n    bleu_score = sentence_bleu(reference_tokens, candidate_tokens,\n                               weights=(0.25, 0.25, 0.25, 0.25),\n                               smoothing_function=smoothing_function)\n    return bleu_score * 100  # Convert to percentage\n\n\n3.5.2 Perplexity\n困惑度（Perplexity）是评估语言模型性能的常用指标，用于衡量模型对给定文本序列的预测能力。困惑度的定义是语言模型对测试集上每个词的平均不确定性，数值越低表示模型对文本的预测越准确。具体来说，给定一个测试集 \\(W = w_1, w_2, \\ldots, w_N\\)，语言模型计算该序列的概率 \\(P(W)\\)，困惑度的计算公式如下:\n\\[\n\\text{Perplexity}(W) = P(W)^{-\\frac{1}{N}} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, w_2, \\ldots, w_{i-1})\\right)\n\\tag{37}\\]\n其中，\\(N\\) 是测试集中的词数，\\(P(w_i | w_1, w_2, \\ldots, w_{i-1})\\) 是语言模型预测第 \\(i\\) 个词的条件概率。困惑度可以理解为模型在预测下一个词时面临的选择数量的指数级增长。\n在实际计算中，困惑度通常通过交叉熵损失来间接计算: \\[\n\\text{Perplexity}(W) = \\exp(\\text{CrossEntropyLoss})\n\\tag{38}\\]\n在之前，我们有提到Label Smoothing, 它会影响困惑度的计算，因为Label Smoothing会改变目标分布，从而影响交叉熵损失的计算，进而影响困惑度的数值。因此，在使用Label Smoothing时，困惑度的数值可能会有所偏差，需要谨慎解释。\n\n\n\n\n\n\nFigure 10: 从图中可以看出，随着 \\(\\epsilon\\) 的增加，Perplexity 也在增加。这是因为 Label Smoothing 引入了噪声，使得模型在训练过程中更难以准确预测目标词，从而导致交叉熵损失增加。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#training",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#training",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.6 Training",
    "text": "3.6 Training\n在训练Transformer模型，我们设置了以下超参数:\n@dataclass\nclass ModelConfig:\n    vocab_size: int = VOCAB_SIZE\n    max_seq_len: int = 128\n\n    d_model: int = 512\n    d_ff: int = 2048\n    num_heads: int = 8\n    num_layers: int = 6\n    dropout: float = 0.1\n\n@dataclass\nclass TrainConfig:\n    batch_size: int = 256\n    gradient_steps: int = 8\n    total_steps: int = 10_000  # set to 0 for automatic calculation\n    warmup_steps: int = 1000  # will be set by total_steps // 10\n\n    lr: float = 5e-3\n    min_lr: float = 1e-5\n    betas: tuple[float, float] = field(default_factory=lambda: (0.9, 0.98))\n    weight_decay: float = 0.01\n    optim_eps: float = 1e-9\n\n    label_smoothing: float = 0.1\n\n    debug: bool = False\n    device = get_device()\n    mixed_precision: bool = True\n    eval_steps: int = 100\n在训练过程中，我们使用了Mixed Precision Training来加速训练过程并减少显存占用。混合精度训练通过在计算过程中使用16位浮点数（FP16）和32位浮点数（FP32）的组合，既保持了模型的精度，又提高了计算效率。具体来说，模型的前向传播和反向传播主要使用FP16进行计算，而关键的参数更新和梯度累积则使用FP32，以确保数值稳定性。\n同时，我们还采用了Gradient Accumulation技术，以便在显存有限的情况下使用较大的有效批量大小进行训练。梯度累积的基本思想是将多个小批量的梯度累积起来，然后再进行一次参数更新。具体来说，假设我们希望使用一个较大的批量大小 \\(B\\) 进行训练，但由于显存限制，我们只能使用一个较小的批量大小 \\(b\\)，那么我们可以将 \\(B/b\\) 个小批量的梯度累积起来，然后再进行一次参数更新。\n简单来看，我们的训练循环如下:\nfor batch in dataloader:\n    optimizer.zero_grad()\n    for micro_step in range(gradient_steps):\n        with torch.autocast(\n            device_type=train_config.device.type, enabled=train_config.mixed_precision, dtype=torch.bfloat16\n        ):\n            logits = model(**batch)\n            loss = cross_entropy_loss(\n                logits,\n                labels,\n                ignore_index=translation_dataset.pad_id,\n                label_smoothing=train_config.label_smoothing,\n            )\n            loss /= train_config.gradient_steps\n        loss.backward()\n    \n    optimizer.update_lr()\n    optimizer.step()\n\n\nNOTE: OOM Error\n\n\n如果大家在训练的过程中，遇到了OOM Error，我们可以调小我们的Batch Size，同时增大我们的Gradient Steps，这样可以保持最终的Batch Size不变。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/01-transformer/Transformer.html#results",
    "href": "posts/100-AI-Papers/01-transformer/Transformer.html#results",
    "title": "01: Attention is All You Need (Transformer)",
    "section": "3.7 Results",
    "text": "3.7 Results\n\n\n\n\n\n\nFigure 11: The Loss Curve of Transformer training for 10,000 steps\n\n\n\n有一个有趣的现象就是，Loss的下降呈现 zig-zag pattern，这个现象在很多NLP模型的训练中都会出现，但是我还没有找到一个很好的解释，可能是Learning Rate 调节的原因，也可能是Dataloader的问题，有时间我就探索这个问题的。欢迎大家在评论区留言讨论！\n下面是训练完10,000步后一个翻译的例子:\nEnglish Input: Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow\nchallenge.\nModel Output: &lt;s&gt; 几年前,在TED, Peter Skillman介绍了一个设计挑战  叫做杨饼干的挑战-- &lt;/s&gt;\nReference: 几年前，在这里的 TED 上，Peter Skillman 提出了一个名为“棉花糖挑战”的设计挑战。\n一些简单的例子\nEnglish Input: Who are you?\nModel Output: &lt;s&gt; 你是谁?&lt;/s&gt;\nEnglish Input: What is your name?\nModel Output: &lt;s&gt; 你叫什么?&lt;/s&gt;\nEnglish Input: I love Artificial Intelligence.\nModel Output: &lt;s&gt; 我喜欢魅力。&lt;/s&gt;\n可以看到，我们的模型已经能够进行简单的英文到中文的翻译了，当然距离实际应用还有很大的差距，比如翻译的流畅度和准确度还需要提升，模型的规模也需要更大，训练的数据也需要更多等等。",
    "crumbs": [
      "100 AI Papers",
      "Transformer"
    ]
  },
  {
    "objectID": "posts/100-AI-Papers/05-vivit/ViViT.html#experiment",
    "href": "posts/100-AI-Papers/05-vivit/ViViT.html#experiment",
    "title": "05: ViViT: A Video Vision Transformer(ViViT)",
    "section": "2.1 Experiment",
    "text": "2.1 Experiment"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "",
    "text": "# Preliminary"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#window-multi-head-attention-w-mha",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.1 Window Multi-Head-Attention (W-MHA)",
    "text": "1.1 Window Multi-Head-Attention (W-MHA)\nW-MHA 的核心思想是：\n\n把图像划分成固定大小的窗口（window），比如 7×7 patch 的窗口。\n在窗口内的 token 之间做局部自注意力，而不是在整张图像的所有 token 之间做全局注意力。\n每个窗口独立计算 Multi-Head Attention → 降低计算量，并且我们可以并行的计算\n\n这样一来：\n\n单个窗口 token 数量固定 = \\(M^{2}\\)（如 7×7=49）。\n注意力计算复杂度从 \\(\\mathcal{O}((hw)^{2}C)\\) 降低为 \\(\\mathcal{O}(M^{2}hwC)\\)，其中 \\(M \\ll \\sqrt{ N }\\)。\n\n除了降低计算复杂度之外，W-MHA，还有保留CNN 在图像处理中强大的一点是 局部感受野 和 平移不变性。\n\nW-MHA 通过窗口限制，使得注意力机制也具备类似的局部归纳偏置（inductive bias），适合图像建模。\n\n\nFor efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#shifted-window-multi-head-attention-sw-mha",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)",
    "text": "1.2 Shifted Window Multi-Head-Attention (SW-MHA)\nW-MHA 很好，但是它存在的一个问题就是：\n\n窗口之间是相互独立的，缺少跨窗口的信息交流。这会导致，模型只能看见局部，不能获得全局的信息。\n\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n为了解决这个问题，Swin- Transformer提出来 Shifted Window Mulit-Head-Attention (SW-MHA) 窗口位置相对前一层平移，比如 7×7 窗口 → 平移 3 个 patch。 这样，新的窗口会跨越原来的边界，token 会和相邻窗口的 token 一起计算注意力。 相当于强制跨窗口交互，让信息可以在不同区域之间流动。\n 如上如所示，我们将Window通过向左上角移动，通过给图片增加Padding来，但是这种办法显然会增加计算的复杂度。Swin Transformer用了一种很聪明的办法，叫做 Cycling Shift，这种方法就是将将一个张量或图像在某个维度上做 平移，但不是把移出去的部分丢掉，而是 重新从另一边补回来。就像“环形队列”或“钟表走一圈又回到起点”。 如下图所示 \n可以看到，通过Cycling Shift，我们得到的每个window的内容，和之前是一样的，但是所需要的Window的数量，小了很多，这也就意味着，所需要的时间复杂度，也小了很多。\n\n不过Cycling Shift也有一个问题，就是同一个窗口里面，可能有来自不同图片的信息，这些信息在原图片上不是相邻的，自然不应该相互交流信息。我们可以将图片，抽象成下图的形式。组织Attention交流，很自然的一种方法是利用Mask，就像Transformer里的Causal Mask一样。但是，这个Mask长什么样子呢\n\n我们可以看一下Mask，如下图所示，有颜色的区域表示Mask == 1， 在此为了更好的"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#consecutive-swin-transformer-block",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.3 Consecutive Swin Transformer Block",
    "text": "1.3 Consecutive Swin Transformer Block\n\nSwin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.4 \n\n\n\\[\n\\begin{split}\n\\hat{z}^l &= \\text{W-MSA} \\left( \\text{LN} \\left( z^{l-1} \\right) \\right) + z^{l-1} \\\\\nz^l &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^l \\right) \\right) + \\hat{z}^l \\\\\n\\hat{z}^{l+1} &= \\text{SW-MSA} \\left( \\text{LN} \\left( z^l \\right) \\right) + z^l \\\\\nz^{l+1} &= \\text{MLP} \\left( \\text{LN} \\left( \\hat{z}^{l+1} \\right) \\right) + \\hat{z}^{l+1}\n\\end{split}\n\\]\n将W-MSA 和 SW-MSA叠在一起，就得到了Transformer Block，当然，还有一个MLP，Layer Normalization，在此就不赘述了。"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#patch-merge",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#patch-merge",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.4 Patch Merge",
    "text": "1.4 Patch Merge\n讲完了W-MHA，和SW-MHA，我们就理解了Swin- Transformer中最难理解，也是最终的部分，接下来我们看看其他简单的部分。 Patch Merge , 图中绿色的部分，逐步降低 token 数量（降采样），同时增加特征维度的操作。这类似于CNN中的操作，随着层数的增加，分辨率逐步降低、通道数逐步增加，这样既减少了计算量，又能提取层级特征。具体的实现：\n\n分组：将相邻的 2×2 patch 合并成一个新的 patch。\n\n假设输入特征大小为 (H, W, C)。\n每 2×2 的 patch → 合并为 1 个新 token。\n新特征图大小变为 (H/2, W/2, 4C)。\n\n线性变换:\n\n将合并后的 4C 维特征通过一个 线性层 (Linear Projection)，降到 2C 维。\n输出维度翻倍（2C），以补偿分辨率减半带来的信息损失。 🔹 为什么提出 Patch Merging\n\n\n分层表示 (Hierarchical Representation) • 模仿 CNN 的金字塔结构，从局部细节逐步聚合到全局语义。 • 有利于下游任务（检测、分割）中不同尺度的目标建模。\n计算效率 • token 数量逐层减少 → Attention 的复杂度大幅下降。 • 保证模型可扩展到大分辨率图像。\n语义信息聚合 • 通过合并相邻 patch，模型能把更大感受野的信息整合到新的 token 中。\n\n\nx = x.view(B, H, W, C)\n\nx0 = x[:, 0::2, 0::2, :]  # (B, H/2, W/2, C)\nx1 = x[:, 1::2, 0::2, :]  # (B, H/2, W/2, C)\nx2 = x[:, 0::2, 1::2, :]  # (B, H/2, W/2, C)\nx3 = x[:, 1::2, 1::2, :]  # (B, H/2, W/2, C)\n\nx = torch.cat([x0, x1, x2, x3], -1)  # (B, H/2, W/2, 4*C)\nx = x.view(B, -1, 4 * C)  # (B, H/2*W/2, 4*C)\n\nx = self.reduction(x)  # (B, H/2*W/2, 2*C)"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#relative-position-encoding",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.5 Relative Position Encoding",
    "text": "1.5 Relative Position Encoding\n与Transformer 和 Vision-Transformer 中不同的是，Swin Transformer利用的是Relative Position Encoding。\n\\[\n\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} +B\\right) V\n\\]\n1.  定义偏置表 (relative_position_bias_table)\n•   大小是 (2*Wh-1) * (2*Ww-1, num_heads)\n•   意味着窗口内的任意两个 token 的相对位置 (dx, dy)，都有一个可学习的偏置值（每个 head 一份）。\n•   例如窗口是 7×7 → 相对位置范围是 [-6,6]，所以表大小是 13×13=169，每个位置存一组偏置\n\n\n2.  计算相对位置索引 (relative_position_index)\n•   首先生成窗口内每个 token 的坐标。\n•   然后做差，得到任意两个 token 的相对坐标 (dx, dy)。\n•   再映射成表的索引（通过移位和哈希成一个整数 index）。\n•   结果是一个 (Wh*Ww, Wh*Ww) 的矩阵，每个元素存两个 token 之间在 bias 表里的索引。\n\n\n•   在图像里，相对位置比绝对位置更重要：\n•   比如一个像素的左邻和右邻很相似，无论这个像素在图像的哪个地方。\n\\[\n\\begin{tabular}\n\\Xhline{1.0pt}\n& \\multicolumn{2}{c|}{ImageNet} & \\multicolumn{2}{c|}{COCO} & \\multicolumn{1}{c}{ADE20k} \\\\\n& top-1 & top-5  & AP$^\\text{box}$ & AP$^\\text{mask}$ & mIoU \\\\\n\\hline\nno pos. & 80.1 & 94.9 & 49.2 & 42.6  & 43.8 \\\\\nabs. pos. & 80.5 & 95.2 & 49.0 & 42.4  & 43.2 \\\\\nabs.+rel. pos. & 81.3 & 95.6 & 50.2 & 43.4 & 44.0\\\\\nrel. pos. w/o app. & 79.3 & 94.7 & 48.2 & 41.9 & 44.1 \\\\\nrel. pos. & \\textbf{81.3} & \\textbf{95.6} & \\textbf{50.5} & \\textbf{43.7} & \\textbf{46.1} \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n\\]\n\n1.5.1 Fine-Tuning in different image size\n和 Vision-Transformer 一样，当输入的图片和训练时不一样，我们可以通过 bi-cubic interpolation 来增大Relative Position\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\\[\n\\begin{table}\n    \\centering\n\\small\n\\addtolength{\\tabcolsep}{-4.0pt}\n\\begin{tabular}\n\\Xhline{1.0pt}\n\\multirow{2}{*}{method} & \\multicolumn{4}{c|}{MSA in a stage (ms)} & \\multicolumn{3}{c}{Arch. (FPS)} \\\\\n& S1 & S2 & S3 & S4 & T & S & B \\\\\n\\hline\nsliding window (naive) & 122.5 & 38.3 & 12.1 & 7.6 & 183 & 109 & 77 \\\\\nsliding window (kernel)  & 7.6 & 4.7 & 2.7 & 1.8 & 488 & 283 & 187 \\\\\n\\hline\nPerformer~\\cite{choromanski2020performer} & 4.8 & 2.8 & 1.8 & 1.5 & 638 & 370 & 241 \\\\\n\\hline\nwindow (w/o shifting) & 2.8 & 1.7 & 1.2 & 0.9 & 770 & 444 & 280 \\\\\n\\hline\nshifted window (padding) & 3.3 & 2.3 & 1.9 & 2.2 & 670 & 371 & 236 \\\\\nshifted window (cyclic)  & 3.0 & 1.9 & 1.3 & 1.0 & 755 & 437 & 278 \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n    \\caption{Real speed of different self-attention computation methods and implementations on a V100 GPU. }\n    \\label{tab:ablation-selfatt-efficient}\n\\normalsize\n\\end{table}\n\\]"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#others",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#others",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.6 Others",
    "text": "1.6 Others\n除了以上几个，Swin Transformer 中还有其他Component，比如 ：\n\nPatch Embedding\nLinear Projection\nFeedForward\nLayer Normalization 在此，就不赘述了，有需要的同学，请参考前一篇 Vision-Transformer， 或者 Transformer"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#downstream-tasks",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.7 Downstream Tasks",
    "text": "1.7 Downstream Tasks\n当一场图片传入Swin Transformer， 它可以提取出图片的特征。 \\[\n\\mathrm{z} = f_{\\theta}(\\mathrm{x}), \\quad \\text{where}\\ \\mathrm{x} \\in \\mathbb{R}^{3\\times H \\times W}, \\mathrm{z} \\in \\mathbb{R}^{H'W' \\times C}\n\\]\n一张图片转化成了 \\(H'W'\\) 个特征，每个特征的大小为 $C。\nSwin Transformer 可以有当作基本的backbone，在此基础上，我们可以对下游进行不同的任务，比如：\n\nImage Classification\nObject Detection\nSemantic segmentation\n\n接下来，我们将如何用Swin Transformer在不同的任务中\n\n\n1.7.1 Image Classification\n对于 \\(\\mathrm{z}\\) 的 hidden states，我们可以进行一个Average Pooling，对于每一个特征求均值，然后再将这个传入一个分类头，就可以得到我们Classification了。与 Vision-Transformer 不同的是，Swin Transformer 没有 [CLS] token 来当收集全部的信息。"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#object-detection-semantic-segmentation",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.8 Object Detection & Semantic segmentation",
    "text": "1.8 Object Detection & Semantic segmentation\nBackbone (Swin Transformer)：\n\nStage 1: [N, C1, H/4, W/4]\nStage 2: [N, C2, H/8, W/8]\nStage 3: [N, C3, H/16, W/16]\nStage 4: [N, C4, H/32, W/32]\n\n可以得到 FPN(Lin et al. 2017) to create a pyramid of feature maps suitable for detection.\n{P2: [N, C, H/4, W/4], \n P3: [N, C, H/8, W/8], \n P4: [N, C, H/16, W/16], \n P5: [N, C, H/32, W/32]}\n有了这些FPN 之后，我们可以结合不同的算法，来进行不同的任务，比如\n例子 1：目标检测 (Object Detection)\n以 Swin Transformer + Faster R-CNN (Ren et al. 2016) 为例： 1. 输入图像：一张 800×1333 的 COCO 数据集图片。\n3.  FPN (特征金字塔网络)：将多尺度特征融合，形成统一的金字塔特征。\n4.  RPN (Region Proposal Network)：在特征图上生成候选区域。\n5.  RoI Head：对候选区域进行分类 (车、人、狗…) 和边框回归。\n6.  输出：预测结果，例如：\n•   “一辆车” → 边框 (x1,y1,x2,y2) + 类别 “car”\n•   “一个人” → 边框 + 类别 “person”\n 👉 在 COCO 数据集上，Swin-T + Faster R-CNN比 ResNet-50 + Faster R-CNN 的 mAP 提高约 5~6 个点。\n语义分割 (Semantic Segmentation)  以 Swin Transformer + UPerNet(Xiao et al. 2018)为例： 1. 输入图像：一张 512×512 的 ADE20K 数据集图片。 2. Backbone (Swin Transformer)：同样输出 1/4, 1/8, 1/16, 1/32 四个尺度特征。 3. FPN/UPerNet Head： • 将多层特征融合，对应不同语义层级。 • 利用融合后的特征生成像素级预测。 4. 预测图 (segmentation map)：大小 512×512，每个像素属于一个类别。 • [0,0] 像素 → “sky” • [100,150] 像素 → “building” • [200,300] 像素 → “road” 5. 输出：完整的语义分割图，每个像素都有类别标签。\n👉 在 ADE20K 上，Swin-L + UPerNet 的 mIoU 达到 53.5+，比传统 CNN backbone 提升显著。 具体的实现细节，等到以后我们阅读到关于Segmentation的内容在，再来实现"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#training-details",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#training-details",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.9 Training Details",
    "text": "1.9 Training Details\n\nWe employ an AdamW optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of in training, except for repeated augmentation and EMA, which do not enhance performance.  Swin Transformer Hierarchical Vision Transformer using Shifted Windows, p.5 \n\n\n1.9.1 DropPath\n论文中还用到了 DropPath 来当作一种 Regularization。 DropPath 也称之为 Stochastic Depth (Huang et al. 2016) , 它是一种应用在Residual Network， 在训练过程中，随机丢弃整个 残差分支 (residual branch) 或 整个路径 (path)。减少过拟合，同时让模型学会依赖不同深度的路径，提升训练稳定性。 \n与Dropout 不同的是， Dropout 丢弃的是单个神经元的输出， 而DropPath 丢弃的是整个残差分支 / 整层 Block\n\n\n\n\n\n\n\n\n特性\nDropout (经典)\nDropPath (Stochastic Depth)\n\n\n\n\n丢弃对象\n单个神经元的输出\n整个残差分支 / 整层 Block\n\n\n应用粒度\n逐元素 (element-wise)\n层级 (layer-wise)\n\n\n使用场景\n全连接层、CNN、RNN 等\n残差网络、Transformer 等\n\n\n推理阶段效果\n不丢弃，使用缩放补偿\n不丢弃，保留完整路径\n\n\n作用\n减少神经元过拟合\n防止深层网络过拟合、提升稳定性\n\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n        \n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(x)\n        \n        shortcut = x \n        ... # FFN\n        x = shortcut + self.drop_path(x)\n        \n        ...\n        \n        return x \n\n📝 TAKEAWAY DropPath（也叫 Stochastic Depth）是一种正则化方法，它在训练时随机跳过（丢弃）整个网络层或分支的计算，以减少过拟合并提高模型的泛化能力。\n\n\n\n1.9.2 Gradient Checkpoint\n在此，我们在介绍一个训练方法，用于加速训练，叫做Gradient Checkpoint又叫做Activation Checkpoint， 用PyTorh实现，是很容易的 的，我们只需要call utils.checkpoint\n正常训练流程： 在前向传播（forward）时，每一层的中间激活值（activation）都会保存下来，以便反向传播（backward）时用来计算梯度。 问题是：保存所有中间激活值会消耗大量显存（GPU memory）。 • Gradient Checkpoint 的思路： 并不是保存所有激活值，而是只在部分关键节点（checkpoint）保存激活。 对于未保存的激活值，在反向传播时重新再跑一次前向计算来得到它们，从而节省显存。\n换句话说：用计算换显存。\n🔹 工作机制 1. 在前向传播时： • 模型被切分成若干块（segments）。 • 只保存每一块的输入，丢弃中间的激活。 2. 在反向传播时： • 需要用到梯度时，重新对那一块做一次 forward 来恢复激活。 • 然后正常计算梯度。\n•   增加计算开销：因为要在 backward 时重新做一次 forward。\n•   一般会带来 20%～30% 额外的训练时间。\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        def custom_forward(*inputs):\n            return self.layer2(self.layer1(*inputs))\n        \n        # 对这部分使用 checkpoint\n        x = checkpoint(custom_forward, x)\n        return x\n\n📝 TAKEAWAY Gradient Checkpointing 是一种 用额外计算换显存 的方法，通过在前向传播时少存激活，反向传播时重算，能让大模型在有限显存下完成训练。"
  },
  {
    "objectID": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#swin-v2",
    "href": "posts/100-AI-Papers/04-swin-transformer/Swin-Transformer.html#swin-v2",
    "title": "04: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer)",
    "section": "1.10 Swin V2",
    "text": "1.10 Swin V2\n“Swin V2” (Liu et al. 2022) 是在原始 Swin Transformer 的基础上，为了更好地 扩展模型容量（更多参数）、处理高分辨率输入 以及 提高训练稳定性 所做的一系列改进。 在视觉任务中，Transformer 模型若要变得更强（更多参数、更高分辨率输入、更多层数）就会遇到几个挑战： 1. 训练不稳定：随着模型变深、通道变宽，内部激活的幅度可能急剧增长，导致梯度、数值不稳定。 2. 分辨率迁移问题：模型在低分辨率下预训练（例如 224×224）后，用在高分辨率（例如 1,536×1,536）或不同窗口尺寸时表现会下降。 3. 对标注数据的过度依赖：大模型需要大量标注数据才能训练得好。\nSwin V2 就是为了克服这些障碍，支持训练超大模型（如 30 亿参数级别），同时能处理大尺寸输入 \n\n1.10.1 Post normalization\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self):\n        ...\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()\n        \n    \n    def forward(self, x):\n        shortcut = x \n        ... # Attention \n        x = shortcut + self.drop_path(self.norm1(x))\n        \n        shortcut = x \n        ... # FFN\n        x = x + self.drop_path(self.norm2(self.mlp(x)))\n        \n        ...\n        \n        return x \n\n\n1.10.2 Scaled cosine attention\nclass WindowAttention:\n    def __init__(self,):\n    \n        ...\n        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n        ...\n    \n    \n    def forward(self, x):\n        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n        attn = attn * logit_scale\n\n\n1.10.3 Log-spaced Continuous Position Bias(Log-CPB)\nlog-spaced continuous position bias approach to address the issue in transferring across window resolutions\n\\[\n\\begin{split}\n\\widehat{\\Delta x} &= \\operatorname{sign}(x) \\cdot \\log(1 + |\\Delta x|) \\\\\n\\widehat{\\Delta y} &= \\operatorname{sign}(y) \\cdot \\log(1 + |\\Delta y|)\n\\end{split}\n\\]\nself.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n                             nn.ReLU(inplace=True),\n                             nn.Linear(512, num_heads, bias=False))\n                             \ndef forward(self, x):\n    relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)"
  },
  {
    "objectID": "posts/CS336/Lecture04/lec04.html",
    "href": "posts/CS336/Lecture04/lec04.html",
    "title": "Lecture 04: Introduction to MoE",
    "section": "",
    "text": "随着2025年春节DeepSeek-R1 的发布，Mixture of Experts (MoE) 模型在自然语言处理领域重新引起了广泛关注。这节课我们将会学习什么的MoE Layer。它的基本原理是什么？它是如何工作的？以及它为什么能够提升模型的性能。\n我们首先来了解一下，什么是Mixture of Experts (MoE) 模型，并且为什么它受到了如此多的关注。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 04: MoE Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture04/lec04.html#routing-function",
    "href": "posts/CS336/Lecture04/lec04.html#routing-function",
    "title": "Lecture 04: Introduction to MoE",
    "section": "2.1 Routing Function",
    "text": "2.1 Routing Function\nMoE 的核心不是“有很多专家”，而是每个 token 该去哪些专家。这个决策由 Router（路由器）完成。我们可以把它理解成一个“轻量的分类器/打分器”：输入是每个 token 的 hidden state，输出是对所有专家的偏好分数，然后选出 top-K 个专家执行。有一点很重要的是：\n\nRouter 是具有上下文感知能力的，也就是说它会根据 token 的内容动态决定路由结果，也就是说同一个 token 在不同语境下可以被送去不同专家。\n\nRouter实现大概有3种思路：\n\nToken-choice（token 选专家）： 每个 token 给所有专家打分，选择 top-K 专家处理它（现代主流）\nExpert-choice（专家选 token）： 每个专家从一批 token 里挑 top-K 个来处理（天然更均衡）\nGlobal assignment（全局分配）：把 token–expert 匹配视作优化问题（如线性分配/最优传输），追求更均衡或更低通信成本\n\n\n\n\n\n\n\nFigure 5: 不同的Router设计思路比较。\n\n\n\n在实际应用中，Token-choice 是目前最主流的设计思路，因为它实现简单且易于扩展。下面我们来看一下Token-choice Router的具体实现。\n\n2.1.1 Token-choice Router\nToken-choice Router, 顾名思义，就是每个 token 给所有专家打分，选择 top-K 专家处理它。当然，这个“打分”过程（Routing）可以有很多种实现方式，比如：\n\nTop-K Gating：使用一个线性层对 token 的 hidden state 进行投影，得到每个专家的分数，然后选择 top-K 个专家。\nHashing-based Routing：使用哈希函数将 token 映射到专家，从而实现路由（通常作为Baseline）。\nRL to learn Routing：使用强化学习方法来学习路由策略。\nSolve a Optimization Problem：将路由问题视作一个优化问题，通过求解该问题来确定路由结果。\n\n下图展示了不同的Token-choice Router实现方式。\n\n\n\n\n\n\n\n\n\n\n\n(a) Top-K routing choice\n\n\n\n\n\n\n\n\n\n\n\n(b) Hashing-based routing（通常作为Baseline）\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) RL-Based Routing\n\n\n\n\n\n\n\n\n\n\n\n(d) Solve a Optimization Problem\n\n\n\n\n\n\n\nFigure 6\n\n\n\n在实际应用中，Top-K Gating 是目前最常用的 Token-choice Router 实现方式，因为它简单且高效。下面我们来看一下 Top-K Gating 的具体实现细节。\n\n2.1.1.1 Top-K Gating\nTop-K Gating 的实现步骤如下：\n\nScore Calculation：对于每个 token 的 hidden state \\(h_i\\)，通过一个线性层计算每个专家的分数： \\[\ns_{i,j} = W_g h_i + b_g\n\\tag{1}\\] 其中，\\(W_g\\) 和 \\(b_g\\) 是路由器的参数，\\(s_{i,j}\\) 是 token \\(i\\) 对专家 \\(j\\) 的分数。\n\n有了score之后，接下来我们需要选择 top-K 个专家，不过在此之前，我们通常会对score进行归一化处理，以便更好地比较不同专家的分数。常用的方法是使用softmax函数： \\[\np_{i,j} = \\frac{exp(s_{i,j})}{\\sum_{k} exp(s_{i,k})}\n\\tag{2}\\] 其中，\\(p_{i,j}\\) 是 token \\(i\\) 对专家 \\(j\\) 的归一化分数。\n\nTop-K Selection：对于每个 token，选择分数最高的 K 个专家： \\[\ng_{i, j} = \\begin{cases}\ns_{i, j}, \\quad s_{i, j} \\in \\text{Top-K}(s_i) \\\\\n0, \\quad \\text{otherwise}\n\\end{cases}\n\\tag{3}\\] 其中，\\(g_{i,j}\\) 是 token \\(i\\) 对专家 \\(j\\) 的选择结果。\nPassing to Experts：将 token 送入选择的专家进行处理。每个专家只处理被选中的 token，其余 token 被忽略。\nCombining Outputs：将专家的输出进行合并，得到最终的 token 表示。通常使用加权平均的方式： \\[\nh_i' = \\sum_{j} g_{i,j} \\text{Expert}_j(h_i) + h_i\n\\tag{4}\\] 其中，\\(h_i'\\) 是 token \\(i\\) 的最终表示，\\(\\text{Expert}_j(h_i)\\) 是专家 \\(j\\) 对 token \\(i\\) 的处理结果, \\(h_i\\) 是 token \\(i\\) 的原始表示(Residual Connection), \\(g_{i,j}\\) 是 token \\(i\\) 对专家 \\(j\\) 的选择结果。\n\n通过以上步骤，Top-K Gating 实现了对 token 的动态路由，使得每个 token 只经过少数几个专家，从而实现了 MoE 的高效计算。\n\n\n2.1.1.2 Top-K Variants\n在实际应用中，Top-K Gating 有一些变体，其中比较常见的是DeepSeek提出的Top-K变形(Dai et al. 2024), 它提出，在选择Top-K专家的基础上，每个token还会固定传入一个Shared Expert。这样做的好处是，可以确保每个token至少有一个专家能够处理它。 并且Expert的大小变小，但是数量变多（fine-grained Experts），从而提升模型的表达能力。\n\n\n\n\n\n\nFigure 7: DeepSeek提出的Top-K变形，每个token除了选择Top-K专家外，还会固定传入一个Shared Expert。\n\n\n\n这种方式，在之后的实验中也被证明是有效的。 Qwen3模型(Yang et al. 2025) 也采用了类似的设计。\n\n\n\n\n\n\nFigure 8: 从这种消融实验中可以看出，在experts变多的情况下，加入Shared Expert能够提升模型的性能。\n\n\n\n\n\n2.1.1.3 Choice of K\n我们了解了Top-K Gating的实现细节，接下来我们来看一下K值的选择对模型性能的影响。 K值的选择对MoE模型的性能有显著影响。一般来说，较小的K值可以减少计算量，但可能会限制模型的表达能力；而较大的K值则可以提升模型的表达能力，但会增加计算量。因此，在实际的应用中，通常使用 K=1 或 K=2 作为默认选择。\n\n\n\n\n\n\nK &gt; 1\n\n\n\n课堂中有提到，K &gt; 1 时。我们可以把它想象成Bandit的问题（熟悉Reinforcement Learning的同学应该了解）。每个token在选择专家时，就像是在玩一个多臂老虎机（Multi-armed Bandit），每次选择K个专家进行尝试，从而获得更好的奖励（模型性能）。这也是RL中探索与利用（Exploration vs. Exploitation）问题的一个体现。\n\n\n\n\n2.1.1.4 Is Top-K Gating Optimal?\n我们了解了Top-K Gating的实现，那么它是不是最优的呢？其实也不尽然。Lecture中有提到一个比较有趣的观察：即使 router 很弱（比如基于 hashing 的确定性映射），很多时候也能比 dense 更强。有一种合理的解释是：只要映射是确定性的，每个专家仍会长期看到某个子分布，从而形成某种“专门化”（不一定是你以为的语义专门化，可能是频率/模式上的专门化）。因此，Router并不一定要设计的很复杂，简单有效即可，这也就是为什么Top-K Gating能这么受欢迎的原因之一。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 04: MoE Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture04/lec04.html#experts",
    "href": "posts/CS336/Lecture04/lec04.html#experts",
    "title": "Lecture 04: Introduction to MoE",
    "section": "2.2 Experts",
    "text": "2.2 Experts\n在MoE模型中，Experts是多个独立的FFN Layer，每个Expert负责处理一部分token。每个Expert通常由一个前馈神经网络（Feed-Forward Neural Network, FFN）组成，结构与传统的Transformer中的FFN类似，但参数是独立的。那么，Experts的中间层的维度应该如何选择呢？ 一般来说，Experts的中间层维度通常设置为输入维度的4倍（即\\(4d_{model}\\)），和传统的FFN Layer保持一致。这是因为较大的中间层维度可以提升模型的表达能力，从而提升整体性能。 但是，在Figure 7中，我们看到DeepSeek介绍了Fine-Grained Experts的概念，即通过增加Experts的数量，同时减少每个Expert的中间层维度，从而提升模型的表达能力。在DeepSeek MoE 中(Dai et al. 2024), 每个Expert的中间层维度被设置为\\(\\frac{1}{4}d_{ff}\\). 这样做的好处是，可以让模型拥有更多的专家，从而提升模型的多样性和表达能力。\n\nDeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the size of a standard FFN. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models, p.9",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 04: MoE Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture04/lec04.html#training-objectives",
    "href": "posts/CS336/Lecture04/lec04.html#training-objectives",
    "title": "Lecture 04: Introduction to MoE",
    "section": "2.3 Training Objectives",
    "text": "2.3 Training Objectives\n如果说 Routing Function 解决的是“每个 token 去哪些专家”，那 Training Objectives 解决的是更现实的问题：“每个专家到底学什么”。在 MoE 里，单纯靠主任务 loss(Next token Prediction Loss) 往往不够.如果你只用语言模型的主损失（next-token loss）去训 MoE，router 很容易把所有 token 都送去同一个专家。结果是：一个专家变成“万能专家”，其它专家几乎从不被激活（dead experts），你白白存了一堆参数，性能也会变差。\n因此，课堂上也在反复强调的一点是：\n\n\n\n\n\n\nImportant\n\n\n\nMoE 的 forward 很简单，难点在于训练时如何避免 expert collapse，并让专家使用更均匀、更有效率。\n\n\n接下来，我们来看看这个训练问题难在哪里，以及有哪些常用的解决方法。\n\n2.3.1 The Challenge of MoE Training\nMoE 的优势在于，在训练时，我们只需要根据Router来选择激活部分（Top-k）专家进行计算，而不是所有专家都参与计算。这种稀疏激活的方式，可以大幅减少每一步的计算量（FLOPs），从而提升训练效率。但是，这种稀疏激活的方式也带来的挑战是：Top-K是不可微的，这使得我们无法直接使用梯度下降法来优化模型参数。因此，研究员们提出了几种方法来解决这个问题。\n\n2.3.1.1 RL Based Optimization\n一种思路是使用强化学习（Reinforcement Learning, RL）的方法来优化路由器的参数。\n\n\n\n\n\n\nRL 101\n\n\n\n对于不熟悉强化学习的同学，简单介绍一下。强化学习是一种机器学习方法，它通过与环境的交互来学习最优策略。在强化学习中，智能体（Agent）通过观察环境状态（State），选择动作（Action），并根据环境反馈的奖励（Reward）来调整策略，从而最大化累积奖励。 在交互的过程中，Agent所产生的动作通常是离散的（Discrete Action），这就导致了强化学习中的一个核心问题：如何在离散动作空间中进行有效的策略优化。常用的方法包括策略梯度（Policy Gradient）和Q-learning等。 因此，我们可以将MoE的路由问题视作一个强化学习问题，通过设计合适的奖励函数，来引导路由器学习更优的路由策略。\n\n\n但是，强化学习的方法通常比较复杂，且训练过程不稳定（Gradient Estimation Variance较大），因此在实际应用中并不常用。\n\n\n2.3.1.2 Stochastic Approximation\n另一种思路是使用随机近似（Stochastic Approximation）的方法来优化路由器的参数。具体来说，在 router 打分（logits）里注入噪声/扰动，让 top-K 的选择在训练早期“偶尔换路”，从而更像 bandit 的探索策略。其中一个经典的例子就是 Stochastic Jittering. 它通过在路由器的打分中添加高斯噪声，从而实现对专家的随机选择。接下来我们来看一下Stochastic Jittering的具体实现细节。\n\n2.3.1.2.1 Stochastic Jittering\nStochastic Jittering 的实现步骤如下：\n\nRouter 计算：对于每个 token 的 hidden state \\(h_i\\)，通过一个线性层计算每个专家的分数： \\[\ns_{i,j} = W_g h_i + b_g\n\\tag{5}\\]\n添加噪声：在每个专家的分数中添加高斯噪声： \\[\n\\tilde{s}_{i,j} = s_{i,j} + \\epsilon_{i,j}, \\quad \\epsilon_{i,j} \\sim \\mathcal{N}(0, \\sigma(x_i)^2)\n\\tag{6}\\]\nTop-K 选择：对于每个 token，选择添加噪声后的分数最高的 K 个专家： \\[\ng_{i, j} = \\begin{cases}\n\\tilde{s}_{i, j}, \\quad \\tilde{s}_{i, j} \\in \\text{Top-K}(\\tilde{s}_i) \\\\\n0, \\quad \\text{otherwise}\n\\end{cases}\n\\tag{7}\\]\n\n其中，\\(\\sigma(x_i)\\) 是噪声的标准差, 它是一个可学习的函数，通常通过一个小的神经网络来实现。通过调整噪声的大小，可以控制路由器的探索程度，从而提升模型的训练效果。\n那么这个Stochastic Jittering解决了什么问题呢？ 它其实解决了类似于“探索与利用”（Exploration vs. Exploitation）的问题。在MoE的训练过程中，我们希望路由器能够既能利用当前的知识（选择表现好的专家），又能探索新的可能性（尝试其他专家）。这于 \\(\\epsilon_{i,j} \\sim \\mathcal{N}(0, \\sigma(x_i)^2)\\) 中的噪声注入机制相呼应，通过在路由器的打分中添加噪声，可以让路由器在训练早期“偶尔换路”，从而更像 bandit 的探索策略。\n但是，它显然没有解决Top-K选择的不可微问题，并且，噪声的引入也可能导致训练过程的不稳定：\n\n噪声过大：可能导致路由器选择的专家过于随机，每个专家不够专门化，影响模型性能。\n噪声过小：可能无法有效促进探索，路由器仍然倾向于选择少数几个专家，导致专家崩溃（Expert Collapse）。\n\n\n\n\n\n\n\nNote\n\n\n\n课上还提到对 logits做乘法噪声（Multiplicative Perturbation），也是类似的思路，但是也有类似的问题。\n\n\n\n\n\n2.3.1.3 Auxiliary / Heuristic Balancing Losses\n既然MoE训练的难点在于避免专家崩溃（Expert Collapse），那么一个直接的思路就是在主任务损失（Cross Entropy Loss）之外，添加一些辅助损失（Auxiliary Losses）来鼓励专家的均衡使用。这样做的好处是，可以直接引导模型学习更均衡的专家使用策略，从而提升整体性能。在这里，我们介绍两种常用的辅助损失：Load Balancing Loss 和 Z-Loss。\n\n2.3.1.3.1 Load Balancing Loss\nLoad Balancing Loss 是Switch Transformer(Fedus, Zoph, and Shazeer 2022)中提出的一种辅助损失，旨在鼓励路由器均衡地使用所有专家。具体来说，它通过计算每个专家被选择的频率，并与理想的均衡分布进行比较，从而计算出负载均衡损失。其公式如下： $$\nL_{} = N _{i=1}^{N} f_i P_i $${#eq-load-balancing-loss}\n其中：\n\n\\(f_i\\) 是专家 \\(i\\) 被选择的频率, 定义为： \\[\nf_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1} \\{\\text{argmax } p(x) = i\\}\n\\tag{8}\\] \\(\\mathcal{B}\\) 是当前批次的样本集合，\\(T\\) 是token 的数量，\\(\\mathbb{1}\\) 是指示函数，当专家 \\(i\\) 被选择时取值为1，否则为0。直观来说：expert i 实际上“承担”了多少负载。\n\\(P_i\\) 是专家 \\(i\\) 的平均路由概率，定义为： \\[\nP_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x)\n\\tag{9}\\] 其中，\\(p_i(x)\\) 是路由器对专家 \\(i\\) 的输出分数。直观来说：router “本来倾向”把多少概率分给 expert i。\n\n直观来看：如果某个 expert 实际拿了很多 tokens (\\(f_i\\) 很大），同时 router 还继续给它很高概率（\\(P_i\\) 很大），那么它的负载均衡损失就会很大，从而促使路由器减少对该专家的选择。反之亦然。\n通过最小化负载均衡损失，可以鼓励路由器均衡地使用所有专家，从而提升模型的训练效果。具体来说： - 如果某个专家被选择的次数远高于平均水平，那么它的负载均衡损失就会很大，从而促使路由器减少对该专家的选择。 - 反之，如果某个专家被选择的次数远低于平均水平，那么它的负载均衡损失也会很大，从而促使路由器增加对该专家的选择。\n\n\n\n\n\n\nDeepseek 根据上面的 Load Balancing Loss 设计了一个改进版本。它们提出：\n\n\n- Per-expert Balancing Loss：将上述的负载均衡损失，拓展为Top-K专家的情况。 - Per-Device Balancing Loss：在分布式训练中，考虑每个计算设备上的专家负载均衡。\n\n\n我们来看看Per-Device Balancing Loss做的是什么。 我们知道，在分布式训练中，不同的计算设备上可能会有不同数量的专家。如果某个设备上的专家被选择的次数远高于平均水平，那么这个GPU很可能过载，甚至损坏，从而影响整体训练效率。而有些设备上的专家可能几乎没有被选择，导致资源浪费。因此，DeepSeek提出了Per-Device Balancing Loss，旨在鼓励每个计算设备上的专家均衡使用。其公式如下： \\[\nL_{\\text{device}} = \\beta \\cdot D \\cdot \\sum_{d=1}^{D} f_d P_d\n\\tag{10}\\]\n\n\n其中： - \\(D\\) 是计算设备的数量。 - \\(f_d\\) 是设备 \\(d\\) 上的专家被选择的频率， - \\(P_d\\) 是设备 \\(d\\) 上的专家的平均路由概率。\n\n\n&gt;Per-expert 保证“专家别死”，Per-device 保证“GPU 别爆”。\n\n\n\nDeepSeek V3 还提出了另一种改进版本，它们称作Auxiliary Loss Free Balancing，其核心思想是：不再主要依赖 \\(\\mathcal{L}_{\\text{ExpBal}}\\) 来做负载均衡，而是通过调整路由器的偏置项（Bias）来实现均衡使用专家。具体来说：\n\\[\ng_{i,j} = \\being{cases}\ns_{i,j} + b_j, \\quad s_{i,j} \\in \\text{Top-K}(s_i) \\\\\n0, \\quad \\text{otherwise}\n\\end{cases}\n\\tag{11}\\]\n直观来看：\n\n如果 expert i 最近拿到 token 太小 → 增大\\(b_i\\) → 它更容易进入 top-K\n如果 expert i 拿到的 token 太多 → 减小 \\(b_i\\) → 它更不容易被选中\n\n这是一种在线控制/在线学习式的负载均衡：用规则更新 \\(b_i\\)， 而不是​通过梯度下降去学习 \\(P_i\\) ​\n\n\n\n\n\n\nFigure 9: 我们看到，不加负载均衡损失时，某些专家几乎没有被选择（Dead Experts）。而加入负载均衡损失后，专家的使用更加均衡。\n\n\n\n\n\n2.3.1.3.2 Z-Loss\nZ-Loss 是另一种常用的辅助损失，旨在鼓励路由器的输出分布更加均匀。其公式如下： \\[\nL_{z} = \\sum_{i} \\left( \\frac{p_i}{\\sum_{j} p_j} - \\frac{1}{E} \\right)^2\n\\tag{12}\\] 其中，\\(p_i\\) 是路由器对专家 \\(i\\) 的输出分数，\\(E\\) 是专家的总数。通过最小化这个损失，可以鼓励路由器的输出分布更加均匀，从而提升模型的训练效果。\n\n\n\n2.3.1.4 Loss Combination\n在实际应用中，MoE模型的总损失通常由主任务损失（Cross Entropy Loss）和辅助损失（Load Balancing Loss 和 Z-Loss）组成： \\[\nL_{total} = L_{CE} + \\lambda_{load} L_{load} + \\lambda_{z} L_{z}\n\\tag{13}\\] 其中，\\(L_{CE}\\) 是主任务损失，\\(L_{load}\\) 是负载均衡损失，\\(L_{z}\\) 是Z-Loss，\\(\\lambda_{load}\\) 和 \\(\\lambda_{z}\\)\n\n\n\n2.3.2 System Side\n在MoE模型的实现中，系统层面的优化也是非常重要的。由于MoE模型通常包含大量的专家，因此在分布式训练中，需要考虑如何高效地管理和调度这些专家，以提升训练效率和模型性能。但是，它们也带来了系统实现上的挑战，比如：\n\n每个 token 只激活少数专家（top-K）才能省 FLOPs，但专家往往分散在不同 GPU/节点上，于是训练要频繁做 all-to-all dispatch + all-to-all gather（把 token 发给对应专家算，再收回来合并）。这类通信是否划算取决于专家 FFN 计算是否“够大够重”，能否 amortize 通信成本。\n\n\n\n2.3.3 Fine-tuning MoE\nMoE fine-tune 更难：更容易不稳定（blow up）+ 更容易过拟合（train–val gap 大）。\n\n\n2.3.4 Upcycling\nMoE 的另一个有趣应用是“Upcycling”（回收利用）。具体来说，先训练好一个 dense Transformer，再把其中的 FFN/MLP 复制成多份专家（experts），加上一个 router，让模型从这一刻起变成 MoE；继续训练一段时间，就能用较低成本得到“总参数更大、推理仍稀疏激活”的 MoE。它解决了“从零训练 MoE 太慢太难”的问题。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 04: MoE Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html",
    "href": "posts/CS336/Lecture03/lec03.html",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "",
    "text": "Lecture 03 介绍了现代大模型的核心架构与不同的超参数设计。这节课是理解大模型的基础，内容比较多，建议多看几遍。 本节课的目标是了解下图中的内容:",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#normalization",
    "href": "posts/CS336/Lecture03/lec03.html#normalization",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.1 Normalization",
    "text": "2.1 Normalization\n\n为什么需要 Normalization？\nNormalization 技术在深度学习中起到了稳定训练过程和加速收敛的作用。 它通过调整神经网络层的输入分布，减少了内部协变量偏移 (Internal Covariate Shift)，从而使得模型在训练过程中更加稳定。此外，Normalization 还可以帮助缓解梯度消失和梯度爆炸问题，提高模型的泛化能力。\n\n\n2.1.1 Position of Normalization\n\n2.1.1.1 Post-Norm\n在Transformer中，Normalization放置在Sublayer之后，也就是所谓的 Post-Norm 结构。用数学公式表示为:\n\\[\n\\text{Post-Norm: } \\quad \\text{Norm}(x + \\text{Sublayer}(x))\n\\tag{1}\\]\n然而，(Ba, Kiros, and Hinton 2016) 指出，Post-Norm存在以下的问题:\n\n梯度不稳定，特别是在初始化阶段： 论文使用平均场理论分析指出，在 Post-Norm 结构 下（即 LayerNorm 放在残差连接之后），靠近输出层的参数在初始化时梯度期望值很大。 这会导致训练初期梯度爆炸，从而影响模型的稳定性和收敛速度。\n依赖复杂的 warm-up 超参数调优： 由于 Post-Norm 结构在训练初期容易出现梯度不稳定的问题，因此需要使用复杂的学习率 warm-up 策略来缓解这一问题。 这增加了模型训练的复杂性和调优难度。\n\n\nSpecifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem.  On Layer Normalization in the Transformer Architecture P.1 \n\n\n\n2.1.1.2 Pre-Norm\n为了缓解 Post-Norm 的问题，(Ba, Kiros, and Hinton 2016) 提出了 Pre-Norm 结构，即将 Normalization 放置在 Sublayer 之前。 用数学公式表示为:\n\\[\n\\text{Pre-Norm: } \\quad x + \\text{Sublayer}(\\text{Norm}(x))\n\\tag{2}\\]\n下图展示了 Post-Norm 与 Pre-Norm 结构的对比:\n\n\n\n\n\n\nFigure 2: Post Norm 与 Pre-Norm 结构对比。\n\n\n\nPre-Norm 有以下优点:\n\n提高训练稳定性： Pre-Norm 结构通过在每个子层之前进行归一化，减少了梯度爆炸和梯度消失的风险，从而提高了训练的稳定性。\n简化超参数调优： 由于 Pre-Norm 结构在训练过程中更加稳定，因此不再需要复杂的学习率 warm-up 策略，简化了模型的训练过程和超参数调优。\n\n\nWe show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.  On Layer Normalization in the Transformer Architecture P.1 \n\n接下来我们来看一下为什么Pre-Norm能有这么多的好处：\n\nGradient Attenuation\n\n\n\n\n\n\n\nFigure 3: 上图展示了 Pre-Norm 与 Post-Norm Transformer 在梯度行为上的差异。可以看到，在初始化阶段，Post-Norm 会导致靠近输出层的梯度期望值显著增大，而底层梯度被明显削弱，呈现出严重的梯度失衡（gradient attenuation across layers），从而使训练过程极不稳定，必须依赖学习率 warm-up 进行缓解。相比之下，Pre-Norm 在初始化时各层梯度规模保持一致且稳定，避免了梯度衰减与爆炸问题。\n\n\n\n\nGradient Spike / Noise\n\n实验表明 (Nguyen and Salazar 2019)\n\n\n\n\n\n\nFigure 4\n\n\n\n\nPost-norm produces noisy gradients with many sharp spikes, even towards the end of training. On the other hand, Pre-norm has fewer noisy gradients with smaller sizes, even without warmup.  Transformers without Tears: Improving the Normalization of Self-Attention P.6 \n\n\n\n2.1.1.3 Other Positions\n除了 Pre-Norm 和 Post-Norm 之外，还有一些其他的 Normalization 位置设计，例如:\n\nSandwich Norm: 将 Normalization 放置在每个子层的输入和输出之间。\nDouble Norm: 在每个子层的输入和输出都进行归一化。\n\n从这些实验结果来看，我们可以得出结论： - 尽量保持Residual Connection两端的信号稳定是非常重要的, 这可以保证梯度在网络中顺利传播。\n\n\n\n2.1.2 Normalization Types\n除了 Normalization 的位置设计之外，Normalization 的形式也是一个重要的设计选择。 常见的 Normalization 形式包括:\n\nLayerNorm(Ba, Kiros, and Hinton 2016): 对每个样本的特征维度进行归一化，适用于序列数据。\nRMSNorm(Zhang and Sennrich 2019): 只使用均方根（Root Mean Square）来进行归一化，省略了均值的计算，减少了计算开销。 RMSNorm 在某些情况下可以提供与 LayerNorm 相似的性能，但计算更高效。\n\n原始的Transformer中使用的是LayerNorm， 但是现代大语言模型中，RMSNorm被广泛采用， 例如在 LLaMA、GPT-4、PaLM 等模型中都使用了 RMSNorm。\nRMSNorm的存在的优势是，\n\n计算效率更高： 由于 RMSNorm 省略了均值的计算，因此在计算上更加高效，特别是在大规模模型中，这种效率提升尤为显著。\n性能相似： 实验表明，在许多任务中，RMSNorm 可以提供与 LayerNorm 相似的性能，尤其是在大语言模型中。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#activations",
    "href": "posts/CS336/Lecture03/lec03.html#activations",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.2 Activations",
    "text": "2.2 Activations\nActivation Functions给模型引入了非线性，使得神经网络能够学习复杂的函数映射关系。 常见的激活函数包括ReLU、Sigmoid、Tanh等。\n\n现代LLM中，最常见的激活函数是 Gated Activations家族，例如 SwiGLU，接下来，我们看看这些架构\n\n2.2.1 Gated Activations(*GLU)\nGated Activations 通过引入门控机制，允许模型在前馈神经网络中动态调整信息流，从而提高模型的表达能力和性能。在传统的FFN中，输入通过两个线性变换和一个非线性激活函数进行处理。而在 Gated Activations 中，输入被分成两部分，一部分通过激活函数处理，另一部分通过门控机制进行调节，最终两部分的输出进行元素级乘法操作。用数学公式表示为:\n\\[\n\\text{FF} = \\textcolor{red}{\\max (0, XW_1 )} \\ctimes (XW_2 )\n\\]\nGated Activations 主要改变的就是红色部分，用数学表达就是： \\[\n\\text{Gated FF} = \\textcolor{red}{(\\max (0, XW_1 ) \\odot XV )}W_2\n\\]\n常见的 Gated Activations 包括:\n\nGeGLU:\n\n\\[\n\\text{GeGLU: } \\quad \\text{Gated FF} = (\\text{GELU}(XW_1 ) \\odot XV )W_2\n\\]\n\nSwiGLU: \\[\n\\text{SwiGLU: } \\quad \\text{Gated FF} = (\\text{SiLU}(XW_1 ) \\odot XV )W_2\n\\]\n\n实验表明，Gated Activations 在许多任务中都优于传统的激活函数，特别是在大语言模型中，例如 LLaMA 和 GPT-4 等模型中都采用了 SwiGLU 作为前馈神经网络的激活函数。\n不过需要主要的一点是，Gated Activation引入了一个额外的线性变换矩阵V，这会增加模型的参数量和计算开销。 因此，在实际应用中，为了保持模型数量的不变，通常我们将 \\(d_ff\\) 设定为 \\(\\frac{8}{3} d_{model}\\)， 这样就可以在引入Gated Activation的同时，保持模型的参数量不变。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#serial-parallel-mlp",
    "href": "posts/CS336/Lecture03/lec03.html#serial-parallel-mlp",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.3 Serial & Parallel MLP",
    "text": "2.3 Serial & Parallel MLP\n传统的Transformer Block是串行的结构 Figure 1， 即先经过Attention模块， 然后再经过MLP模块。 这种设计虽然简单，但在某些情况下可能会限制模型的表达能力。有研究提出了并行的MLP设计，将Attention和MLP模块并行处理，然后将它们的输出进行融合。这种设计可以提高模型的表达能力和计算效率。 用数学公式表示为:\n\\[\ny = x + \\text{MLP}(Norm(x)) + \\text{Attention}(Norm(x))\n\\]\n通过并行这两层，可以让模型训练的更快速，同时可以减少模型的参数，比如Norm层只需要一层。\n不过，目前主流的大语言模型仍然采用串行的Transformer Block设计，但并行的MLP设计为未来的模型架构提供了一个有趣的方向。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#position-encoding",
    "href": "posts/CS336/Lecture03/lec03.html#position-encoding",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "2.4 Position Encoding",
    "text": "2.4 Position Encoding\n位置编码 (Positional Encoding) 用于引入序列中单词的位置信息，因为自注意力机制本身不具备顺序信息。 传统的Transformer使用的是绝对位置编码，例如正弦和余弦函数编码(Vaswani et al. 2023)。常见的位置编码方法包括:\n\nAbsolute Position Encoding: 使用固定的编码方式为每个位置分配一个唯一的向量表示。\nRelative Position Encoding: 通过计算单词之间的相对位置来引入位置信息，增强模型对序列中单词相对关系的理解能力。\nRotary Position Embedding (RoPE)(Su et al. 2023): 通过旋转位置向量来引入位置信息，增强模型对长距离依赖关系的捕捉能力。\n\n接下来我们重点介绍 RoPE。\n\n2.4.1 Rotary Position Embedding (RoPE)\n\n\n\n\n\n\nFigure 5\n\n\n\nRoPE 通过对查询和键的向量进行旋转来引入位置信息。 具体来说，RoPE 将位置编码表示为一个旋转矩阵，然后将查询和键的向量与该旋转矩阵相乘，从而引入位置信息。 用数学公式表示为:\n\\[\n\\text{RoPE}(Q, K, P) = (Q R(P), K R(P))\n\\] 其中，\\(R(P)\\) 是位置编码对应的旋转矩阵。\n用数学表示就是： \\[\nR_{\\Theta,m}^{d} \\mathbf{x}\n=\n\\begin{pmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n\\vdots\\\\\nx_{d-1}\\\\\nx_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{1})\\\\\n\\cos(m\\theta_{2})\\\\\n\\cos(m\\theta_{2})\\\\\n\\vdots\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\cos\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n- x_2\\\\\nx_1\\\\\n- x_4\\\\\nx_3\\\\\n\\vdots\\\\\n- x_d\\\\\nx_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{1})\\\\\n\\sin(m\\theta_{2})\\\\\n\\sin(m\\theta_{2})\\\\\n\\vdots\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\\\\\n\\sin\\!\\big(m\\theta_{d/2}\\big)\n\\end{pmatrix}\n\\]\n我非常推荐以下的视频，它很清楚的介绍了RoPE以及它的扩展，有兴趣的同学可以前去查看：",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#mlp-width",
    "href": "posts/CS336/Lecture03/lec03.html#mlp-width",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "3.1 MLP Width",
    "text": "3.1 MLP Width\nMLP的宽度通常设置为模型维度的4倍，例如对于一个512维的模型，MLP的宽度通常设置为2048。 这种设计可以提供足够的表达能力，同时不会过度增加计算开销。 当然，我们在之前提到过， 如果使用Gated Activation, 那么MLP的宽度通常设置为 \\(\\frac{8}{3}\\) 倍的模型维度。基本上目前主流的大语言模型都是采用这个比例。\n除了Gated Activation之外， 也有一些模型使用更宽的MLP， 例如 T5 使用了 64 倍的MLP宽度， 但是这种设计会显著增加计算开销， 因此需要权衡模型性能与计算资源。\n至于为什么选择4倍或者 \\(\\frac{8}{3}\\) 倍的MLP宽度， 主要是基于经验和实验结果。 研究表明，这些比例可以在保持模型性能的同时，提供足够的表达能力。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#attention-heads",
    "href": "posts/CS336/Lecture03/lec03.html#attention-heads",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "3.2 Attention Heads",
    "text": "3.2 Attention Heads\n模型的Head Dim 通常设置为 d_model / num_heads， 也就是说，Head Dim 与模型维度成反比。 常见的Head Dim设置包括64、128等。不过，Head Dim 不一定就是d_model / num_heads。 不过并没有实验表明，Head Dim 过大或者过小会显著影响模型性能。 因此，在实际应用中，通常根据计算资源和模型规模来选择Head Dim。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#aspect-ratio",
    "href": "posts/CS336/Lecture03/lec03.html#aspect-ratio",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "3.3 Aspect Ratio",
    "text": "3.3 Aspect Ratio\nAspect Ratio 指的是模型的深度（n_layer）与宽度（d_model）之比。 研究表明，较高的Aspect Ratio（即更深的模型）通常可以提供更好的性能，特别是在处理复杂任务时。 然而，过深的模型也可能导致训练困难和过拟合问题。\n\n不过过深的模型也会带来一些挑战，例如Parallelism和训练稳定性问题。 因此，在选择Aspect Ratio时，需要权衡模型性能与训练稳定性。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#vocabulary-size",
    "href": "posts/CS336/Lecture03/lec03.html#vocabulary-size",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "3.4 Vocabulary Size",
    "text": "3.4 Vocabulary Size\n词表大小（Vocabulary Size）是指模型在训练和推理过程中使用的唯一单词或子词的数量。 词表大小的选择对于模型的性能和计算效率有着重要影响。对于单一个语言的模型，常见的词表大小范围在30,000到100,000之间。 对于多语言模型，词表大小通常更大，以覆盖更多的语言和词汇。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#regularization",
    "href": "posts/CS336/Lecture03/lec03.html#regularization",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "3.5 Regularization",
    "text": "3.5 Regularization\n正则化技术用于防止模型过拟合，提高模型的泛化能力。但是许多人提出了一个问题， 大模型是否还需要正则化？ 当模型足够大时， 它们似乎并不容易过拟合， 因此正则化的必要性受到质疑。",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#grouped-query-attention-gqa-multi-query-attention-mqa",
    "href": "posts/CS336/Lecture03/lec03.html#grouped-query-attention-gqa-multi-query-attention-mqa",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "5.1 Grouped Query Attention (GQA) / Multi-Query Attention (MQA)",
    "text": "5.1 Grouped Query Attention (GQA) / Multi-Query Attention (MQA)",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#sparse-sliding-window-attention",
    "href": "posts/CS336/Lecture03/lec03.html#sparse-sliding-window-attention",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "5.2 Sparse / Sliding Window Attention",
    "text": "5.2 Sparse / Sliding Window Attention\n\nChild et al. (2019)",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#sliding-window-attention",
    "href": "posts/CS336/Lecture03/lec03.html#sliding-window-attention",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "5.3 Sliding Window Attention",
    "text": "5.3 Sliding Window Attention",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/Lecture03/lec03.html#interleaved-attention",
    "href": "posts/CS336/Lecture03/lec03.html#interleaved-attention",
    "title": "Lecture 03: LM Model Architecture & Hyperparameters",
    "section": "5.4 Interleaved Attention",
    "text": "5.4 Interleaved Attention",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Lecture 03: Transformer LM Architecture"
    ]
  },
  {
    "objectID": "posts/CS336/index.html",
    "href": "posts/CS336/index.html",
    "title": "CS336: LLM from Scratch Lecture Notes and Assignments",
    "section": "",
    "text": "Note\n\n\n Due to the time constraint, these notes may contain errors or omissions. To speed up the writing process, I only wrote the notes in CHINESES. If you are interested in collaborating to improve these notes,or transalting to different languages, please feel free to contact me. \n\n\n\n\n\nRelated Resources:\n\nLecture Website: CS336 LLM from Scratch\nLecture Recordings: YouTube Playlist\nMy Solution Repo: GitHub\n\n\n\nAbout this Course:\n\nThis course has 17 Lectures and 5 Assignments in total.\nIt might take around 200 hours to finish all the lectures and assignments.\n\n\n\n\n\nFor those who have limited time, I recommend focusing on the following key lectures and assignments:\n\nLECTURE 1, 2, 3, 4 & Assignment01: After completing these, you will have a solid understanding of the fundamentals of LLMs, such as the Transformer Language Model architecture, attention mechanism, Mixture of Experts, and the training process of LLMs using autoregressive language modeling.\nLECTURE 15, 16, 17 & Assignment05: These cover advanced topics such as LLM aligment algorithms, such as SFT, RLHF(PPO, DPO), and RLVR(GRPO, Dr.GRPO). After completing these, you will understand how to align LLMs with human preferences and train a reasoning LLM.\nLECTURE 5, 6, 7, 8 & Assignment02: These focus on the hardware and parallelism techniques for training large models. After completing these, you will understand how to efficiently train large LLMs using distributed systems, such as data parallelism, model parallelism, and pipeline parallelism, and speed up the training process by leveraging the power of GPU, and undertand FlashAttention and its implementation.\nThe remaining lectures and assignments are also important, but they can be studied at a later time based on your interests and needs. Those includes:\n\nLecture 9 & 11: Scaling Laws\nLecture 10: Inference Optimization\nLecture 12 & Assignment 03: Evaluation of LLMs\nLecture 13, 14 & Assignment 04: Data Collection and Processing\n\n\n\n  \n\nLecture Notes for CS336\n\n\n\n\n\nNo matching items\n\n\n\nAssignments\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Stanford CS336: LLM from Scratch",
      "Course Overview"
    ]
  }
]